{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ykAZa7jcxm7c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Conv2D,MaxPool2D,concatenate,AveragePooling2D,Add,GlobalAveragePooling2D,Flatten,ZeroPadding2D,BatchNormalization,Dense,ZeroPadding2D,Activation,ReLU,Input,DepthwiseConv2D\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from keras.preprocessing import image\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU') \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5xwBZf8SJn_8"
   },
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "train_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Train'\n",
    "test_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Test'\n",
    "valid_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CvuDm_pgJn_9"
   },
   "outputs": [],
   "source": [
    "#Rescaling and augmentation of data\n",
    "data_augmentation=tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\",input_shape=(224,224,3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "],name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WW8Ztgk-Jn__",
    "outputId": "920e5fa8-0d85-498e-8ab0-b24ca2fb8163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE=(224,224)\n",
    "BATCH_SIZE=5\n",
    "training_set=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = test_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode = 'categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = valid_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode = 'categorical',\n",
    ")\n",
    "class_names=validation_set.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U6XfIjOkNnUC"
   },
   "outputs": [],
   "source": [
    "def inverted_residual_block(inputs, filters, expansion, stride, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Constructs an inverted residual block with bottleneck.\n",
    "\n",
    "    Arguments:\n",
    "    - inputs: Input tensor.\n",
    "    - filters: Number of filters for the pointwise convolution.\n",
    "    - expansion: Expansion factor for the inner bottleneck.\n",
    "    - stride: Stride of the depthwise convolution.\n",
    "    - kernel_size: Kernel size of the depthwise convolution.\n",
    "\n",
    "    Returns:\n",
    "    - Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pointwise convolution expanding dimensions\n",
    "    x = Conv2D(filters * expansion, (1, 1), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Depthwise convolution\n",
    "    x = DepthwiseConv2D(kernel_size, strides=stride, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Pointwise convolution squeezing back dimensions\n",
    "    x = Conv2D(filters, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Adding the residual connection when possible\n",
    "    if stride == 1 and inputs.shape[-1] == filters:\n",
    "        x = Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "def MobileNetV2(input_shape=(224, 224, 3), classes=5):\n",
    "    \"\"\"\n",
    "    Constructs the MobileNetV2 architecture.\n",
    "\n",
    "    Arguments:\n",
    "    - input_shape: Shape of the input tensor.\n",
    "    - num_classes: Number of classes for the classification task.\n",
    "\n",
    "    Returns:\n",
    "    - Model object.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial convolution layer with larger filter size and stride\n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Sequence of inverted residual blocks\n",
    "    # The configurations of filters, expansion factors, strides, and kernel sizes\n",
    "    # are based on the MobileNetV2 paper and its typical implementations\n",
    "    x = inverted_residual_block(x, 16, 1, 1, 3)\n",
    "    x = inverted_residual_block(x, 24, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 24, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 32, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 32, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 32, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 64, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 64, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 64, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 64, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 96, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 96, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 96, 6, 1, 3)\n",
    "    x = inverted_residual_block(x, 160, 6, 2, 3)\n",
    "    x = inverted_residual_block(x, 160, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 160, 6, 1, 5)\n",
    "    x = inverted_residual_block(x, 320, 6, 1, 3)\n",
    "\n",
    "    # Final pointwise convolution layer\n",
    "    x = Conv2D(1280, (1, 1), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    # Global pooling and fully connected layer for classification\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=inputs,outputs=outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y1h9OqVWkxvf"
   },
   "outputs": [],
   "source": [
    "model=MobileNetV2(input_shape=(224,224,3),classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kt6IBEy1JoAJ"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "#Checkpoint to save the best model per epoch\n",
    "model_path=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
    "checkpoint=ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1FPPpQFkJoAL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 32  896         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 112, 112, 32  128        ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 112, 112, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 112, 112, 16  528         ['re_lu[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 112, 112, 16  64         ['conv2d_1[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 112, 112, 16  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 112, 112, 16  160        ['re_lu_1[0][0]']                \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 112, 112, 16  64         ['depthwise_conv2d[0][0]']       \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 112, 112, 16  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 112, 112, 16  272         ['re_lu_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 112, 112, 16  64         ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 112, 112, 14  2448        ['batch_normalization_3[0][0]']  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 112, 112, 14  576        ['conv2d_3[0][0]']               \n",
      " rmalization)                   4)                                                                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 112, 112, 14  0           ['batch_normalization_4[0][0]']  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " depthwise_conv2d_1 (DepthwiseC  (None, 56, 56, 144)  1440       ['re_lu_3[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 56, 56, 144)  576        ['depthwise_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 56, 56, 144)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 56, 56, 24)   3480        ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 56, 56, 24)  96          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 56, 56, 144)  3600        ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 56, 56, 144)  576        ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 56, 56, 144)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " depthwise_conv2d_2 (DepthwiseC  (None, 56, 56, 144)  1440       ['re_lu_5[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 56, 56, 144)  576        ['depthwise_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 56, 56, 144)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 56, 56, 24)   3480        ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 56, 56, 24)  96          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 56, 56, 24)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'batch_normalization_6[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 56, 56, 192)  4800        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 56, 56, 192)  768        ['conv2d_7[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 56, 56, 192)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_3 (DepthwiseC  (None, 28, 28, 192)  1920       ['re_lu_7[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 28, 28, 192)  768        ['depthwise_conv2d_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 28, 28, 192)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 28, 28, 32)   6176        ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 28, 28, 32)  128         ['conv2d_8[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 28, 28, 192)  6336        ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 28, 28, 192)  768        ['conv2d_9[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 28, 28, 192)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_4 (DepthwiseC  (None, 28, 28, 192)  1920       ['re_lu_9[0][0]']                \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 28, 28, 192)  768        ['depthwise_conv2d_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 28, 28, 192)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 28, 28, 32)   6176        ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 28, 28, 32)  128         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 28, 28, 32)   0           ['batch_normalization_15[0][0]', \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 28, 28, 192)  6336        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 28, 28, 192)  768        ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 28, 28, 192)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_5 (DepthwiseC  (None, 28, 28, 192)  1920       ['re_lu_11[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 28, 28, 192)  768        ['depthwise_conv2d_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 28, 28, 192)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 28, 28, 32)   6176        ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 28, 28, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 28, 28, 32)   0           ['batch_normalization_18[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 28, 28, 384)  12672       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 28, 28, 384)  1536       ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 28, 28, 384)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_6 (DepthwiseC  (None, 14, 14, 384)  3840       ['re_lu_13[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_14[0][0]']               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 14, 14, 64)  256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 14, 14, 384)  24960       ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 14, 14, 384)  1536       ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_7 (DepthwiseC  (None, 14, 14, 384)  3840       ['re_lu_15[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_7[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 14, 14, 64)  256         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 14, 14, 64)   0           ['batch_normalization_24[0][0]', \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 14, 14, 384)  24960       ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 14, 14, 384)  1536       ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_8 (DepthwiseC  (None, 14, 14, 384)  9984       ['re_lu_17[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_8[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 14, 14, 64)  256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 14, 14, 64)   0           ['batch_normalization_27[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 14, 14, 384)  24960       ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 14, 14, 384)  1536       ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_9 (DepthwiseC  (None, 14, 14, 384)  9984       ['re_lu_19[0][0]']               \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_9[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 14, 14, 384)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 14, 14, 64)   24640       ['re_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 14, 14, 64)  256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 14, 14, 64)   0           ['batch_normalization_30[0][0]', \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 14, 14, 576)  37440       ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 14, 14, 576)  2304       ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_10 (Depthwise  (None, 14, 14, 576)  14976      ['re_lu_21[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_32 (BatchN  (None, 14, 14, 576)  2304       ['depthwise_conv2d_10[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 14, 14, 96)   55392       ['re_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 14, 14, 96)  384         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 14, 14, 576)  55872       ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 14, 14, 576)  2304       ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_11 (Depthwise  (None, 14, 14, 576)  5760       ['re_lu_23[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 14, 14, 576)  2304       ['depthwise_conv2d_11[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 14, 14, 96)   55392       ['re_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 14, 14, 96)  384         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 14, 14, 96)   0           ['batch_normalization_36[0][0]', \n",
      "                                                                  'batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 14, 14, 576)  55872       ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 14, 14, 576)  2304       ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_12 (Depthwise  (None, 14, 14, 576)  5760       ['re_lu_25[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 14, 14, 576)  2304       ['depthwise_conv2d_12[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 14, 14, 576)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 14, 14, 96)   55392       ['re_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 14, 14, 96)  384         ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 14, 14, 96)   0           ['batch_normalization_39[0][0]', \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 14, 14, 960)  93120       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 14, 14, 960)  3840       ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 14, 14, 960)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_13 (Depthwise  (None, 7, 7, 960)   9600        ['re_lu_27[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 7, 7, 960)   3840        ['depthwise_conv2d_13[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 7, 7, 160)    153760      ['re_lu_28[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 7, 7, 160)   640         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 7, 7, 960)    154560      ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 7, 7, 960)   3840        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " depthwise_conv2d_14 (Depthwise  (None, 7, 7, 960)   24960       ['re_lu_29[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 7, 7, 960)   3840        ['depthwise_conv2d_14[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 7, 7, 160)    153760      ['re_lu_30[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 7, 7, 160)   640         ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 7, 7, 160)    0           ['batch_normalization_45[0][0]', \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 7, 7, 960)    154560      ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 7, 7, 960)   3840        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_15 (Depthwise  (None, 7, 7, 960)   24960       ['re_lu_31[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 7, 7, 960)   3840        ['depthwise_conv2d_15[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 7, 7, 960)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 7, 7, 160)    153760      ['re_lu_32[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 7, 7, 160)   640         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 7, 7, 160)    0           ['batch_normalization_48[0][0]', \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 7, 7, 1920)   309120      ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 7, 7, 1920)  7680        ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)                (None, 7, 7, 1920)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " depthwise_conv2d_16 (Depthwise  (None, 7, 7, 1920)  19200       ['re_lu_33[0][0]']               \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 7, 7, 1920)  7680        ['depthwise_conv2d_16[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)                (None, 7, 7, 1920)   0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 7, 7, 320)    614720      ['re_lu_34[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 7, 7, 320)   1280        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 7, 7, 1280)   410880      ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 7, 7, 1280)  5120        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)                (None, 7, 7, 1280)   0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['re_lu_35[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            6405        ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,981,301\n",
      "Trainable params: 2,939,893\n",
      "Non-trainable params: 41,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "87_dyg84JoAN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HarG5-LZJoAP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.3019 - accuracy: 0.4847\n",
      "Epoch 1: val_loss improved from inf to 1.67381, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet01-0.3488.hdf5\n",
      "276/276 [==============================] - 100s 253ms/step - loss: 1.3019 - accuracy: 0.4847 - val_loss: 1.6738 - val_accuracy: 0.3488\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.0053 - accuracy: 0.5858\n",
      "Epoch 2: val_loss did not improve from 1.67381\n",
      "276/276 [==============================] - 64s 228ms/step - loss: 1.0053 - accuracy: 0.5858 - val_loss: 2.0244 - val_accuracy: 0.3488\n",
      "Epoch 3/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.9319 - accuracy: 0.6364\n",
      "Epoch 3: val_loss did not improve from 1.67381\n",
      "276/276 [==============================] - 76s 273ms/step - loss: 0.9348 - accuracy: 0.6359 - val_loss: 2.4056 - val_accuracy: 0.4244\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.8676 - accuracy: 0.6541\n",
      "Epoch 4: val_loss improved from 1.67381 to 1.62540, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet04-0.5291.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.8676 - accuracy: 0.6541 - val_loss: 1.6254 - val_accuracy: 0.5291\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.6940\n",
      "Epoch 5: val_loss improved from 1.62540 to 1.62248, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet05-0.5988.hdf5\n",
      "276/276 [==============================] - 37s 131ms/step - loss: 0.7750 - accuracy: 0.6940 - val_loss: 1.6225 - val_accuracy: 0.5988\n",
      "Epoch 6/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.6649 - accuracy: 0.7476\n",
      "Epoch 6: val_loss did not improve from 1.62248\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.6652 - accuracy: 0.7471 - val_loss: 2.0331 - val_accuracy: 0.4709\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6272 - accuracy: 0.7529\n",
      "Epoch 7: val_loss did not improve from 1.62248\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.6272 - accuracy: 0.7529 - val_loss: 2.6206 - val_accuracy: 0.5407\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.7783\n",
      "Epoch 8: val_loss did not improve from 1.62248\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.6093 - accuracy: 0.7783 - val_loss: 2.0602 - val_accuracy: 0.5581\n",
      "Epoch 9/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.5446 - accuracy: 0.8022\n",
      "Epoch 9: val_loss improved from 1.62248 to 1.39754, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet09-0.6395.hdf5\n",
      "276/276 [==============================] - 37s 131ms/step - loss: 0.5482 - accuracy: 0.8016 - val_loss: 1.3975 - val_accuracy: 0.6395\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.8140\n",
      "Epoch 10: val_loss improved from 1.39754 to 1.26425, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet10-0.6744.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.5112 - accuracy: 0.8140 - val_loss: 1.2643 - val_accuracy: 0.6744\n",
      "Epoch 11/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.8225\n",
      "Epoch 11: val_loss improved from 1.26425 to 0.84148, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet11-0.7035.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.4849 - accuracy: 0.8219 - val_loss: 0.8415 - val_accuracy: 0.7035\n",
      "Epoch 12/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8436\n",
      "Epoch 12: val_loss improved from 0.84148 to 0.57306, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet12-0.8023.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.4227 - accuracy: 0.8430 - val_loss: 0.5731 - val_accuracy: 0.8023\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.8408\n",
      "Epoch 13: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.4619 - accuracy: 0.8408 - val_loss: 2.8020 - val_accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.8285\n",
      "Epoch 14: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.4739 - accuracy: 0.8285 - val_loss: 0.6635 - val_accuracy: 0.7558\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8459\n",
      "Epoch 15: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.4303 - accuracy: 0.8459 - val_loss: 0.6317 - val_accuracy: 0.8198\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.8983\n",
      "Epoch 16: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.2772 - accuracy: 0.8983 - val_loss: 1.9411 - val_accuracy: 0.7209\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.8706\n",
      "Epoch 17: val_loss did not improve from 0.57306\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.3434 - accuracy: 0.8706 - val_loss: 0.6245 - val_accuracy: 0.8314\n",
      "Epoch 18/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.3601 - accuracy: 0.8771\n",
      "Epoch 18: val_loss improved from 0.57306 to 0.45022, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet18-0.8779.hdf5\n",
      "276/276 [==============================] - 37s 132ms/step - loss: 0.3602 - accuracy: 0.8772 - val_loss: 0.4502 - val_accuracy: 0.8779\n",
      "Epoch 19/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8938\n",
      "Epoch 19: val_loss did not improve from 0.45022\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.3015 - accuracy: 0.8932 - val_loss: 0.4549 - val_accuracy: 0.8023\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.9041\n",
      "Epoch 20: val_loss improved from 0.45022 to 0.44926, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet20-0.8895.hdf5\n",
      "276/276 [==============================] - 37s 130ms/step - loss: 0.2799 - accuracy: 0.9041 - val_loss: 0.4493 - val_accuracy: 0.8895\n",
      "Epoch 21/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.9178\n",
      "Epoch 21: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.2503 - accuracy: 0.9179 - val_loss: 0.9803 - val_accuracy: 0.7616\n",
      "Epoch 22/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9135\n",
      "Epoch 22: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.2665 - accuracy: 0.9135 - val_loss: 0.8077 - val_accuracy: 0.8081\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9346\n",
      "Epoch 23: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1990 - accuracy: 0.9346 - val_loss: 0.5037 - val_accuracy: 0.8837\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.9186\n",
      "Epoch 24: val_loss did not improve from 0.44926\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.2612 - accuracy: 0.9186 - val_loss: 0.9238 - val_accuracy: 0.7733\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.8983\n",
      "Epoch 25: val_loss improved from 0.44926 to 0.42447, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet25-0.8663.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.2796 - accuracy: 0.8983 - val_loss: 0.4245 - val_accuracy: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1957 - accuracy: 0.9331\n",
      "Epoch 26: val_loss improved from 0.42447 to 0.39421, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet26-0.9070.hdf5\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.1957 - accuracy: 0.9331 - val_loss: 0.3942 - val_accuracy: 0.9070\n",
      "Epoch 27/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9375\n",
      "Epoch 27: val_loss did not improve from 0.39421\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.2005 - accuracy: 0.9375 - val_loss: 1.6773 - val_accuracy: 0.6453\n",
      "Epoch 28/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9680\n",
      "Epoch 28: val_loss improved from 0.39421 to 0.38159, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet28-0.8953.hdf5\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.1059 - accuracy: 0.9680 - val_loss: 0.3816 - val_accuracy: 0.8953\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9484\n",
      "Epoch 29: val_loss did not improve from 0.38159\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1765 - accuracy: 0.9484 - val_loss: 0.4797 - val_accuracy: 0.8779\n",
      "Epoch 30/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1579 - accuracy: 0.9491\n",
      "Epoch 30: val_loss improved from 0.38159 to 0.34691, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet30-0.9128.hdf5\n",
      "276/276 [==============================] - 36s 128ms/step - loss: 0.1589 - accuracy: 0.9484 - val_loss: 0.3469 - val_accuracy: 0.9128\n",
      "Epoch 31/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9636\n",
      "Epoch 31: val_loss improved from 0.34691 to 0.17775, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet31-0.9593.hdf5\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.1247 - accuracy: 0.9637 - val_loss: 0.1778 - val_accuracy: 0.9593\n",
      "Epoch 32/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9607\n",
      "Epoch 32: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.1147 - accuracy: 0.9608 - val_loss: 0.8016 - val_accuracy: 0.8081\n",
      "Epoch 33/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9571\n",
      "Epoch 33: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1615 - accuracy: 0.9571 - val_loss: 0.8278 - val_accuracy: 0.8372\n",
      "Epoch 34/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9397\n",
      "Epoch 34: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1656 - accuracy: 0.9397 - val_loss: 0.5951 - val_accuracy: 0.8779\n",
      "Epoch 35/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9585\n",
      "Epoch 35: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1215 - accuracy: 0.9578 - val_loss: 0.6440 - val_accuracy: 0.8547\n",
      "Epoch 36/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9745\n",
      "Epoch 36: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0872 - accuracy: 0.9746 - val_loss: 1.2387 - val_accuracy: 0.8081\n",
      "Epoch 37/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9578\n",
      "Epoch 37: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1275 - accuracy: 0.9578 - val_loss: 0.4122 - val_accuracy: 0.9012\n",
      "Epoch 38/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1533 - accuracy: 0.9469\n",
      "Epoch 38: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1569 - accuracy: 0.9462 - val_loss: 0.7631 - val_accuracy: 0.7267\n",
      "Epoch 39/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9680\n",
      "Epoch 39: val_loss did not improve from 0.17775\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0987 - accuracy: 0.9680 - val_loss: 0.2289 - val_accuracy: 0.9419\n",
      "Epoch 40/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9869\n",
      "Epoch 40: val_loss improved from 0.17775 to 0.14820, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet40-0.9709.hdf5\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0444 - accuracy: 0.9869 - val_loss: 0.1482 - val_accuracy: 0.9709\n",
      "Epoch 41/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9469\n",
      "Epoch 41: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.1840 - accuracy: 0.9469 - val_loss: 1.2799 - val_accuracy: 0.7442\n",
      "Epoch 42/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9695\n",
      "Epoch 42: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0947 - accuracy: 0.9695 - val_loss: 0.2589 - val_accuracy: 0.9128\n",
      "Epoch 43/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9724\n",
      "Epoch 43: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0964 - accuracy: 0.9724 - val_loss: 0.2284 - val_accuracy: 0.9419\n",
      "Epoch 44/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.9731\n",
      "Epoch 44: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0813 - accuracy: 0.9731 - val_loss: 0.4869 - val_accuracy: 0.8895\n",
      "Epoch 45/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9644\n",
      "Epoch 45: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1065 - accuracy: 0.9644 - val_loss: 0.3298 - val_accuracy: 0.9244\n",
      "Epoch 46/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9673\n",
      "Epoch 46: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1097 - accuracy: 0.9673 - val_loss: 0.2838 - val_accuracy: 0.9360\n",
      "Epoch 47/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9782\n",
      "Epoch 47: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0682 - accuracy: 0.9782 - val_loss: 0.5353 - val_accuracy: 0.8779\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9833\n",
      "Epoch 48: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.5576 - val_accuracy: 0.8430\n",
      "Epoch 49/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1358 - accuracy: 0.9600\n",
      "Epoch 49: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1358 - accuracy: 0.9600 - val_loss: 0.3484 - val_accuracy: 0.8895\n",
      "Epoch 50/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9782\n",
      "Epoch 50: val_loss did not improve from 0.14820\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0782 - accuracy: 0.9782 - val_loss: 0.2650 - val_accuracy: 0.9244\n",
      "Epoch 51/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.9825\n",
      "Epoch 51: val_loss improved from 0.14820 to 0.13931, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet51-0.9651.hdf5\n",
      "276/276 [==============================] - 36s 128ms/step - loss: 0.0688 - accuracy: 0.9826 - val_loss: 0.1393 - val_accuracy: 0.9651\n",
      "Epoch 52/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9826\n",
      "Epoch 52: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0543 - accuracy: 0.9826 - val_loss: 0.3609 - val_accuracy: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9578\n",
      "Epoch 53: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1642 - accuracy: 0.9578 - val_loss: 0.5547 - val_accuracy: 0.8430\n",
      "Epoch 54/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9920\n",
      "Epoch 54: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0299 - accuracy: 0.9920 - val_loss: 0.2178 - val_accuracy: 0.9477\n",
      "Epoch 55/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9876\n",
      "Epoch 55: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0489 - accuracy: 0.9876 - val_loss: 0.2332 - val_accuracy: 0.9477\n",
      "Epoch 56/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9716\n",
      "Epoch 56: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0865 - accuracy: 0.9709 - val_loss: 0.5752 - val_accuracy: 0.9012\n",
      "Epoch 57/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9811\n",
      "Epoch 57: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0679 - accuracy: 0.9811 - val_loss: 0.2160 - val_accuracy: 0.9535\n",
      "Epoch 58/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0429 - accuracy: 0.9876\n",
      "Epoch 58: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0429 - accuracy: 0.9876 - val_loss: 0.1776 - val_accuracy: 0.9593\n",
      "Epoch 59/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9796\n",
      "Epoch 59: val_loss did not improve from 0.13931\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0786 - accuracy: 0.9797 - val_loss: 0.2215 - val_accuracy: 0.9477\n",
      "Epoch 60/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9927\n",
      "Epoch 60: val_loss improved from 0.13931 to 0.11801, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet60-0.9767.hdf5\n",
      "276/276 [==============================] - 37s 128ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.1180 - val_accuracy: 0.9767\n",
      "Epoch 61/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9884\n",
      "Epoch 61: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0421 - accuracy: 0.9884 - val_loss: 0.3138 - val_accuracy: 0.9302\n",
      "Epoch 62/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0669 - accuracy: 0.9818\n",
      "Epoch 62: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0669 - accuracy: 0.9818 - val_loss: 0.2057 - val_accuracy: 0.9477\n",
      "Epoch 63/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9818\n",
      "Epoch 63: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0551 - accuracy: 0.9818 - val_loss: 0.5458 - val_accuracy: 0.9070\n",
      "Epoch 64/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9825\n",
      "Epoch 64: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0527 - accuracy: 0.9818 - val_loss: 0.9343 - val_accuracy: 0.8314\n",
      "Epoch 65/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1847 - accuracy: 0.9469\n",
      "Epoch 65: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1845 - accuracy: 0.9469 - val_loss: 0.1567 - val_accuracy: 0.9767\n",
      "Epoch 66/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0250 - accuracy: 0.9942\n",
      "Epoch 66: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0297 - accuracy: 0.9935 - val_loss: 0.2236 - val_accuracy: 0.9477\n",
      "Epoch 67/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0461 - accuracy: 0.9891\n",
      "Epoch 67: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0461 - accuracy: 0.9891 - val_loss: 0.1736 - val_accuracy: 0.9535\n",
      "Epoch 68/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 0.9869\n",
      "Epoch 68: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 35s 125ms/step - loss: 0.0460 - accuracy: 0.9869 - val_loss: 0.3459 - val_accuracy: 0.9477\n",
      "Epoch 69/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9905\n",
      "Epoch 69: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0278 - accuracy: 0.9898 - val_loss: 0.1710 - val_accuracy: 0.9651\n",
      "Epoch 70/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9862\n",
      "Epoch 70: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.4377 - val_accuracy: 0.9070\n",
      "Epoch 71/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9964\n",
      "Epoch 71: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0098 - accuracy: 0.9964 - val_loss: 0.4504 - val_accuracy: 0.9186\n",
      "Epoch 72/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9811\n",
      "Epoch 72: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0681 - accuracy: 0.9804 - val_loss: 0.8221 - val_accuracy: 0.8779\n",
      "Epoch 73/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0931 - accuracy: 0.9724\n",
      "Epoch 73: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0931 - accuracy: 0.9724 - val_loss: 1.0895 - val_accuracy: 0.6570\n",
      "Epoch 74/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9775\n",
      "Epoch 74: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0914 - accuracy: 0.9775 - val_loss: 0.2344 - val_accuracy: 0.9477\n",
      "Epoch 75/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 75: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2893 - val_accuracy: 0.9302\n",
      "Epoch 76/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9920\n",
      "Epoch 76: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.3401 - val_accuracy: 0.9302\n",
      "Epoch 77/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9971\n",
      "Epoch 77: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0161 - accuracy: 0.9971 - val_loss: 0.3257 - val_accuracy: 0.9186\n",
      "Epoch 78/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9782\n",
      "Epoch 78: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0659 - accuracy: 0.9782 - val_loss: 0.5459 - val_accuracy: 0.8953\n",
      "Epoch 79/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9789\n",
      "Epoch 79: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0650 - accuracy: 0.9789 - val_loss: 0.6055 - val_accuracy: 0.9070\n",
      "Epoch 80/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9956\n",
      "Epoch 80: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.3689 - val_accuracy: 0.9186\n",
      "Epoch 81/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9869\n",
      "Epoch 81: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0380 - accuracy: 0.9869 - val_loss: 0.5394 - val_accuracy: 0.8779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9869\n",
      "Epoch 82: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0501 - accuracy: 0.9869 - val_loss: 0.3339 - val_accuracy: 0.9360\n",
      "Epoch 83/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9709\n",
      "Epoch 83: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.1050 - accuracy: 0.9709 - val_loss: 1.3468 - val_accuracy: 0.8081\n",
      "Epoch 84/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9767\n",
      "Epoch 84: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 128ms/step - loss: 0.0732 - accuracy: 0.9767 - val_loss: 0.2908 - val_accuracy: 0.9360\n",
      "Epoch 85/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9993\n",
      "Epoch 85: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0083 - accuracy: 0.9993 - val_loss: 0.2599 - val_accuracy: 0.9477\n",
      "Epoch 86/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 86: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.2244 - val_accuracy: 0.9651\n",
      "Epoch 87/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9985\n",
      "Epoch 87: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.2510 - val_accuracy: 0.9477\n",
      "Epoch 88/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9927\n",
      "Epoch 88: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.5407 - val_accuracy: 0.8895\n",
      "Epoch 89/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9556\n",
      "Epoch 89: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.1545 - accuracy: 0.9557 - val_loss: 0.4296 - val_accuracy: 0.9244\n",
      "Epoch 90/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9949\n",
      "Epoch 90: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0232 - accuracy: 0.9942 - val_loss: 0.3798 - val_accuracy: 0.9128\n",
      "Epoch 91/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0846 - accuracy: 0.9775\n",
      "Epoch 91: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0846 - accuracy: 0.9775 - val_loss: 0.1992 - val_accuracy: 0.9419\n",
      "Epoch 92/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9920\n",
      "Epoch 92: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0182 - accuracy: 0.9920 - val_loss: 0.2639 - val_accuracy: 0.9419\n",
      "Epoch 93/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9971\n",
      "Epoch 93: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.3010 - val_accuracy: 0.9477\n",
      "Epoch 94/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 94: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.1581 - val_accuracy: 0.9593\n",
      "Epoch 95/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0326 - accuracy: 0.9913\n",
      "Epoch 95: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 0.2131 - val_accuracy: 0.9651\n",
      "Epoch 96/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9964\n",
      "Epoch 96: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.2882 - val_accuracy: 0.9302\n",
      "Epoch 97/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9985\n",
      "Epoch 97: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.2528 - val_accuracy: 0.9419\n",
      "Epoch 98/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 98: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.2174 - val_accuracy: 0.9593\n",
      "Epoch 99/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9985\n",
      "Epoch 99: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.1925 - val_accuracy: 0.9651\n",
      "Epoch 100/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9753\n",
      "Epoch 100: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0998 - accuracy: 0.9753 - val_loss: 0.2133 - val_accuracy: 0.9244\n",
      "Epoch 101/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9797\n",
      "Epoch 101: val_loss did not improve from 0.11801\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0625 - accuracy: 0.9797 - val_loss: 0.3882 - val_accuracy: 0.9302\n",
      "Epoch 102/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9971\n",
      "Epoch 102: val_loss improved from 0.11801 to 0.10895, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet102-0.9767.hdf5\n",
      "276/276 [==============================] - 37s 129ms/step - loss: 0.0191 - accuracy: 0.9971 - val_loss: 0.1090 - val_accuracy: 0.9767\n",
      "Epoch 103/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9942\n",
      "Epoch 103: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0201 - accuracy: 0.9942 - val_loss: 0.3299 - val_accuracy: 0.9302\n",
      "Epoch 104/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9876\n",
      "Epoch 104: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0459 - accuracy: 0.9876 - val_loss: 0.2496 - val_accuracy: 0.9360\n",
      "Epoch 105/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0510 - accuracy: 0.9825\n",
      "Epoch 105: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0510 - accuracy: 0.9826 - val_loss: 0.4094 - val_accuracy: 0.8953\n",
      "Epoch 106/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 106: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.1640 - val_accuracy: 0.9651\n",
      "Epoch 107/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9833\n",
      "Epoch 107: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.1492 - val_accuracy: 0.9767\n",
      "Epoch 108/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9978\n",
      "Epoch 108: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.1733 - val_accuracy: 0.9826\n",
      "Epoch 109/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9985\n",
      "Epoch 109: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.2468 - val_accuracy: 0.9477\n",
      "Epoch 110/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 0.9978\n",
      "Epoch 110: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0049 - accuracy: 0.9978 - val_loss: 0.1902 - val_accuracy: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9985\n",
      "Epoch 111: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.1364 - val_accuracy: 0.9709\n",
      "Epoch 112/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9782\n",
      "Epoch 112: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0872 - accuracy: 0.9782 - val_loss: 0.4604 - val_accuracy: 0.9244\n",
      "Epoch 113/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9796\n",
      "Epoch 113: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0642 - accuracy: 0.9797 - val_loss: 0.3254 - val_accuracy: 0.9186\n",
      "Epoch 114/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9978\n",
      "Epoch 114: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.1367 - val_accuracy: 0.9651\n",
      "Epoch 115/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9898\n",
      "Epoch 115: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0278 - accuracy: 0.9898 - val_loss: 0.1864 - val_accuracy: 0.9477\n",
      "Epoch 116/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9891\n",
      "Epoch 116: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0316 - accuracy: 0.9891 - val_loss: 0.3381 - val_accuracy: 0.9419\n",
      "Epoch 117/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9964\n",
      "Epoch 117: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0186 - accuracy: 0.9964 - val_loss: 0.1816 - val_accuracy: 0.9535\n",
      "Epoch 118/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9906\n",
      "Epoch 118: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0211 - accuracy: 0.9906 - val_loss: 0.1995 - val_accuracy: 0.9419\n",
      "Epoch 119/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9964\n",
      "Epoch 119: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0162 - accuracy: 0.9964 - val_loss: 0.3037 - val_accuracy: 0.9186\n",
      "Epoch 120/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9978\n",
      "Epoch 120: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0152 - accuracy: 0.9978 - val_loss: 0.1522 - val_accuracy: 0.9535\n",
      "Epoch 121/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9906\n",
      "Epoch 121: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.1263 - val_accuracy: 0.9709\n",
      "Epoch 122/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9956\n",
      "Epoch 122: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0150 - accuracy: 0.9956 - val_loss: 0.3205 - val_accuracy: 0.9302\n",
      "Epoch 123/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9985\n",
      "Epoch 123: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.1968 - val_accuracy: 0.9535\n",
      "Epoch 124/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9869\n",
      "Epoch 124: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0380 - accuracy: 0.9869 - val_loss: 0.8270 - val_accuracy: 0.8605\n",
      "Epoch 125/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9876\n",
      "Epoch 125: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0320 - accuracy: 0.9876 - val_loss: 0.2471 - val_accuracy: 0.9360\n",
      "Epoch 126/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9775\n",
      "Epoch 126: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0903 - accuracy: 0.9775 - val_loss: 0.2147 - val_accuracy: 0.9419\n",
      "Epoch 127/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9956\n",
      "Epoch 127: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0126 - accuracy: 0.9956 - val_loss: 0.1897 - val_accuracy: 0.9767\n",
      "Epoch 128/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 128: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 0.1273 - val_accuracy: 0.9826\n",
      "Epoch 129/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9978\n",
      "Epoch 129: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0051 - accuracy: 0.9978 - val_loss: 0.1763 - val_accuracy: 0.9709\n",
      "Epoch 130/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9956\n",
      "Epoch 130: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0095 - accuracy: 0.9956 - val_loss: 0.2668 - val_accuracy: 0.9302\n",
      "Epoch 131/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9949\n",
      "Epoch 131: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0119 - accuracy: 0.9949 - val_loss: 0.1332 - val_accuracy: 0.9826\n",
      "Epoch 132/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 132: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9651\n",
      "Epoch 133/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 133: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1913 - val_accuracy: 0.9593\n",
      "Epoch 134/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 5.8337e-04 - accuracy: 1.0000\n",
      "Epoch 134: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 5.8337e-04 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9593\n",
      "Epoch 135/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 135: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1741 - val_accuracy: 0.9709\n",
      "Epoch 136/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9797\n",
      "Epoch 136: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0801 - accuracy: 0.9797 - val_loss: 1.9446 - val_accuracy: 0.7326\n",
      "Epoch 137/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0430 - accuracy: 0.9891\n",
      "Epoch 137: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0431 - accuracy: 0.9891 - val_loss: 0.2375 - val_accuracy: 0.9593\n",
      "Epoch 138/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9949\n",
      "Epoch 138: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.2606 - val_accuracy: 0.9419\n",
      "Epoch 139/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9891\n",
      "Epoch 139: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0319 - accuracy: 0.9891 - val_loss: 1.5143 - val_accuracy: 0.7791\n",
      "Epoch 140/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9942\n",
      "Epoch 140: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.1158 - val_accuracy: 0.9709\n",
      "Epoch 141/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 141: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1370 - val_accuracy: 0.9709\n",
      "Epoch 142/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9985\n",
      "Epoch 142: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 125ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.1387 - val_accuracy: 0.9593\n",
      "Epoch 143/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0390 - accuracy: 0.9891\n",
      "Epoch 143: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0390 - accuracy: 0.9891 - val_loss: 0.1111 - val_accuracy: 0.9709\n",
      "Epoch 144/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9775\n",
      "Epoch 144: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0736 - accuracy: 0.9775 - val_loss: 0.2438 - val_accuracy: 0.9302\n",
      "Epoch 145/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9935\n",
      "Epoch 145: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0175 - accuracy: 0.9927 - val_loss: 0.1398 - val_accuracy: 0.9651\n",
      "Epoch 146/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9935\n",
      "Epoch 146: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0331 - accuracy: 0.9935 - val_loss: 0.2104 - val_accuracy: 0.9535\n",
      "Epoch 147/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9942\n",
      "Epoch 147: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0175 - accuracy: 0.9942 - val_loss: 0.3549 - val_accuracy: 0.8953\n",
      "Epoch 148/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9993\n",
      "Epoch 148: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 124ms/step - loss: 0.0104 - accuracy: 0.9993 - val_loss: 0.2482 - val_accuracy: 0.9302\n",
      "Epoch 149/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9920\n",
      "Epoch 149: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0210 - accuracy: 0.9920 - val_loss: 0.2592 - val_accuracy: 0.9593\n",
      "Epoch 150/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9985\n",
      "Epoch 150: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.3430 - val_accuracy: 0.9360\n",
      "Epoch 151/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 7.5866e-04 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 7.5866e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9477\n",
      "Epoch 152/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.9302\n",
      "Epoch 153/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9971\n",
      "Epoch 153: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0074 - accuracy: 0.9971 - val_loss: 0.2211 - val_accuracy: 0.9419\n",
      "Epoch 154/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2544 - val_accuracy: 0.9360\n",
      "Epoch 155/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9862\n",
      "Epoch 155: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0363 - accuracy: 0.9862 - val_loss: 1.3961 - val_accuracy: 0.7209\n",
      "Epoch 156/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9818\n",
      "Epoch 156: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0636 - accuracy: 0.9818 - val_loss: 0.5941 - val_accuracy: 0.8779\n",
      "Epoch 157/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9855\n",
      "Epoch 157: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0433 - accuracy: 0.9855 - val_loss: 0.2356 - val_accuracy: 0.9535\n",
      "Epoch 158/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0300 - accuracy: 0.9898\n",
      "Epoch 158: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0300 - accuracy: 0.9898 - val_loss: 0.1326 - val_accuracy: 0.9593\n",
      "Epoch 159/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9956\n",
      "Epoch 159: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0200 - accuracy: 0.9956 - val_loss: 0.1092 - val_accuracy: 0.9651\n",
      "Epoch 160/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9964\n",
      "Epoch 160: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0157 - accuracy: 0.9964 - val_loss: 0.1686 - val_accuracy: 0.9477\n",
      "Epoch 161/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9971\n",
      "Epoch 161: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0086 - accuracy: 0.9964 - val_loss: 0.2389 - val_accuracy: 0.9535\n",
      "Epoch 162/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9847\n",
      "Epoch 162: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0575 - accuracy: 0.9847 - val_loss: 0.1721 - val_accuracy: 0.9477\n",
      "Epoch 163/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9971\n",
      "Epoch 163: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0081 - accuracy: 0.9971 - val_loss: 0.2584 - val_accuracy: 0.9477\n",
      "Epoch 164/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9985\n",
      "Epoch 164: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0032 - accuracy: 0.9985 - val_loss: 0.2131 - val_accuracy: 0.9593\n",
      "Epoch 165/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 5.9109e-04 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 5.9117e-04 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9593\n",
      "Epoch 166/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2243 - val_accuracy: 0.9651\n",
      "Epoch 167/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9993\n",
      "Epoch 167: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.2005 - val_accuracy: 0.9593\n",
      "Epoch 168/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0499 - accuracy: 0.9825\n",
      "Epoch 168: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0499 - accuracy: 0.9826 - val_loss: 0.4842 - val_accuracy: 0.8895\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9956\n",
      "Epoch 169: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0106 - accuracy: 0.9956 - val_loss: 0.4059 - val_accuracy: 0.9244\n",
      "Epoch 170/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9985\n",
      "Epoch 170: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.1164 - val_accuracy: 0.9593\n",
      "Epoch 171/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9942\n",
      "Epoch 171: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0244 - accuracy: 0.9942 - val_loss: 0.2145 - val_accuracy: 0.9419\n",
      "Epoch 172/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9971\n",
      "Epoch 172: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 35s 124ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.1857 - val_accuracy: 0.9535\n",
      "Epoch 173/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9927\n",
      "Epoch 173: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0251 - accuracy: 0.9927 - val_loss: 0.9524 - val_accuracy: 0.8953\n",
      "Epoch 174/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 174: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.1697 - val_accuracy: 0.9651\n",
      "Epoch 175/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9993\n",
      "Epoch 175: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0088 - accuracy: 0.9993 - val_loss: 0.1287 - val_accuracy: 0.9709\n",
      "Epoch 176/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9876\n",
      "Epoch 176: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0511 - accuracy: 0.9876 - val_loss: 0.2192 - val_accuracy: 0.9477\n",
      "Epoch 177/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9978\n",
      "Epoch 177: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0059 - accuracy: 0.9978 - val_loss: 0.2742 - val_accuracy: 0.9302\n",
      "Epoch 178/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9985\n",
      "Epoch 178: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 0.1841 - val_accuracy: 0.9593\n",
      "Epoch 179/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9993\n",
      "Epoch 179: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.2771 - val_accuracy: 0.9477\n",
      "Epoch 180/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1869 - val_accuracy: 0.9709\n",
      "Epoch 181/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 8.2704e-04 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 8.2652e-04 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9593\n",
      "Epoch 182/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9869\n",
      "Epoch 182: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0621 - accuracy: 0.9869 - val_loss: 0.2398 - val_accuracy: 0.9535\n",
      "Epoch 183/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 183: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.1766 - val_accuracy: 0.9477\n",
      "Epoch 184/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9985\n",
      "Epoch 184: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0031 - accuracy: 0.9985 - val_loss: 0.1441 - val_accuracy: 0.9767\n",
      "Epoch 185/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1679 - val_accuracy: 0.9709\n",
      "Epoch 186/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 8.5975e-04 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0066 - accuracy: 0.9993 - val_loss: 0.1814 - val_accuracy: 0.9709\n",
      "Epoch 187/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9789\n",
      "Epoch 187: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0785 - accuracy: 0.9789 - val_loss: 0.7108 - val_accuracy: 0.8953\n",
      "Epoch 188/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9985\n",
      "Epoch 188: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.1386 - val_accuracy: 0.9709\n",
      "Epoch 189/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9971\n",
      "Epoch 189: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0075 - accuracy: 0.9971 - val_loss: 0.2202 - val_accuracy: 0.9477\n",
      "Epoch 190/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0046 - accuracy: 0.9993 - val_loss: 0.2641 - val_accuracy: 0.9593\n",
      "Epoch 191/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9935\n",
      "Epoch 191: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0275 - accuracy: 0.9935 - val_loss: 0.2222 - val_accuracy: 0.9593\n",
      "Epoch 192/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9964\n",
      "Epoch 192: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.2418 - val_accuracy: 0.9651\n",
      "Epoch 193/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9964\n",
      "Epoch 193: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 127ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.2266 - val_accuracy: 0.9709\n",
      "Epoch 194/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0462 - accuracy: 0.9862\n",
      "Epoch 194: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0465 - accuracy: 0.9862 - val_loss: 0.4276 - val_accuracy: 0.9012\n",
      "Epoch 195/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9978\n",
      "Epoch 195: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.4467 - val_accuracy: 0.9302\n",
      "Epoch 196/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9964\n",
      "Epoch 196: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0123 - accuracy: 0.9964 - val_loss: 0.2544 - val_accuracy: 0.9477\n",
      "Epoch 197/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 197: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 126ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.3792 - val_accuracy: 0.9360\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9956\n",
      "Epoch 198: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.2697 - val_accuracy: 0.9535\n",
      "Epoch 199/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9993    \n",
      "Epoch 199: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0063 - accuracy: 0.9993 - val_loss: 0.2797 - val_accuracy: 0.9477\n",
      "Epoch 200/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0446 - accuracy: 0.9876\n",
      "Epoch 200: val_loss did not improve from 0.10895\n",
      "276/276 [==============================] - 36s 125ms/step - loss: 0.0446 - accuracy: 0.9876 - val_loss: 0.2429 - val_accuracy: 0.9535\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "history=model.fit(\n",
    "    training_set,\n",
    "    epochs=200,\n",
    "    validation_data=validation_set,\n",
    "    batch_size=5,\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AlzqG49vJoAQ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model,load_model\n",
    "model=load_model(r\"C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\MobileNetV2_Checkpoints\\MobileNet102-0.9767.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "psn8PMk8JoAR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]], shape=(172, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ar=np.empty(0)\n",
    "for im,y in test_set:\n",
    "    ar=np.append(ar,y)\n",
    "yt=np.zeros((172,5))\n",
    "count=0\n",
    "for i in range(0,172):\n",
    "    for j in range(5):\n",
    "        yt[i][j]=ar[count]\n",
    "        count+=1\n",
    "yt=tf.convert_to_tensor(yt,dtype=tf.float32)\n",
    "print(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "odSm3QFaJoAS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 20s 480ms/step\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "yp=model.predict(test_set)\n",
    "arr=np.zeros(yp.shape)\n",
    "for i in range(yp.shape[0]):\n",
    "    for j in range(yp.shape[1]):\n",
    "        c=yp[i].argmax()\n",
    "        arr[i][c]=1\n",
    "yp=arr\n",
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0RNi77WUJoAT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.9476744186046512\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1 Score= 0.9836065573770492\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(yt,yp)\n",
    "print('Accuracy=',accuracy)\n",
    "precision=precision_score(yt,yp,average=None)\n",
    "print('Precision=',precision[precision.argmax()])\n",
    "recall=recall_score(yt,yp,average=None)\n",
    "print('Recall=',recall[recall.argmax()])\n",
    "f1=f1_score(yt,yp,average=None)\n",
    "print('F1 Score=',f1[f1.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABZbklEQVR4nO2dd3hUZfbHvyedJPQaOkpAAQUUBQs2RLGia1ls2NtPESy7Yu+uuuraF1BZ+1pXRQVRWVkrUhSpYigBAgFiAiRAes7vj3Nf7p2ZeyczSWYmyZzP88wzM7e+c+fe833POW8hZoaiKIoSvyTEugCKoihKbFEhUBRFiXNUCBRFUeIcFQJFUZQ4R4VAURQlzlEhUBRFiXNUCJS4gIh6ExETUVII215CRN9Fo1yK0hhQIVAaHUSUS0QVRNTBb/kvljHvHaOiKUqzRIVAaaysA3Ce+UJEBwBIj11xGgeheDSKEi4qBEpj5XUA4x3fLwbwmnMDImpNRK8RUQERrSeiO4kowVqXSESPE9EfRLQWwCku+75MRPlEtImIHiSixFAKRkTvEdEWItpJRN8Q0UDHuhZE9IRVnp1E9B0RtbDWHUlEPxDRDiLaSESXWMvnEtEVjmP4hKYsL+g6IsoBkGMte9o6RjERLSKikY7tE4nodiJaQ0Ql1voeRPQ8ET3h91tmENGNofxupfmiQqA0VuYBaEVE+1sGehyAN/y2eRZAawD7ADgaIhyXWuuuBHAqgKEAhgE422/fVwBUAehrbXMCgCsQGrMAZAPoBOBnAG861j0O4GAAhwNoB+CvAGqIqJe137MAOgIYAmBxiOcDgDMADAcwwPq+wDpGOwBvAXiPiNKsdTdBvKmTAbQCcBmAPQBeBXCeQyw7ADje2l+JZ5hZX/pqVC8AuRADdSeAvwEYA+BLAEkAGEBvAIkAKgAMcOx3NYC51uf/ArjGse4Ea98kAJ0BlANo4Vh/HoCvrc+XAPguxLK2sY7bGlKxKgUw2GW72wB86HGMuQCucHz3Ob91/ONqKcd2c14AqwCM9dhuJYDR1ufrAcyM9f+tr9i/NN6oNGZeB/ANgD7wCwsB6AAgGcB6x7L1ALpZn7sC2Oi3ztDL2jefiMyyBL/tXbG8k4cAnAOp2dc4ypMKIA3AGpdde3gsDxWfshHRLQAuh/xOhtT8TXI92LleBXAhRFgvBPB0PcqkNBM0NKQ0Wph5PSRpfDKA//it/gNAJcSoG3oC2GR9zocYROc6w0aIR9CBmdtYr1bMPBC1cz6AsRCPpTXEOwEAsspUBmBfl/02eiwHgN3wTYR3cdlm7zDBVj7grwDOBdCWmdsA2GmVobZzvQFgLBENBrA/gI88tlPiCBUCpbFzOSQsstu5kJmrAbwL4CEiamnF4G+CnUd4F8ANRNSdiNoCmOzYNx/AFwCeIKJWRJRARPsS0dEhlKclREQKIcb7YcdxawBMB/AkEXW1kraHEVEqJI9wPBGdS0RJRNSeiIZYuy4G8CciSieivtZvrq0MVQAKACQR0d0Qj8DwEoAHiCibhAOJqL1VxjxIfuF1AB8wc2kIv1lp5qgQKI0aZl7DzAs9Vk+A1KbXAvgOkvScbq17EcBsAL9CErr+HsV4ACkAVkDi6+8DyAqhSK9BwkybrH3n+a2/BcBSiLEtAvAogARm3gDxbG62li8GMNja5x+QfMdWSOjmTQRnNoDPAfxulaUMvqGjJyFC+AWAYgAvA2jhWP8qgAMgYqAoIGadmEZR4gkiOgriOfViNQAK1CNQlLiCiJIBTATwkoqAYlAhUJQ4gYj2B7ADEgJ7KqaFURoVGhpSFEWJc9QjUBRFiXOaXIeyDh06cO/evWNdDEVRlCbFokWL/mDmjm7rmpwQ9O7dGwsXerUmVBRFUdwgovVe6zQ0pCiKEueoECiKosQ5KgSKoihxjgqBoihKnKNCoCiKEudETAiIaDoRbSOiZR7riYieIaLVRLSEiA6KVFkURVEUbyLpEbwCmVnKi5Mg0/1lA7gKwD8jWBZFURTFg4j1I2Dmb4iod5BNxgJ4zRr4ah4RtSGiLGuseEUJoKwMeOUV4JJLgLS02rYOZPlyYMMGIDUV2GcfoFcvwJ6gLHRqaoCdO+3vmZlAcnL4x2EGSkuBdGtKmsrKuh3H7bhAeL+tqgqYOhXYutV3eb9+wIUXeu9XVCT/yYgRwPDhQGKi97Y7dgDFxUDPnlLGX38FvvpKlmVlAZdeCuTkAEuXAuedV7f/pqAAmD0b+P13ICEBOOQQ4Nhj7WtsfmuSh+UrKgJyc4GDHPGJykpg3TopW06ObJOSAlx3HdC2rWxjfs+cOUBGBnDYYcBga5Dxmhopy65dwOrVwJAh4f2moiLZt2fP2retM5GcBxMye9Myj3WfAjjS8X0OgGEe214FYCGAhT179mSlebNrF/OsWYHL33yTGWC+/XbmbduY77uPeedOe31pKfP27e7H3LOHuUUL2d+8LryQubrauxxvvsl88cXM113HvHmzbPvss8z77ut7nP79mWtq7P2efpr5ww99lzEz/+tfzF9/LZ9vvJG5TRvZv21b5nbtmBMSmH/6yd7+xReZL73U9zj//S9zUZF8/uQT5vPOYx49Wq5DdTXzyy8zd+nCfP31gb+npIQ5N5d5wwbmyy5jPv10OcYff8hxAGYi+2V+37Zt7tentJT58MPt7c491/tabtzIvM8+zC1bMq9bx3zFFfZ+5lxZWXINAOZFi9yPs2MH8+zZcm2mTGH+9VdZ/vnnzMOH+5bbvAYOlGtTUcH8+OPMrVoxn3MOc34+8733yr22Zw/z2Wfb5//6a+bff2c+8EDmxMTAYwLM99wj587LYz7uON91qalyvV9/nTkpSa5T69ay7sUXmQsLmadPl9+wcqUc59575b984AG5P267jXnaNOYOHZg7dmTevdv7+oYCgIXsZau9VjTEq6GEwPk6+OCD63c1lHrhb9zcWLKEuaoq+DbTp8uD5sakSXJn/vKL7/LrrpPlSUlifAHmf/7TXn/ZZczduzMXFwce85tvZPt//IN5zhzmm26S75Mny/oNG5gvuUQeXmbmZ56R9Z06MaekMA8dKusBeaj//nfmp55ivuACWbZuney3bp1tDE44wX54i4qYk5OZDziAuaBAfsMxxzA/+CDzNdcwX321GKG77rLLPGiQHOfll+X7hx/K94MPFgNiypeQwDx+vLwAMTgpKcxbtvheg9GjfQ1VVpav8XrkEd/tf/hBlr//vnwvLGQeMYL51luZKyvFcAIicDfeKJ+//Va2rakR4/z3vzNfdRVzjx5igFu2lP8IYL7hBruMc+YwjxrFfO21UvZJkwL/w7w8e1/z6tmTuaxMlvfuLZWDBQvE8JeVMT/2mGz3/ffMd9whn41gGKOfkCDXmoj5L38Rwzt2LPOf/8ycmSn7vfKKHGPbNvlto0fLudevFyOdni731ubNzO+9J8edPZv5jDNE5A85hHncOBGMxERZZn5DdrYIZXKyVAoAqbQkJcnnPn3kfdq0wGsSDo1VCKYCOM/xfRWArNqOqUIQOz7/XB7kt9/23mbWLLmrTjtNavZuFBTw3prw3LmybMMG5v/8Rx6ktDRZf/fdvvsNHcp80EHM7dvLNm3bMp91lqyrqLBr2LfdFnjORx+VdVu3yveaGhEOQB7CF16Qz2+9ZYvGmWfKcWfOtGuF/mVauFCWm2tijjN5shiWiy+Wc730kv3gX3utvC9e7HusYcOYjzpKPuflyTYpKfI733lHDFSfPrYBO/ZY5vJy5jvvtI99zz3Mv/0mn++7zz72zp1iWE49VWqca9fKb/vsMzGW774beM0qKsTATZggRnXkSPs8gwfL+5NPyra7d4uwHHGE/N4vv7S3bd+e+cgjmefNkwqAEUmvysKf/iQCV1npW/4hQ8Qwz5ghns1rr8mxjAD+5z+Bx9qxQwzspEnMnTuLF8TM/NFH4sF8950YfSIRNGYx/MYrMhUFf95+W87Zr58Y7aVL7XUlJXK/TJ4s9+Tll/v+jsMPZz7+ePH+zG8YOlTOl5Nje3c7d4qwlpbK9R44MLSKmBeNVQhOATALMuH2CADzQzmmCkFofPUV8/z5DXvMyZPth9s8NMxSsx8/nnnTJjGeGRlirI44QgyVP7/8IsdIT5f3I4+0wzYdO8q+/fpJ7dmwa5c8XHfeybxsmXgdl10mRrKqSlx5U3tKSWG+5RZxyw1nnCEhHSdffSX7zJkjNUFAQiSXXy4Gx+mKf/CBeB/+D2JFhYiSqcGeeqqEQGpqxCgDEk4aNUpqxcnJsmy//QKPdcstUvY9e2yD+c47UpM2tfjly2XdySfbYbCKCgl1PPqofayTThIDfOqpzFOn2t6EEd5QGT1awiO33y77v/mmGGo3D+Kf/5TlX3whoZ/MTFt4DTU1UllwhvT8MWX97DP5vnGjHaJxhgyrqsQLAJi7dvUVDicnnmhf9xkzAtdXV0sFxJCXJ6KZkSGVFjdKS+3au/O6G4YPF+E218yL6mq5zwHmU07x3s7cD1995b1NbcRECAD8G0A+gEoAeZAJua8BcI21ngA8D2ANZI7XWsNCrEIQEjU1UvsZNSr0fdavlxqbk40bpdZiwh5/+pMYuWHDbCO9a5fUVACp5SQliUF76y3e6/7788knsu7zz6V22q+fuM1PPCFG9bLLpKYJMK9eLfsYQ2+MA7N9jvnzJdSTkiK14f79pRyJiRL/Ntfjoot8y2HCOC++KJ4FIDW4Nm0kfxAqRxzBfNhhYhxatLDj81VV4hmZOPidd4ogAcyPTi6y41B+1+XrryUskZUlZd+5U/6HZctCL9M339ix5fR0CeNkZlrC7G+d9+yRuI8LDz7Ie0MV558vy8rLA70ZZvEaOncWw9u+vYhqXSgvF4+gRw8Rsc6dRQy/+CJw28cfZ594vRsmjNali7dYuB33pZeCb/PAA+KVVVQErrv1VrvS5BQZN4zw/fe/3tuUljIfeijzxx/XWnRPYuYRROKlQlA7v/4q/6x/DdiLlSvlYUtPt2/q116za1GpqeJiDxokhs08fLm5YrSJbEMKSK2V2Y71Gzc7JUXipubB3LgxsCxFRVKGtWtlm7//XZY/9JB8d9qrrVtl2UMPSZz1xBPtdfPny7pXXrGP9cILvueqqpLfOHmyhJycyWSn4NTGzTfLNZoxQ/adOdNeV1oqtfOEBOYVK+T392+xnis7ZQVkV7dvl+t0441iSC++OPQyeLFihZ1APf10tpMfzjjKhAnMffu6xh2+/VY2T0piXrOm9vMZL8grVBMqP/9s5zAGDvQWwZIS+f88dIyZJSmcnOweMowUM2fyXs8vFPLzI1seZhWCuOOJJ+SfTU6uPWlbVCQPnImBL1zI/O9/894Y9COPyOcvv5Ta+s03M69axXtj3UTiAVRUSAz36JHVEnCurOSKCgmL3HWXvBITmR+7YSO/OP4bTkiovXZ22GFStu3bxdvo3z9wGxOrBpiff95eXlMjCcQzzmB+4w1Z7598ZhYBOecc8QIuukjEqn1791qeF+++K8fv0EFqrmWz/uuTqa2ocCTGd+7kGuNCufyggw6yf49bGKMujBsnx/v0mhm2Khx+uL3BCSfIMtMEx0FZmVyPiRNDO5cxuhkZ4mjUh40b5T+t73GYRRDLyup/HE/WrxcX16K4WK7DdddF8JxhokLQjJk0KdBdPOkk25hs2GAv37BBaqSmuRoz88MPy3bvvy/vzz0ndiE7W1z0P/6Q5abFztSpsl+/fvLdGUfdtYt5z6dzLKvzaUBZ992X+dPsSVyWlM49utaiUCyilJAgZQGkeZ0/s2dLDXrq1ECDcf31Usvv21c8HjfhGTNGwl3G+7jjDmn+GQ4bNthC8OM3FWIBbr3VfeP77pONR46U7fwK9eWXkpD2D9PVh5wcEdLyg0dIFdW4VwsWyAYmSP23v7nuX1RUe4XCyYMPyiuuMO1h33hj76Iff5Tnp7GgQtBMMTFuZ+WurExCPH/p8iqfhM/4m28kiWvaiRuDtX27hC06dxZjWFMjMdSzz/ZNfjJLArZjR94bv2YWLwBwsXf/+AcHVM8txoxh/qjdpcwAn3WgR9tRP0yzxKuvDt7m3w2TDE5NlaZ/bhiBAyQhXFc++sjKpaxfLwcbNy5wo/JyuchjxtjNiEy8pbhYEiqmk0C4PP643XbTi7Ztpa3qjh2SMDBJE5PVHDnSd/sFC5jvv9/7eM88Iy7XJZdILWDbNgkz+eU+mhSrVslN4eWGfPyxxDb9OfRQ3uuGjx1rx6F++02SQ14379NP1y+GFgYqBE2EkhJpdZOTE9r2JtxLZHf6mTtXlhW36sqf4BR+7TVpu5yWJvfmm2/K9pMm2YlAk6QaO9ZumuhsnXHOObaxzMuTZStXSsuVgFYVpk2mS0B2wgTmd5MkTvHo8NCsbnm5tOoJVwSYJSQzfnzwBJsJowESl64333/Pe5tC+WNiVLNmMf/vf7w3Y84szbBMM6Fw2bJF9g2W4Tau3RNPyPdx46TJTXm5LE9Pl9idU4iOOELWebVZ7NXL7iX17LN2htSZJGlKbN1qN9p3hHl8GDZMjL0zA1xdLa7x+PGSROvSRY5RVmY/ZCtWBB7r2WdlXTitOupBMCHQ0UcbEfPmAR9+CLz+ur1s3TrgvfeA6urA7T/5BGjZUszYzJmy7JtvgHYoQsvizUhGJVatAr7/XrrDP/wwcP75wJVXAk89Bdx5J3DiicAxx8i+w4dLd/jUVOCokQyceirw0UcYNkzWp6cDXbvK5/32Az77DOjQwa9Qy6wxBvPyAsqbnQ0kV5UCAAbBbyzC2bOlIFVVPotTUoDjjpMu+uGSnAy8+ipw+umOhbNmASNHAuXlAIB997VX7bOPx4GY5QJee23guoceAsaOtb9v2uT77mTKFKB/f+CEE+RiADJmAWD/gc7r9s9/Ahdd5Pn79jJ7trzn5npvY85jzrvvvsDGjcDmzfL9nHPkJnvzTfm+aJHcOICMseBGeTlw7rly4zz1FPDii3Y58vOBoUNlvIiGoqJCrsff/lb7tmvWAIceCrz9dujHv/RSYMsWGdti3rzA9aWlwOLFcj2mTLGX5+YCu3fLfTVjBnDXXbJ8xw5g+3b57H+8Tz4BJk6Uz0VFoZcxQqgQNCJWrJD3//7XXnbTDVUYf24phg4FfvrJXl5cDMydC1x1lRjnTz6R5d9/D5zWR4xsZkoFPvxQntfDDrP3ffBB4LTT5Ln97DN7TJcRI+R95EggvaxIVn755V4h6NcvyPgvu3eLiixfLt9dDGF2NpCGMgDAPqV+QjBtGvC//wUOdtPQvPQS8N13opiwhaBtW6B1a4997r4beOEF4P33fZe/+KKo6YwZe4VlryHftEmuh5NVq2Tgm4QEoEsXGaQoJ0fE74sv7P0Mn30GvPGGbay9MCKy3nNK2kAh6NVLDP+iRfL9zDNFcW+6CfjoI+Cxx+x9KytFDHft8j1mRYUo9cSJYniNQVu/HvjhBzGajz4aWJbiYjleODADV18t1+Mf/wi8tobNm2XQn5NPBhYsAMaPlwfFya5d7jWrhQuBCy4ABg70fdjKy0UEfvlF/qv27UUIzH9uKj8HHCDvZgCi7dttIfjpJ/kNa9fKfz1unAxodNZZQGFheNciAqgQNCKMEPz0k9jV4mJg6KyHsTJjGHbsAI44AnjySdnmiy/k+Rw7Voz67NnAnj3y/J3YzRKC1Mq9x3QKQceOYruuuMJ3kLBDDpF7+JxzYBuV/HwcfLB87NfPo+BPPy07vvmmFDwhwdMjaAHxCLL+cNQUKyqAL7+Uz9u2hXax6oLzPJbxNF6Apzfw7beinJ07A3/8IbU8QAz2//2fGAVAateA/bsrKmR7A7MYSrM9EdC3rxjoH3+0R7FzXjfz+fPPvX9TVZX8+URSpooK9+1ycuR/MT+0d295//FHee/WDfjgA/mTzjwTePddESpAbrQ33gC6d7cNGyCGMDVVjFnXruIB7Luv1JCN8Lz7rngHhj/+kBHmwqmpA/IfvPIKcPjhMrKcETAnS5YAPXrIqG7r1wOffirlufhie5uaGvmNjzziu6/5fzp2lBqRMdyAGO0xY+xa/dNPy3364IPy3Xg9AwbIu5sQzJsnlZB99xU3vGNHqb316KEegeLL8uUyqmZlpdTsP/0UGFS9GD2qc7FkidyLN98s3uusWUCbNmLgzzlHKjl33QWUlAAHpciNmZksRqFXLzukE4zMTLElV14JO8ywZQtatwZuuUUqVwF89BFw441S6JtukmXDh4sR86v19eoFpFtC0Gprjpzsu+/kVVIiG0VSCL7/Xs7TuvVeIUhPF/vWt6/HPv/4B9CunbwDvqGcqirg/vvlu7leboYcEFWvrpZjGbKz5XgzZ8pwmAcd5L6/qfED4jUVF9vf580TcTrpJDFyeXlSQ/X3yHJy5A9ISZHvRgiMcevSRW6ob74RI/3OO8Dtt8u6igo57s6dtucC2EKQkiK17o8/luMaIcjIkGv0T8cI84sXS43FCHIovPmmeGXjx8s5iMRbWrUK+O03e7unnpIH6PXXpXZ/yinycGzcaHsAeXnyAL33nnxftEi80JISu7Y/fLgY59WrZZsffpDrMm2aXMPzz5cw0oMPSuxx2TL53S1byvZuQrB0qXhHBx4IvPWWXPcuXeR8u3Z5C/j334s3wgw8+6yvEDckXsmDxvpqrsnimhpp1HHBBdJ559ZbJXH8c/IhXJOQwFxTwz//LLmlf/1LutSffba974ABdhPx0mFHMgO8udNgzwYstWKyqL17B99u8GDpaTZxop11vfdeeXdpAfNbyiCuhNVpwfQYGjrU3ve11+pQ2BAxYziYNrNWt+Uff/ToLLVunWTPJ0+WHk2APV7AmWfKqGPO7snMkmBt2ZIDOgKsWSPLXnnFXnb77ZKgbdVK2uxedJEkYJml1Qog61u2lMz30qWy7Npr7WOYpqAffCDvc+bIdb3kEt/fcvDBcg5DWZlsbwZ2cus4MXWq3ULA/Kfjx8u66mr7v3Zy+eWSLB05UhLmo0bJ/WEwXcYHDHC54B707Sutcsx4JYcdJm1+zSh2S5ZIa4nUVGkV5eSpp+R8ph2naUoGSPfotDRpZ2x6HU6fbl/n116zey2a15//LMepqLC70puxPAymo83rr8vzYcZS8WteyszSug5w71E2b56sO/VUu0PP44+Hft38gCaLGz/btonYH3KIVEiee048xz4pm0A1NUBlJYYMEa/6ySclFHriibIvEXDDDXKnZXVhpK6W0FBaoiT5nGGhkDE13Px8u2a/caPEaS+9dG+MHXl5klS4+WaJM/XuDey/v6xbtw647z6fEEnLpFIshRVLLSiQQdt/+UV+NOCdI/jPf2Tw+vowcyZw9NFW7At7a9ojRniEhp5+Wi7u//2fuPREUtM1IaaTThJ3IjHRDqXl5dm/xVm7N+6/v0dQXS0155dekvCMyS2YGv1pp0ltde5c4JlnZNmrr9o1w99/F3fPDHI/d678Z/5hqZwcOz8ASE0+K0smeejQwX0iBLOsosJOGM+aJeUzNVjjYRh695Ya9/Llcr6+fX3/UxNPX7lSPJm//U1+QzC2bZM/yZzr5JMl1p6eLrXwMWOAs88WD+WGG3z3NdfbXH/j0QEy6UFZmVxrs759e7l/MzOl1m7Ka66vSaQlJ0vOqH9/ifGb/ADg6xEUFUleCJDrbe49gwkVmvOXlkreqahIQgKAvE+eLIn5G28Mfq3qiApBI8HE8gcMEC/41FOBC86tROs9Vny1rAxE8gyYkKQRAkAaU7RvD5x56CaQFcdukVSBlBRg9Og6FMgYtvJyO379ySfiHr/+urjhFRXyEHTpIrHOv/5VZo3p1k22nz4duPdeaQpl0TKpFOvbDAZGjZKY79dfiwt/111inLxCQw88AEyaVIcfYjF/vlzkM84Q49Sli4QpvPjgAxGCiy+W35aWJu85ORLK2rVL/oykJBGD3FzbgB90kIiDUwhMQtA8+ICI0vDh8qD36CHHqaqSa2CE4JJLxIBccYXE6UeOlNDKyy/LemPgu3eXHMA778hyZ/iooEC+O4UAsMNDWVnu18AY3spK2/Cb+LxJlKam+u7Tq5e8FxXJ+UxuxYRmli4VA84sInD77cATT7ifH5D9iott4wpIQve44+S6maZrubnScsJUQgzmepvrn5Nj/5crV8qybdvs9e3ayX93+OFybxohmDZNlp12mn1sE2I88ki5hw1t2si7CQ317Suz+zz4YKBwGqEy5//oI2mJ9uSTcuyRI+XeP+ssqQDUpflcCERshjIlPExjm4EDpYJ3wgkANm4B3rJq46WlQKtWOPlksQH77y/3siE9XSowHX9eDswAsM8+SKuqwO7d1mxMBQWS9HrlFd8dvcjNlRows9Qw27Sx4/iHHSaGassW+W4MycMPy7sREdMO1tGaJTOxFGPPbwk856jdm5pP587eQlBRIYZ8/Xrb2ITD008DrVrZzTG7dJFr4s9f/iIP44YNUvt77jl7nYnpf/KJ3a4VkPLk5krZq6pkKqmuXWv3CPr08W1W2L27vOfl2fvut5+c76ij5B544QXg+uulXDffLOUZO1bK07WrXbt2CsH8+Xb5nfTuLcliLyEwHoERguRk+X2zZtn/gb8QGHEBxABu2yb30B9/SIJ0+XKpFb/6qi0AJt9y4YVSu7n0UvsYJjnvFII+fWQqMMOvv7qXH3D3CPr2lZYXU6fKPbd1q69HAIi3d+ONcu3btweGDbOb0zrp2VMaFDhJThaPoqBAKgxt20rFyQ1/j8Dkg55/3vaYJk/2/n0NhHoEjYQlS6SC4fNMOhN+ZdLs8vjjgRYtfCsghr59gdZVVs2iZ0+gstKeku/rr6VdqvNmrqlxb4bHLIZt4ED5blp97NolNZK+fe2kGxBoSLKyRESMMXK0b6eyMlB6C/eL0KmTtxCY/gXOxGkwTFQWkOv47rvAZZfZCb1OnQLDUMzAv/4lynnFFZKYbOEoa3a2JChfe01cNtOqpndvESjzf3XvLi/TiofZ3SPwx3hSmzbZQtCtG3DwwWJ8n3sOGDRIkpXr10tIraDANvBOI2yu/dq18rv32Udqrk7M9l26uJfH1F5NaKh1a7lumzZ5ewTOMmRny/aA/K+5ueLNHHmkCFx1tYRW8vLE4L3zjlx30xYasENgTiEIBzePIDtbjPy994oo+XsEgHh7gIQjDzgg/Hkz27aV0KjzmG44PYLqamkh1r+/LYAnnRTeeeuICkGUeeMN8WadPPOMhIhHjfK735w1ylKrtU0r4Oef5R52xbjwmZm+LRFMPMkYb0A6SY0cGXgMM7msiYeafUpK5Lg9esgy02TS35CkpEhNy2A8Amb5HS3qIAQmRh2qEPz979KcEZBWGlVVUpMOdq4tW+SBvPZaqZF17Oi7PjtbwmRFRXZnIECM36ZN9oNvhGD5cjHADzxg1/iCGTR/j6B1a1tsjjpK/i/A/l/eeMMulymHwXhv11wjv33mTLl5nJhafageQUqKhFXKy71zBF272hMC9+3rKwTmHhw0SMIsycny3wCSS+rZU8JqF1xgH9/NkwoHp0dQXS3CmJ0txvaee+TeLS62+2qY7bOz7U4mgwaFf962beVc5rMXTo9gwQLxnO65R1oXdesm71FAhSCK1NSI/Zg0ya6sLl4sy04/3bdHMQBXIQCkMpWR4XES8wBlZPgKgYl1mtr9li0SY3Lr+WkMtzE4To8gM1MMVk2NHWN3MyTGqB13nO0RmFpkMCHwShYbj2DOHGn6Z4yuF4sX22GSrVsldubsRuwmBP4dg/wxBnfIEF8B7dVLrocJ83TrJq8tW0Qgfv5ZBKZ1a+9Z002ZkpJsITDX0J+BA+X/NUbU2UkMEANsPII1a6RW2b9/4HFqyxE4k8UmNJSWJt6pl0eQmCgVhawsuVecQmCu78CBEi//3//E8A8dKkb6+usldFdSYndeq69H0KaN1K6KiiTcV1HhGyIzFZbffhNv0fxmk5ADvO+HYLRrZ9+jwcqemSn/eWGhiHVCgoTHPvjAt7dnhFEhiCIrV9rNk01y2IQ3H3tMbJUPTiGwQkO1UptHYIz6lClS0ysp8REZALbhHjxYHnw3IQCkBkNkP+xOBg0SIRk50g6RmPPU5hG49TqtrBRDXloqrSfGjPHuXQrIcUpLZZs9ewIvbufOsnz3bnuZMVReNcDBg8XQ/eUvvg+oMaivvmqHTwYMkNryPvvI9Swqqr1Wm5Bg5xY2bfIWgsREaV5mRNMI3JAhcm1POkkMdXm5GFIvQzRwoBg+0xHKH2eyuLLS9giCCYEpxyGHyGdjaLdtEw+pZ08xuFlZdnO2s84Sg3355b7hKKD+QpCYKMcuLAzsXQ3Y9+5vvwWG7c46S/4TUyEKh7Zt7Wc2WNmJ5LxFRRK6PfRQuU/69pX7LUqoEEQRZ07po4/kPSdH7tU+fVx2cOYI/I21F04hMOGU3bttN3XLFnmIp0yxaz/+SVMjBL17ywPrDA21bGnHshcu9G56OGWKxFd79RLDnpdn/4a0NPeyd+ok5XcmOg1VVZJBX71aEr+//x68x62p7ZeWuguBs6b60UciakuXynL/kJChZ08xvuef77vcCEFhod2y4/LLRUBPOkk8rMLC0MIb3btLyC0vz77Obpgmqj162MJ61llyzxhDt3OnhPm8ztu9u/ye4493X+8VGnIKgX9oCJAOYKbncJs2UuPdulVudjfP5NZb5f5s167hhQCwDW0wIcjJCbxORx8t90ddwjPO8tZW9nbt5P5YulQ8pBigQhBFvvlG7OqIEXaLypwcsSNuthR5efZDXhePoKZGXO6VK8UYp6aKcVq82Neg+YdI1q6V/du3lxiql0dQWOgdVkhNlfCFMZK5ubV7BM7aoz+VlXKR9t1X4t5ZWSIIXphj7NkjL/9YmjEAW7dKV+qLL5YHsbYwgFuyt0cPqQE//7w9AF1CgjzgvXqJMV67Nnii2DBihIRM8vO9PQKzHeBr1IjE6JhcgOndHcwQtW3rHX5wGmX/0JC5z9w8ghYt7P84IUGE1QiBf8slQITClDESQmAM7e+/y33gvGfNfVBZ6f7/hPKfuRGOELRvL6GB4uK65SMaABWCKPDBB+L1ffutRErOPFOaYm/c6P1sAJAH2Yx9EK5HYAxfRYUdFjrySDEwpmZkOrr4G15TKCJ5aPyFoF07u1bvJQSGcITAPJSzZ0trDlPrBMQjMPH1lBTp5PXFF9KKx/D668Add4gAGi/HCIGXR/D775KgW7lSPJy6PIjJydJE0210UvP7f/89NI/goYekaSMQXAiMR+B28xghMLmeuhrRUDwCNyHwp1MniYXu3BnkZrdwE4K0NG8vMhScHkHfvr7C5wxr1jUh7Ua4HoEZzkKFoHmyfbtUvI8/3u6Ea5p+fvVVECGoqZGWDGZlOB5BUpL9gFZWSuw7LU1aahQWykPpjH36J2idhXILDRHZYQuvpocG09Fp/frQheD226XXptPIV1X5uk2mP4DpbVxeLgMiPfus/EaTPzB5AC8h8G8bXpfEYDCMEDCHVrtMS5Nmq1df7dtj0J+sLGk6dvnlgesaWghM89H6CIEZJK4uQlAfbwCwPQK3hy0jw74f61r7d8OUuUWL2q+R87wqBM2Td9+Ve3rECLGHo0ZJbq5DB1m3a5fHs1FQIDvWxSNISfF9oFaskB5opob53XcSsjDfnR5BZaXU3k2hunSRh7GszPYIAHvf2jyC5GQRjdxcW8xqEwLT9NE5vn5lpW+Lm549pWxmuOC335bfUVLiOxBZbR7Bd9/J+7nnyntDC4Gz81uoNU4zzHHPnsG3u+ceOynrpKGEwL9nsVurIbccgT+dO9v5qlgIQfv2dj8G//MT2SHJSHgEoZTdnLdbt/r/1joSUSEgojFEtIqIVhNRQPc4IupFRHOIaAkRzSWiIL5w0+T118Xwf/ut3If77y/33tFH2/OJDG7xe2BLmTVr5N206AjHI0hJ8a3N7dghymOM9k8/yQORkSEG0ikE69ZJXsE8MCZxWlRUNyEA7BEpa/MIzCw3phu9MWSm45vTIyCS8Mi8eXLtnn7a3m/BAns7LyFISxODuWKFHGvKFGmX72ZY60PHjpGpcQYjGqGhYDkCf4zoeraKcBApj2D3bvEq3YTIlC8SHkEoZTfnbehKSBhETAiIKBHA8wBOAjAAwHlE5N9O7XEArzHzgQDuBxDC1ENNhzVrJPIwfrzdvNpw9NFivwZjMY66sn/gDEYmjm9aLNTVI6istIcLNmEcZ1tq//b0/i0rTG3ljz9ECEzPXCMEtYWGAKnZbthQuxCkpMi2F10k2xiPwPQh8G+DP2KElHfGDOlle8UVstwMqQB4CwFgG4CePeWBveCChm+3TWSHhxqyxhmMhvYIGiI0BARpFeFxTqDhPAJDMCGItUcQo7AQEFmP4FAAq5l5LTNXAHgbwFi/bQYAMPNxfe2yvkkzdapUUi+4IHDdMcfI+4EJ1iBDztEiAbtdqRlEK5hHsHMn8O9/y2e30FBZmTzAztq7eSD8x/fxFwLzEJkWKMYjMDmCUDwCk6yrrfkoIN7KlCn2+D2AtxCYhOn//Z88cHfeKd9D8QgA2wDUFq6oL0YIouURGLFuTMliILTrHCmPwNAYPYJmLgTdAGx0fM+zljn5FcCfrM9nAmhJRFF6WhqOlStlLDAnO3eKPTv3XPfGHwMHyv8/pK31sPrPC2valaaliQEM5hG8/bZkpDdvdg8NGSFwDvvg5RGsXi01ShOmMTepMSpGCA4/XNx8/9Ee3WjbVprGmd6iXh4BIB5GWpo9fg9gXxv/2uSwYaK0mzdLE9Du3aV8ps8E0DiEwOQJouURZGSIJ1JYKNfM7beHglvz0dTU8HMEsRYCY+BbtnTv/Bhrj2DIEPFKjz664c4fJrFOFt8C4Ggi+gXA0QA2AQiYTJSIriKihUS0sMBtxMgYM22aDAPjnDxo6lTJW/71r+77JCTI0OnH9M6VBSaEc/nlEkZxtnBo0SK4EJgT79kTPDSUnGwbeKcQOFsNOZuOAvZDtGGDvJva5sEHi8ENmL3eBfMwmNZHwYTAEIpH0LKlKGpiovwBRLbRNeVvDEIQbY+AyA4PBesnUBtOj8AZGjK9loHQPAJTAQlXCKqqAoegrgvO8YPcroUpX0P+P2Yo6lDKnp0tlR7nWFFRJpLDUG8C4BzvuLu1bC/MvBmWR0BEmQDOYuYd/gdi5mkApgHAsGHDXMYfiC3Gvi1ZIqJeUyMDyR1/vD3umRv33APg+1z5Ulkpbc2nT5dhBnJy7NEijTvuhWllY5J4XqEhQEI527fbN12nTtJCqaZG1Cknxw65AN4eQTiYh8EM7BWKEPTuLTXaXbu8PQJAmoxu3my3sOndW4YyMD10d+wQg+I2OFM4Bqo+nHmm/LfRfNBbtRK3tD5G1Cs0BNj3XChCMGiQ5H1OP732bZ33rdsQ1HXBGHiv//mUU6SvTW2J7HBITpahSEL5zY2ASHoECwBkE1EfIkoBMA4yUv5eiKgDEZky3AZgegTLEzFMfyszbtC8edLT/69HfC/jhezZ472zM/xhDN5rr/m2K63NIzBDMphRIb1CQ4DE9vfZx17fqZMYyh07ZNv1630fmPR0edhjIQSAnNfLIwAkE+8cr93sZ95N7sXNIzBC0K9f7eWpD/36yQB/tSVKGxLjEdQn3GGut3/PYsCerCiU35SWJvd0KPNIOIWgIXoVA7YQeP3P2dmR+X8eeyxw6O9GSsQ8AmauIqLrAcwGkAhgOjMvJ6L7IXNnzgBwDIC/ERED+AbAdZEqTyTxF4IPP5R76siU+eImbN1q1zaKi+UBS0+XWribEJgwjDHItXkEbkLgfKBMaAiQnqvOsXycY+4UF0uZnDVXIjEmppwmNBQO5kHOz5cwTigPnLNHspkXIZT9jLExHdmCCcH554unEGmPIBY4Q0N1hUiuuX9oCJB7JSWl4VtZuQlBfWP3rVvLSK2mJ70SQERnKGPmmQBm+i272/H5fQDvR7IM0cAZGmIWITjuOKBFhVVrcnoEZ5whRm76dBEIE2t1CoEhXI/ALTRUWir9AswD7D+olVMITDt8/+x2+/b2FGp18QjMg7x5c2jeAGAb9PXr7ZpcsCGcDUZAOncW4x9MCNq2lakgmyNGsOtbm05JsT0CpxDs3BlaWKgu5wMa1iMAZH5ixZNYJ4ubPHv2iB1OTpaRHJZ9twP7rPkCZ54J2312CsGaNXaN3zGFY4AQJCXZxtDfI5gzx7elTzCPwKzzarJpwiNbt/rOiuWkXTu7w1t9QkP5+aGPGdO5sxia3Fw7NBSOJ9GpU+1C0JxpCI8AsD0Ct9BQUxICJSgqBPXEhIWOPFJs9fcXTcHnGIOxxxa7C8H27XYzSv8hFIwQHHmkvEwN2OkRlJfLWPwPPmjvGyxHUFtSz3QIc06P6OYRGOoTGqquDt0jSEiQxPbmzfZ1CcUj6N9fROSgg1QIgIYRgvJy+e/8PYJQmo6Gi1MIzH3dunXDn0fxQSevrycmLHTiiTItMNbnIgGMLilFgUJQVeU7+5JTCKqqbIP35JO+Qx20aGEbtI0bZdsff7TXBwsNGSHwqol36CBGY/VqMbSZmYFTGjpjtHXxCMzokWVloQuB2c80IwRCE4JWrew/JT3d93M80VBCkJJiT97jnyOIhEfgbORQW090pcFQj6CeGI/guOPERg7taNWqt2+3DbR5kExzOPN9/Xq7vbFxv4HAEEhamv1QGPH49Vc7XBQsNFSbR0AkuYicHHtWLP8EoPEIkpLqXgt0jsYYKikpvp5SuK060tPtOW9VCOpGcrJ9v0YjNJSYKC/T2g2o3xDUSkioENQTIwS9esl8L4d0tbpKbN8e6BGYmKfTIzDTDAYzeM7QkBGCykoZXwdw70dgjlFbjgCwhcBrnlzjEZghqOtCXYUgXI/AidP4qxDUDS+PIFJCYM6jQhBVVAjqyZYtUoHp0EFsesImh0dQmxAUFkpS09lED3D3CMxD4Uwwz5snsVvzoAbzCGoTgvXrpaew2/SIxiOoS1jI0BBCEK5H4OxEpkJQN5wegVMIyssjkyMw5zFCkJJit2ZTIobmCOpJfr7kJhMSIDeuieUHE4KyMjHgxcV25y6nEPg/YP4egelF+9NPtqEH6hYaAkQIamqkJVIwjyDaQpCcbI98CdTPI3DrWdycMa3BQhkUMBheQgBE3iMgUm8gSqgQ1JP8fMez5pxs3k0ITLwakIfLzPgVjkeQmytNJDt3Fo/A2TmsPqEhg5sQOAftqit19QhKSuruEcRzaOiUU4Affqh/Z7mUFPu+deYIgMgLQU2NCkGUUJ+rnmzZ4hiS3ykE27bZxtvfIwAkPFRcLC58bUJgPAJmWwiGDZNwjumTANQvNGSItEcQzoNtDEJDeATx1vIkMRE47LD6H8ffI3Aa/2jkCFQIooIKQT3x8QhMO3zAN5bvJgRmWGanEARrNcQsD+TmzSIEprPZypX2dkYIkpND70cASI3fGOpgHkEscgSVlfX3CFJTxTAq4ZOSYue0/END0cgRqBBEBRWCelBVJQN3BghBVpZM+WhwEwLTvj1UjwCQlj01Nb7zDa9Y4Xse0/GHSGrQoYSGANsrcEsWO1sN1ZX6JIvr6xHEW1ioIfFqPgpExyOIN08uRqgQhMHLL0un3g8+kAr6woVil820wti0SQx7z56+ncXchMCMxBmKEJiHz0zK3ru3uxCY2r+pqZkYu/MYXmRny/Zu8wukpYkxjXWrIRWC6JOSIje5+ayhoWaJJovD4IUXgJ9/lknnX3hBwkIJCcAJJ1gb5OVJjbptW9+pJ92EwHRACMcjcAqBcUNMaCgx0a79uwlBbQ/txIkyrIVXP4Gnngo+uUJtNESrobqGhlQI6o7zmqek2KPHmtFII4FzoDsVgqigQhAixcXSYez224EvvwSefVZs2mGHOUZgMB2ynG23U1J8haB1a2lN5OURJCYGtps2D8PKlbKue3c5bufOMuQEAHTs6O4RmN7MtT1QhxziO6yFP1deGXz/2lCPoGniFALzOS1N7tVIewSlpTrgXJTQ0FCI/PCDeMjHHitzpa9cKd7BySc7NnITgi5dfIWghzVpmxEC/+ajbrVeYzwXLQL69rUNvTOx26FDoBA4jxXrmlUsOpSpENQfZ63ffDb3koaGmg0qBCHy7bdSWR8xQiakN0MEnXSStQGzJICzsnyFICvLXQi8QkNuxs48DGvXyrR/BiMEGRli7NxCQ4ZIPbSh0q2blNF0hgsF/7GGwvUITCeyeOtM1pD4h4aA6AqBJoujgoaGQuTbb2VkY5MvnTBBksZDhlgblJaKy9Cqla/B6tIFWLVKPm/fbhtvfyGoqrKbfvrjfBicQmBa+LRqJQ+lW2gIkPLEuvlkmzaSTA9nSGH1CGJPMI9Am482G9QjCIHycmD+fOCoo+xl990nE9Hsza2aJnaZmb6dp9q0EY/ADEHdtavs5JUjCOYRAMABB9ifjah4CYEzptsYaNMmvEHrUlJsgQQ0RxALvHIEgIaGmhEqBCEwdaqIwd7WQRB7RlwDXHGFxO5NpxunELRuLWGJPXvspG27drKN2b5lSzFwznlh/fHyCJxCkJbm7RHEOixUV4zhMaE19QiiTyxDQ6WlKgRRQoWgFjZuBO64Q/oPjB7tt3LbNulcMGuWbdgzMmwhaNVKjNCePb7T7jlj187meLUli1NTJVls8A8NGfyFoKk+TKb8RgjUI4g+miyOC1QIauHWWyX0/8ILLlEN01egpMQ9NNS6tS0EZuCutm3tRIMZKjjU0ND++/saQ+MRtGzp3vW/sYWGwsVfCMIdjliFoP4ECw1FMkdQWirPgyaLo4IKQRC2bAHeew+45hqgTx+XDZxC4BUaMkbIJIedQmCGbAjVI3CGhYDQPYKmGhoy5d+9W65LuJPiqBDUHzePwNxPkfQIQu0RrzQIERUCIhpDRKuIaDURTXZZ35OIviaiX4hoCRGd7HacWPHyy5KrvPpqjw3chMAZGnIKgen41a6dHRry9wi8Wg1lZso+w4f7Ls/IkOaYPXo0/9BQuGEhQIQ2M9N9/CQlNGKVIzBNhpvqvdvEiFjzUSJKBPA8gNEA8gAsIKIZzOwYHAd3AniXmf9JRAMAzATQO1JlCofqamDaNGDUKKBfP2vhqlXiJhx9tHz3Cg1lZkrs3+QIABkwDhCjVJfQ0KpV9mQjThYsEIN3xx32Mv/QUFP3CPbsCT9RDNjXrWPHhi1XPBGr0JBBhSAqRNIjOBTAamZey8wVAN4GMNZvGwZgWUO0BrA5guUJi++/l6H+fUZWePBBaSVkKCiQd//QEJHMZj98uK8QZGaK8Q9XCAARELdacadOEjpqjh6BuRa7d9fNIwCkuW5dREQRzD3kHPokGh6Boaneu02MSHYo6wZgo+N7HgC/2AbuBfAFEU0AkAHgeLcDEdFVAK4CgJ7h9EytBz/+KO+jRjkWbt9uTzYDeIeGAOCLL+T944/l/ffffXsCA+5CUNdalluyuKkLQX09AqX+mOvuZpyjIQSaLI4KsU4WnwfgFWbuDuBkAK8TUUCZmHkaMw9j5mEdo+Tm//STTEbvMyqzc9pEwD005D+cgfEIcnNtIaiLR1AbzTlZXNccgVJ/go1bpR5BsyGSQrAJQA/H9+7WMieXA3gXAJj5RwBpAFwGxI8uzDId8IgRfitKSuwkFhDoEbRoETiUgxGCmproC0Fzaj6qHkFsCOYRaI6g2RBJIVgAIJuI+hBRCoBxAGb4bbMBwCgAIKL9IUJQEMEyhURenrT2dBUCN4/ATDvpNriZs+miab1itgu1+WgoNOfQUH1yBEr9cBvAUD2CZkfEhICZqwBcD2A2gJWQ1kHLieh+Ijrd2uxmAFcS0a8A/g3gEmbmSJUpVObNk3f/1pqeQrBrl7zcZvByCkFtHoFX89FQ0NCQEglinSNQIYgKEX26mHkmpEmoc9ndjs8rABwRyTLUhZ9+knt88GC/FSUl0q7UUFAgLSlqamS4CTchcHoJwYQAkER0Q3oETT005Gw1pKGh2GCue6xyBJosjgqxThY3SubPlyGnfUKg1dVSMzU5gj17pBu8c1jp2kJDtQnBnj11j7s6H0r/WlxTFQJT/upq9QhihVtoaL/9pE+L2/zWDXlOoOneu00MFQI/qqtl5rFhw/xWmOahNTXyMmEhM/ZEfn7toSH/HIG/EJSW1j805ByKobmEhgD1CGKFW2ho1CjpWGlyXA2NCkHUUSHwIydHIhEHHeS3wox9Aoha+AvBH3+4C0FqqhjmlBS7BjV8OHDKKXbsydR26xMCcWvJ0dRDQ87foh5BbHBrPhqtcwJN995tYujT5ceiRfJ+8MF+K5xCUFlpC8E++8g7s3toiEi8gs6d7Zp6ly7Ap5/a25iHrKam/h5BsGGDmxrqEcQeN48g0miOIOqoR+DHokViN/ff32+FUwiqqgI9AsDdIwBECIINfOY2nku4uHkEzSk0pB5BbIi1EDTVe7eJoULgx88/S8QmwO74C4EZZygUIWjfXrope9EQQuDmETT10FBDXBelfsQyNJSUpBWAKKFX2UFNjQjBRRe5rHTzCBISZBhog1toCJDxhtq08T6x21C/4dLcQ0NqEGJDLD2CpnrfNkH06XKwerXY+4D8ABCYI9i+XeYbaN3aXu7lEewdx9oDDQ25o0IQe9yaj0brnCoEUUNDQxaVlcCECWJvRo502cDfI6ioEAPrNP5eQlAbkQoNNfUHymn8NTQUG9w6lEUac99qojhqqBBY3HqrjBw9dSqQne2yQXGx/bmqyh4XKCHBdzL6uhApj6CpT0xjmt0C6hHECg0NxQUqBBYzZgCnnw5cdpnHBv4egXOAONOxRj2ChicWyUrFRkNDcYEKgcWWLUDfvkE28M8RNAUhOOoo4LbbgEMOqdsxGwPmeqhHEBvUI4gLahUCIjrNbbKY5oSZV6ZLl1o2Mnh5BLEMDSUkyL7OBzYzE3j44aYbGgLUI4g1sWw+qkIQNUIx8H8GkENEjxHRfpEuUCzYskXe6y0EDeER1KfmlZoa3ZpbNNAcQWyJpUegyeKoUasQMPOFAIYCWAPgFSL6kYiuIqIIjTgVffLz5T0rK8hGjT00BDRvIVCPIDYQAV27Bu8Z39A09Y6QTZCQqlnMXExE7wNoAWASgDMB/IWInmHmZyNYvqgQskdAJGMKNcbQECAPTnMVAvUIYsfKlb6j6EYaInkOVAiiRig5gtOJ6EMAcwEkAziUmU8CMBgyw1iTJ2QhML2DG2OrIQA44QTgiEY3z0/9UCGIPa1aRf/6p6SoEESRUP7dswD8g5m/cS5k5j1EdHlkihVd8vPF/rZrF2SjkhKgbVvpUWyEwNyoZl6ButaaGqrj1PTpdd+3sRKLDk1K7FEhiCqhCMG9APLNFyJqAaAzM+cy85xIFSyabNkio0QnBPOPSkpkyOm1awNzBOPGiRgkJtatADq4mjfqEcQnd97ZtJs9NzFCebreA3C443u1tazZ/EtbttQSFgJsjwAIDA0dfLDHAEUh0lCthpojmiyOT266KdYliCtCaT6axMwV5ov1uVlZq/z8WloMlZeL4fcSgvqiHoE36hEoSsQJRQgKiOh084WIxgL4I5SDE9EYIlpFRKuJaLLL+n8Q0WLr9TsR7Qi55A1IrR6BaTpqkggqBNFDPQJFiTihVLOuAfAmET0HgABsBDC+tp2IKBHA8wBGA8gDsICIZjDzCrMNM9/o2H4CpL9CVKmuljlmwhIC/xxBfVEh8EY9AkWJOLU+Xcy8BsAIIsq0vu8K8diHAljNzGsBgIjeBjAWwAqP7c8DcE+Ix24wtm2TCWmChoZM+9LOneVdPYLooa2GFCXihFTNIqJTAAwEkEbWBOzMfH8tu3WDeA+GPADDPY7fC0AfAP/1WH8VgKsAoKdzRrAGIKQ+BMuWyfvgwfKuQhA91CNQlIgTSoeyKZDxhiZAQkPnAOjVwOUYB+B9Zq52W8nM05h5GDMP69ixY4Oe2AwvUasQZGTYw5M2dGgoMVF6UwLaasgfFQJFiTihJIsPZ+bxALYz830ADgNQy9yLAIBNAHo4vne3lrkxDsC/Qzhmg7PJKlHXrkE2WrYMGDjQHsWzoT0CQEMgXmiyWFEiTihCUGa97yGirgAqAQSLqBsWAMgmoj5ElAIx9jP8N7JGNG0L4MfQitywrFsnlc2gY2otXQoMGmTXSlUIood6BIoScUIRgk+IqA2AvwP4GUAugLdq24mZqwBcD2A2gJUA3mXm5UR0v7M5KkQg3mZmDrPsDUJuLtCjRxA7s22bNCtSIYgN6hEoSsQJWs2yJqSZw8w7AHxARJ8CSGPmnaEcnJlnApjpt+xuv+/3hlPghiY3F+jTJ8gGJlF8wAG2EJSXyyikKgSRR2coU5SIE9QjYOYaSF8A8708VBFoKuTmAr17B9lg6VJ5HzTINkp79si7CkHkUY9AUSJOKKGhOUR0Fpl2o82I0lJpNRRUCJYtA9q39x2VrrRU3iMhBFrz9UVzBIoScUIRgqshg8yVE1ExEZUQUXGEyxUVNmyQ96BCsHIlMGCANO8kEoMUCSFISpLjNT+9rR/qEShKxAllqsqWzJzAzCnM3Mr63ioahYs0ubnyHjRHkJMDZGfb3yMlBMnJauzcUI9AUSJOrU8XER3lttx/opqmiBECT4+guFhaDTmFIDk5cjkCFYJAVAgUJeKE8nT9xfE5DTKG0CIAx0WkRFFk3TqxvZ7jDOXkyLuXR9CQvYBVCNzRJLqiRJxQBp07zfmdiHoAeCpSBYomublAr15BJhbzEgL1CKKHegSKEnFCSRb7kwdg/4YuSCyotemoEQIzxhCgQhBt+vSRoT3MyK+KojQ4oeQIngVgev0mABgC6WHcpKmoAJYvB8YHm1khJ0fGnnBOSp+cHLlksQ44F8gRR0iuRq+NokSMUPzthY7PVQD+zczfR6g8UePHH4Fdu4Djjw+ykX+LIUBbDcUCFQFFiSihCMH7AMrMENFElEhE6cy8J7JFiyyzZ0tu4LhgKe+cHOBPf/JdFikhSElRIVAUJSaEIgRzABwPwMxM1gLAFwAOj1ShosHs2cBhhwGtW3tssH07UFgY6BEkJ8ty87mhmDhRzqkoihJlQhGCNOf0lMy8i4jSg+3Q2Nm2Dfj5Z+CBB4JstGaNvDsTxUDkPIKgMSpFUZTIEUqrod1EdJD5QkQHAyiNXJEiz9dfy/uJJwbZyNT6O3XyXR4pIVAURYkRoXgEkwC8R0SbIVNVdoFMXdlkMZX9QYOCbLRjh7y3aeO7PClJhqAGVAgURWkWhNKhbIE1i1h/a9EqZq6MbLEiy+bNYt9btAiykZcQ6ETziqI0M0KZvP46ABnMvIyZlwHIJKL/i3zRIsfmzbXMUQwAO61pF9w8AoMKgaIozYBQcgRXWjOUAQCYeTuAKyNWoiiQnx+CEOzYIUY/3S8vrkKgKEozIxQhSHROSkNEiQCadA+fkDyCHTvEG/CfH0CFQFGUZkYoyeLPAbxDRFOt71cDmBW5IkWWmhrxCDxHHDUYIfBHcwSKojQzQhGCWwFcBeAa6/sSSMuhJklhIVBZGYZH4I96BIqiNDNCmaGsBsBPAHIhcxEcB2BlZIsVOfLz5T2kZLFbt2MVAkVRmhmeQkBE/YjoHiL6DcCzADYAADMfy8zPhXJwIhpDRKuIaDURTfbY5lwiWkFEy4norbr8iHDYvFne6+wRaGhIUZRmRrDQ0G8AvgVwKjOvBgAiujHUA1tJ5ecBjIbMYbCAiGYw8wrHNtkAbgNwBDNvJ6JO7kdrOIwQ1DlHoB6BoijNjGChoT8ByAfwNRG9SESjID2LQ+VQAKuZeS0zVwB4G8BYv22uBPC81SQVzLwtjOPXCRMa2isE+fnAtdfKBAVOQhECnTVLUZRmgKcQMPNHzDwOwH4AvoYMNdGJiP5JRCeEcOxuADY6vudZy5z0A9CPiL4nonlENMbtQER0FREtJKKFBQUFIZzam82bgXbtgLQ0a8HnnwNTpgC//WZvVFEhs5AFE4KkpMCmpYqiKE2QUJLFu5n5LWvu4u4AfoG0JGoIkgBkAzgGwHkAXiSiNi5lmMbMw5h5WMeOHet1woA+BEVF8r7HMb2CV69iQCdTVxSl2RHWnMXMvN0yyqNC2HwTgB6O792tZU7yAMxg5kpmXgfgd4gwRIzNm/3yA2aU0VCFwHgEKgSKojQT6jJ5fagsAJBNRH2IKAXAOAAz/Lb5COINgIg6QEJFayNYpsDhJdw8Aq8B5wAVAkVRmh0REwJmrgJwPYDZkH4H7zLzciK6n4hOtzabDaCQiFZA8hB/YebCSJUJEAegQwe/BYAKgaIocUtEm70w80wAM/2W3e34zABusl4Rp7pa7H3Llo6FwTwCtw5lmiNQFKWZEcnQUKNjlzXhZmamY6F6BIqixDlxKQS1egSaLFYUJY6IKyEoKZF3HyHw8ggSEvxcBwsjBClNeiRuRVGUvcS3EJSV2QLgLwRucxEAmiNQFKXZEZdCsLeib8JCgLsQuKGhIUVRmhlxJQQBOQKnEOzebX9WIVAUJY6IKyEICA0VOrosOD2C3Fygk8dAqBoaUhSlmRHfQmA8goQEWwjy8oBly4BRHqNoqEegKEozI76FwHgEXbvaQjDLmo755JPdD6JCoChKMyOuhMDkCNLTrQXGI+jRwxaCmTOBXr2A/fd3P4gKgaIozYy4EoKSEmkxlGB+dWGh9Afo0EGEoLwc+Oor8Qa85hrQHIGiKM2MuBOCgF7F7dsDGRkiBPPni9swxnV+HEE9AkVRmhnxLQSFhTJdWXq6CMEma7qE7CBTIqgQKIrSzIgrIdi1y2/UCOMRGCEwyeP27b0PokKgKEozI66EoFaPwCSP27b1PojmCBRFaWbEtxDs2CFGPz1dxh0qKABatQpu5NUjUBSlmRHfQrBrlyww7Uk3bRIPIRgqBIqiNDPiSgiM3QcAMNvtSY0QbNwYPD8AqBAoitLsiCshMHYfgPQZqK4OFILaPALNESiK0syIGyGoqZEBRvd6BM6hSDMy5PPWrRoaUhQl7ogbIQgYgto5OYHxCJg1NKQoStwRUSEgojFEtIqIVhPRZJf1lxBRAREttl5XRKosAULgnMl+7+BD0NCQoihxR1KkDkxEiQCeBzAaQB6ABUQ0g5lX+G36DjNfH6lyGAJmJ3Mqg1MI1CNQFCXOiKRHcCiA1cy8lpkrALwNYGwEzxeUgCGo3UJDgOYIFEWJOyIpBN0AbHR8z7OW+XMWES0hoveJqIfbgYjoKiJaSEQLCwoK6lSYACHwCg3V5hFkZQGXXgocd1ydyqEoitLYiHWy+BMAvZn5QABfAnjVbSNmnsbMw5h5WMeOHet0Is8cgX9oKBSPYPp0oH//OpVDURSlsRFJIdgEwFnD724t2wszFzJzufX1JQAHR6owATkCr9BQbR6BoihKMyOSQrAAQDYR9SGiFADjAMxwbkBEWY6vpwNYGanChBwaqs0jUBRFaWZErNUQM1cR0fUAZgNIBDCdmZcT0f0AFjLzDAA3ENHpAKoAFAG4JFLlcRWChASgRQv5npgoPY2DjTyqKIrSDImYEAAAM88EMNNv2d2Oz7cBuC2SZTAcfzzw7LN2J+K9402YKSnT00UMEhOjURxFUZRGQ0SFoDExZIi89uI/S016ut+sNYqiKPFBrFsNxQ5/IcjI0PyAoihxSfwKgf/kBBkZ2mJIUZS4JG5CQwH4ewQPPwy0bh278iiKosSI+BaCLEfr1VNPjV1ZFEVRYoiGhhRFUeKc+BUC/9CQoihKnKJCoCiKEufEpxAw+81kryiKEr/EpxDs2SNioB6BoihKnAqBc8A5RVGUOCc+hSBgTGpFUZT4JT77EQTMUqMoSmOisrISeXl5KCsri3VRmhxpaWno3r07ksOYTje+hUA9AkVplOTl5aFly5bo3bs3yIwQrNQKM6OwsBB5eXno06dPyPtpaEhRlEZHWVkZ2rdvryIQJkSE9u3bh+1JxacQaGhIURo9KgJ1oy7XLb6FQD0CRVGUOBWCHTvkvVWrmBZDUZTGSWFhIYYMGYIhQ4agS5cu6Nat297vFRUVQfdduHAhbrjhhiiVtGGIz2Tx9u3yrsNOK4riQvv27bF48WIAwL333ovMzEzccsste9dXVVUhKcndfA4bNgzDhg2LRjEbjPgVgtatdX5iRWkCTJoEWDa5wRgyBHjqqfD2ueSSS5CWloZffvkFRxxxBMaNG4eJEyeirKwMLVq0wL/+9S/0798fc+fOxeOPP45PP/0U9957LzZs2IC1a9diw4YNmDRpkqu3cO2112LBggUoLS3F2Wefjfvuuw8AsGDBAkycOBG7d+9Gamoq5syZg/T0dNx66634/PPPkZCQgCuvvBITJkyo1/WIXyHQaSkVRQmTvLw8/PDDD0hMTERxcTG+/fZbJCUl4auvvsLtt9+ODz74IGCf3377DV9//TVKSkrQv39/XHvttQFt/B966CG0a9cO1dXVGDVqFJYsWYL99tsPf/7zn/HOO+/gkEMOQXFxMVq0aIFp06YhNzcXixcvRlJSEoqKiur9uyIqBEQ0BsDTABIBvMTMj3hsdxaA9wEcwswLI1kmACIEbdtG/DSKotSfcGvukeScc85BohVJ2LlzJy6++GLk5OSAiFBZWem6zymnnILU1FSkpqaiU6dO2Lp1K7p37+6zzbvvvotp06ahqqoK+fn5WLFiBYgIWVlZOOSQQwAArayc5ldffYVrrrlmb2iqXQNUaiOWLCaiRADPAzgJwAAA5xHRAJftWgKYCOCnSJUlABUCRVHqQEZGxt7Pd911F4499lgsW7YMn3zyiWfb/dTU1L2fExMTUVVV5bN+3bp1ePzxxzFnzhwsWbIEp5xyStR7VEey1dChAFYz81pmrgDwNoCxLts9AOBRANH75SoEiqLUk507d6Jbt24AgFdeeaXOxykuLkZGRgZat26NrVu3YtasWQCA/v37Iz8/HwsWLAAAlJSUoKqqCqNHj8bUqVP3CkpDhIYiKQTdAGx0fM+zlu2FiA4C0IOZPwt2ICK6iogWEtHCgoKC+pdMhUBRlHry17/+FbfddhuGDh0aUMsPh8GDB2Po0KHYb7/9cP755+OII44AAKSkpOCdd97BhAkTMHjwYIwePRplZWW44oor0LNnTxx44IEYPHgw3nrrrXr/FmLmeh/E9cBEZwMYw8xXWN8vAjCcma+3vicA+C+AS5g5l4jmArilthzBsGHDeOHCeqQRmIG0NGmK8OijdT+OoigRY+XKldh///1jXYwmi9v1I6JFzOzarjWSHsEmAD0c37tbywwtAQwCMJeIcgGMADCDiCLbALe0FKioUI9AURTFIpJCsABANhH1IaIUAOMAzDArmXknM3dg5t7M3BvAPACnR7zVkOlMpkKgKIoCIIJCwMxVAK4HMBvASgDvMvNyIrqfiE6P1HlrRYVAURTFh4j2I2DmmQBm+i2722PbYyJZlr2oECiKovgQf4POqRAoiqL4oEKgKIoS58TfWENGCHSsIUVRPCgsLMSoUaMAAFu2bEFiYiI6duwIAJg/fz5SUlKC7j937lykpKTg8MMPj3hZG4L4FAIiHYJaURRPahuGujbmzp2LzMxMFYJGixmCOiH+omKK0iRpJONQL1q0CDfddBN27dqFDh064JVXXkFWVhaeeeYZTJkyBUlJSRgwYAAeeeQRTJkyBYmJiXjjjTfw7LPPYuTIkXuPM3/+fNfhq6urq12Hl3YbirplA0+zG59CoPkBRVHCgJkxYcIEfPzxx+jYsSPeeecd3HHHHZg+fToeeeQRrFu3DqmpqdixYwfatGmDa665xtOL2G+//VyHr3YbXrqiosJ1KOqGJv6EoKhIhUBRmhKNYBzq8vJyLFu2DKNHjwYAVFdXIysrCwBw4IEH4oILLsAZZ5yBM844o9ZjeQ1f7Ta89NKlS12Hom5o4k8I1CNQFCVMmBkDBw7Ejz/+GLDus88+wzfffINPPvkEDz30EJYuXRr0WGb46g8//BC5ubk45phjIlTq0Im/QLkKgaIoYZKamoqCgoK9QlBZWYnly5ejpqYGGzduxLHHHotHH30UO3fuxK5du9CyZUuUlJS4Hstr+Gq34aW9hqJuaOJHCKZPBwYOBFavViFQFCUsEhIS8P777+PWW2/F4MGDMWTIEPzwww+orq7GhRdeiAMOOABDhw7FDTfcgDZt2uC0007Dhx9+iCFDhuDbb7/1OZbX8NVuw0t7DUXd0ERsGOpIUedhqD/+GHjjDWk6OnEiYI35rShK40OHoa4f4Q5DHT85grFj5aUoiqL4ED+hIUVRFMUVFQJFURolTS1s3Vioy3VTIVAUpdGRlpaGwsJCFYMwYWYUFhYiLS0trP3iJ0egKEqToXv37sjLy0NBQUGsi9LkSEtLQ/fu3cPaR4VAUZRGR3JyMvr06RPrYsQNGhpSFEWJc1QIFEVR4hwVAkVRlDinyfUsJqICAOvrsGsHAH80cHEaAi1XeDTWcgGNt2xarvBorOUC6le2Xszc0W1FkxOCukJEC726V8cSLVd4NNZyAY23bFqu8Gis5QIiVzYNDSmKosQ5KgSKoihxTjwJwbRYF8ADLVd4NNZyAY23bFqu8Gis5QIiVLa4yREoiqIo7sSTR6AoiqK4oEKgKIoS5zR7ISCiMUS0iohWE9HkGJajBxF9TUQriGg5EU20lt9LRJuIaLH1OjlG5csloqVWGRZay9oR0ZdElGO9R3WOTyLq77gui4momIgmxeKaEdF0ItpGRMscy1yvDwnPWPfcEiI6KAZl+zsR/Wad/0MiamMt701EpY5rNyXK5fL874joNuuarSKiE6NcrnccZcolosXW8mheLy8bEfn7jJmb7QtAIoA1APYBkALgVwADYlSWLAAHWZ9bAvgdwAAA9wK4pRFcq1wAHfyWPQZgsvV5MoBHY/xfbgHQKxbXDMBRAA4CsKy26wPgZACzABCAEQB+ikHZTgCQZH1+1FG23s7tYlAu1//OehZ+BZAKoI/13CZGq1x+658AcHcMrpeXjYj4fdbcPYJDAaxm5rXMXAHgbQAxma+SmfOZ+WfrcwmAlQC6xaIsYTAWwKvW51cBnBG7omAUgDXMXJde5fWGmb8BUOS32Ov6jAXwGgvzALQhoqxolo2Zv2BmMzP6PADhjUscoXIFYSyAt5m5nJnXAVgNeX6jWi4iIgDnAvh3JM4djCA2IuL3WXMXgm4ANjq+56ERGF8i6g1gKICfrEXXW67d9GiHXxwwgC+IaBERXWUt68zM+dbnLQA6x6ZoAIBx8H04G8M187o+je2+uwxSczT0IaJfiOh/RDQyBuVx++8ayzUbCWArM+c4lkX9evnZiIjfZ81dCBodRJQJ4AMAk5i5GMA/AewLYAiAfIhbGguOZOaDAJwE4DoiOsq5ksUXjUlbYyJKAXA6gPesRY3lmu0lltcnGER0B4AqAG9ai/IB9GTmoQBuAvAWEbWKYpEa3X/nx3nwrXBE/Xq52Ii9ROo+a+5CsAlAD8f37taymEBEyZA/+E1m/g8AMPNWZq5m5hoALyJC7nBtMPMm630bgA+tcmw1rqb1vi0WZYOI08/MvNUqY6O4ZvC+Po3iviOiSwCcCuACy4DACr0UWp8XQWLx/aJVpiD/XcyvGRElAfgTgHfMsmhfLzcbgSjcZ81dCBYAyCaiPlatchyAGbEoiBV7fBnASmZ+0rHcGdM7E8Ay/32jULYMImppPkMSjcsg1+pia7OLAXwc7bJZ+NTSGsM1s/C6PjMAjLdadYwAsNPh2kcFIhoD4K8ATmfmPY7lHYko0fq8D4BsAGujWC6v/24GgHFElEpEfaxyzY9WuSyOB/AbM+eZBdG8Xl42AtG4z6KRDY/lC5JZ/x2i5HfEsBxHQly6JQAWW6+TAbwOYKm1fAaArBiUbR9Ii41fASw31wlAewBzAOQA+ApAuxiULQNAIYDWjmVRv2YQIcoHUAmJxV7udX0grTiet+65pQCGxaBsqyHxY3OvTbG2Pcv6jxcD+BnAaVEul+d/B+AO65qtAnBSNMtlLX8FwDV+20bzennZiIjfZzrEhKIoSpzT3ENDiqIoSi2oECiKosQ5KgSKoihxjgqBoihKnKNCoCiKEueoECiKH0RUTb6jnjbYqLXWaJax6vegKK4kxboAitIIKWXmIbEuhKJEC/UIFCVErHHqHyOZt2E+EfW1lvcmov9aA6nNIaKe1vLOJHMB/Gq9DrcOlUhEL1pjzn9BRC1i9qMUBSoEiuJGC7/Q0J8d63Yy8wEAngPwlLXsWQCvMvOBkMHdnrGWPwPgf8w8GDL+/XJreTaA55l5IIAdkN6rihIztGexovhBRLuYOdNleS6A45h5rTU42BZmbk9Ef0CGSqi0luczcwciKgDQnZnLHcfoDeBLZs62vt8KIJmZH4zCT1MUV9QjUJTwYI/P4VDu+FwNzdUpMUaFQFHC48+O9x+tzz9ARrYFgAsAfGt9ngPgWgAgokQiah2tQipKOGhNRFECaUHW5OUWnzOzaULaloiWQGr151nLJgD4FxH9BUABgEut5RMBTCOiyyE1/2sho14qSqNCcwSKEiJWjmAYM/8R67IoSkOioSFFUZQ4Rz0CRVGUOEc9AkVRlDhHhUBRFCXOUSFQFEWJc1QIFEVR4hwVAkVRlDjn/wFYMd5IXjiCgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Train acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Test acc')\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Train loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Test loss')\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gjQJ8S64JoAU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[1.7456063e-04 9.9967182e-01 1.5333116e-04 1.4373294e-08 1.9984185e-07]\n",
      "['Groundnut__Alternaria__Leafspot', 'Groundnut__Early__Late__Leafspot', 'Groundnut__Healthy', 'Groundnut__Rosette', 'Groundnut__Rust']\n",
      "The disease of the given groundnut leaf is Groundnut__Early__Late__Leafspot predicted with 99.96718168258667 % confidence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image=tf.keras.utils.load_img(r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Images for checking\\img.jpg',target_size=(224,224))\n",
    "test_image=tf.keras.utils.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=model.predict(test_image)\n",
    "result=result.flatten()\n",
    "print(result)\n",
    "print(class_names)\n",
    "index=result.argmax()\n",
    "confidence=result[index]*100;\n",
    "pred_class=class_names[index]\n",
    "if pred_class!='Groundnut__Healthy':\n",
    "    print(f'The disease of the given groundnut leaf is {pred_class} predicted with {confidence} % confidence')\n",
    "else:\n",
    "    print(f'The groundnut leaf is healthy predicted with {confidence} % confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLH8KuHyZbdJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
