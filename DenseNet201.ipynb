{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c11a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Conv2D,MaxPool2D,concatenate,AveragePooling2D,GlobalAveragePooling2D,Flatten,ZeroPadding2D,BatchNormalization,Dense,ZeroPadding2D,Activation,ReLU,Input\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from keras.preprocessing import image\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU') \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f74ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "train_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Train'\n",
    "test_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Test'\n",
    "valid_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe7eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescaling and augmentation of data\n",
    "data_augmentation=tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\",input_shape=(224,224,3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "],name=\"data_augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35616795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE=(224,224)\n",
    "BATCH_SIZE=5\n",
    "training_set=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = test_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    label_mode = 'categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = valid_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    label_mode = 'categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "class_names=validation_set.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f251ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet201(input_shape=(224,224,3),classes=5,filters=32):\n",
    "    def conv(x,filters,kernel=1,strides=1):\n",
    "        x=BatchNormalization()(x)\n",
    "        x=ReLU()(x)\n",
    "        x=Conv2D(filters,kernel,strides=strides,padding='same')\n",
    "        return x\n",
    "    def dense_block(x,repetition,filters):\n",
    "        for i in range(repetition):\n",
    "            y=conv(x,4*filters)(x)\n",
    "            y=conv(y,filters,3)(y)\n",
    "            x=concatenate([y,x])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        x=conv(x,K.int_shape(x)[-1]//2)(x)\n",
    "        x=AveragePooling2D(2,strides=2,padding='same')(x)\n",
    "        return x\n",
    "    x_input=Input(input_shape)\n",
    "    x=Conv2D(64,7,strides=2,padding='same')(x_input)\n",
    "    x=MaxPool2D(3,strides=2,padding='same')(x)\n",
    "    for repetition in [6,12,48,32]:\n",
    "        d=dense_block(x,repetition,filters)\n",
    "        x=transition_layer(d)\n",
    "    x=GlobalAveragePooling2D()(d)\n",
    "    x=Dense(units=classes,activation='softmax')(x)\n",
    "    \n",
    "    model=Model(inputs=x_input,outputs=x,name='DenseNet201')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a298262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DenseNet201(input_shape=(224,224,3),classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262b74ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DenseNet201\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 64  9472        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 56, 56, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 56, 56, 128)  8320        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 56, 56, 32)   36896       ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 56, 56, 96)   0           ['conv2d_2[0][0]',               \n",
      "                                                                  'max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 56, 56, 128)  12416       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 56, 56, 32)   36896       ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 56, 56, 128)  0           ['conv2d_4[0][0]',               \n",
      "                                                                  'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 56, 56, 128)  16512       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 56, 56, 32)   36896       ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 56, 56, 160)  0           ['conv2d_6[0][0]',               \n",
      "                                                                  'concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 56, 56, 128)  20608       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 56, 56, 32)   36896       ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 56, 56, 192)  0           ['conv2d_8[0][0]',               \n",
      "                                                                  'concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 56, 56, 128)  24704       ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 56, 56, 32)   36896       ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 56, 56, 224)  0           ['conv2d_10[0][0]',              \n",
      "                                                                  'concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 56, 56, 128)  28800       ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 56, 56, 32)   36896       ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 56, 56, 256)  0           ['conv2d_12[0][0]',              \n",
      "                                                                  'concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 56, 56, 128)  32896       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 28, 28, 128)  0          ['conv2d_13[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 28, 28, 128)  16512       ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 28, 28, 160)  0           ['conv2d_15[0][0]',              \n",
      "                                                                  'average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 28, 28, 128)  20608       ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 28, 28, 192)  0           ['conv2d_17[0][0]',              \n",
      "                                                                  'concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 28, 28, 128)  24704       ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 28, 28, 224)  0           ['conv2d_19[0][0]',              \n",
      "                                                                  'concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 28, 28, 128)  28800       ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concatenate_9 (Concatenate)    (None, 28, 28, 256)  0           ['conv2d_21[0][0]',              \n",
      "                                                                  'concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 28, 28, 128)  32896       ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 28, 28, 288)  0           ['conv2d_23[0][0]',              \n",
      "                                                                  'concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 28, 28, 128)  36992       ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 28, 28, 320)  0           ['conv2d_25[0][0]',              \n",
      "                                                                  'concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 28, 28, 128)  41088       ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 28, 28, 352)  0           ['conv2d_27[0][0]',              \n",
      "                                                                  'concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 28, 28, 128)  45184       ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 28, 28, 384)  0           ['conv2d_29[0][0]',              \n",
      "                                                                  'concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 28, 28, 128)  49280       ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 28, 28, 416)  0           ['conv2d_31[0][0]',              \n",
      "                                                                  'concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 28, 28, 128)  53376       ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 28, 28, 448)  0           ['conv2d_33[0][0]',              \n",
      "                                                                  'concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 28, 28, 128)  57472       ['concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 28, 28, 480)  0           ['conv2d_35[0][0]',              \n",
      "                                                                  'concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 28, 28, 128)  61568       ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 28, 28, 32)   36896       ['conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 28, 28, 512)  0           ['conv2d_37[0][0]',              \n",
      "                                                                  'concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 28, 28, 256)  131328      ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 14, 14, 256)  0          ['conv2d_38[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 14, 14, 128)  32896       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 14, 14, 288)  0           ['conv2d_40[0][0]',              \n",
      "                                                                  'average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 14, 14, 128)  36992       ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 14, 14, 320)  0           ['conv2d_42[0][0]',              \n",
      "                                                                  'concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 14, 14, 128)  41088       ['concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 14, 14, 352)  0           ['conv2d_44[0][0]',              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 14, 14, 128)  45184       ['concatenate_20[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 14, 14, 384)  0           ['conv2d_46[0][0]',              \n",
      "                                                                  'concatenate_20[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 14, 14, 128)  49280       ['concatenate_21[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 14, 14, 416)  0           ['conv2d_48[0][0]',              \n",
      "                                                                  'concatenate_21[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 14, 14, 128)  53376       ['concatenate_22[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 14, 14, 448)  0           ['conv2d_50[0][0]',              \n",
      "                                                                  'concatenate_22[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 14, 14, 128)  57472       ['concatenate_23[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 14, 14, 480)  0           ['conv2d_52[0][0]',              \n",
      "                                                                  'concatenate_23[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 14, 14, 128)  61568       ['concatenate_24[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_53[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 14, 14, 512)  0           ['conv2d_54[0][0]',              \n",
      "                                                                  'concatenate_24[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 14, 14, 128)  65664       ['concatenate_25[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_55[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 14, 14, 544)  0           ['conv2d_56[0][0]',              \n",
      "                                                                  'concatenate_25[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 14, 14, 128)  69760       ['concatenate_26[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_57[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 14, 14, 576)  0           ['conv2d_58[0][0]',              \n",
      "                                                                  'concatenate_26[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 14, 14, 128)  73856       ['concatenate_27[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_59[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 14, 14, 608)  0           ['conv2d_60[0][0]',              \n",
      "                                                                  'concatenate_27[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 14, 14, 128)  77952       ['concatenate_28[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_61[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 14, 14, 640)  0           ['conv2d_62[0][0]',              \n",
      "                                                                  'concatenate_28[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 14, 14, 128)  82048       ['concatenate_29[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_63[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 14, 14, 672)  0           ['conv2d_64[0][0]',              \n",
      "                                                                  'concatenate_29[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 14, 14, 128)  86144       ['concatenate_30[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 14, 14, 704)  0           ['conv2d_66[0][0]',              \n",
      "                                                                  'concatenate_30[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 14, 14, 128)  90240       ['concatenate_31[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_67[0][0]']              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concatenate_32 (Concatenate)   (None, 14, 14, 736)  0           ['conv2d_68[0][0]',              \n",
      "                                                                  'concatenate_31[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 14, 14, 128)  94336       ['concatenate_32[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_69[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 14, 14, 768)  0           ['conv2d_70[0][0]',              \n",
      "                                                                  'concatenate_32[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 14, 14, 128)  98432       ['concatenate_33[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_71[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 14, 14, 800)  0           ['conv2d_72[0][0]',              \n",
      "                                                                  'concatenate_33[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 14, 14, 128)  102528      ['concatenate_34[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_73[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 14, 14, 832)  0           ['conv2d_74[0][0]',              \n",
      "                                                                  'concatenate_34[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 14, 14, 128)  106624      ['concatenate_35[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 14, 14, 864)  0           ['conv2d_76[0][0]',              \n",
      "                                                                  'concatenate_35[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 14, 14, 128)  110720      ['concatenate_36[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_77[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 14, 14, 896)  0           ['conv2d_78[0][0]',              \n",
      "                                                                  'concatenate_36[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 14, 14, 128)  114816      ['concatenate_37[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_79[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenate)   (None, 14, 14, 928)  0           ['conv2d_80[0][0]',              \n",
      "                                                                  'concatenate_37[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 14, 14, 128)  118912      ['concatenate_38[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_81[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenate)   (None, 14, 14, 960)  0           ['conv2d_82[0][0]',              \n",
      "                                                                  'concatenate_38[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 14, 14, 128)  123008      ['concatenate_39[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_83[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 14, 14, 992)  0           ['conv2d_84[0][0]',              \n",
      "                                                                  'concatenate_39[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 14, 14, 128)  127104      ['concatenate_40[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_85[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 14, 14, 1024  0           ['conv2d_86[0][0]',              \n",
      "                                )                                 'concatenate_40[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 14, 14, 128)  131200      ['concatenate_41[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_87[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 14, 14, 1056  0           ['conv2d_88[0][0]',              \n",
      "                                )                                 'concatenate_41[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 14, 14, 128)  135296      ['concatenate_42[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_89[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 14, 14, 1088  0           ['conv2d_90[0][0]',              \n",
      "                                )                                 'concatenate_42[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 14, 14, 128)  139392      ['concatenate_43[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_91[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 14, 14, 1120  0           ['conv2d_92[0][0]',              \n",
      "                                )                                 'concatenate_43[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 14, 14, 128)  143488      ['concatenate_44[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_93[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 14, 14, 1152  0           ['conv2d_94[0][0]',              \n",
      "                                )                                 'concatenate_44[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 14, 14, 128)  147584      ['concatenate_45[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_95[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 14, 14, 1184  0           ['conv2d_96[0][0]',              \n",
      "                                )                                 'concatenate_45[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 14, 14, 128)  151680      ['concatenate_46[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 14, 14, 32)   36896       ['conv2d_97[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 14, 14, 1216  0           ['conv2d_98[0][0]',              \n",
      "                                )                                 'concatenate_46[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 14, 14, 128)  155776      ['concatenate_47[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_99[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_48 (Concatenate)   (None, 14, 14, 1248  0           ['conv2d_100[0][0]',             \n",
      "                                )                                 'concatenate_47[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 14, 14, 128)  159872      ['concatenate_48[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_101[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_49 (Concatenate)   (None, 14, 14, 1280  0           ['conv2d_102[0][0]',             \n",
      "                                )                                 'concatenate_48[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 14, 14, 128)  163968      ['concatenate_49[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_103[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_50 (Concatenate)   (None, 14, 14, 1312  0           ['conv2d_104[0][0]',             \n",
      "                                )                                 'concatenate_49[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 14, 14, 128)  168064      ['concatenate_50[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_105[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_51 (Concatenate)   (None, 14, 14, 1344  0           ['conv2d_106[0][0]',             \n",
      "                                )                                 'concatenate_50[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 14, 14, 128)  172160      ['concatenate_51[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_107[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_52 (Concatenate)   (None, 14, 14, 1376  0           ['conv2d_108[0][0]',             \n",
      "                                )                                 'concatenate_51[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 14, 14, 128)  176256      ['concatenate_52[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_109[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_53 (Concatenate)   (None, 14, 14, 1408  0           ['conv2d_110[0][0]',             \n",
      "                                )                                 'concatenate_52[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 14, 14, 128)  180352      ['concatenate_53[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_111[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_54 (Concatenate)   (None, 14, 14, 1440  0           ['conv2d_112[0][0]',             \n",
      "                                )                                 'concatenate_53[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 14, 14, 128)  184448      ['concatenate_54[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_113[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_55 (Concatenate)   (None, 14, 14, 1472  0           ['conv2d_114[0][0]',             \n",
      "                                )                                 'concatenate_54[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 14, 14, 128)  188544      ['concatenate_55[0][0]']         \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_116 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_115[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_56 (Concatenate)   (None, 14, 14, 1504  0           ['conv2d_116[0][0]',             \n",
      "                                )                                 'concatenate_55[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)            (None, 14, 14, 128)  192640      ['concatenate_56[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_117[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_57 (Concatenate)   (None, 14, 14, 1536  0           ['conv2d_118[0][0]',             \n",
      "                                )                                 'concatenate_56[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)            (None, 14, 14, 128)  196736      ['concatenate_57[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_119[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_58 (Concatenate)   (None, 14, 14, 1568  0           ['conv2d_120[0][0]',             \n",
      "                                )                                 'concatenate_57[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)            (None, 14, 14, 128)  200832      ['concatenate_58[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_121[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_59 (Concatenate)   (None, 14, 14, 1600  0           ['conv2d_122[0][0]',             \n",
      "                                )                                 'concatenate_58[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)            (None, 14, 14, 128)  204928      ['concatenate_59[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_123[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_60 (Concatenate)   (None, 14, 14, 1632  0           ['conv2d_124[0][0]',             \n",
      "                                )                                 'concatenate_59[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)            (None, 14, 14, 128)  209024      ['concatenate_60[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_125[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_61 (Concatenate)   (None, 14, 14, 1664  0           ['conv2d_126[0][0]',             \n",
      "                                )                                 'concatenate_60[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)            (None, 14, 14, 128)  213120      ['concatenate_61[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_127[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_62 (Concatenate)   (None, 14, 14, 1696  0           ['conv2d_128[0][0]',             \n",
      "                                )                                 'concatenate_61[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)            (None, 14, 14, 128)  217216      ['concatenate_62[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_129[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_63 (Concatenate)   (None, 14, 14, 1728  0           ['conv2d_130[0][0]',             \n",
      "                                )                                 'concatenate_62[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)            (None, 14, 14, 128)  221312      ['concatenate_63[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_131[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_64 (Concatenate)   (None, 14, 14, 1760  0           ['conv2d_132[0][0]',             \n",
      "                                )                                 'concatenate_63[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)            (None, 14, 14, 128)  225408      ['concatenate_64[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)            (None, 14, 14, 32)   36896       ['conv2d_133[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_65 (Concatenate)   (None, 14, 14, 1792  0           ['conv2d_134[0][0]',             \n",
      "                                )                                 'concatenate_64[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_135 (Conv2D)            (None, 14, 14, 896)  1606528     ['concatenate_65[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 7, 7, 896)   0           ['conv2d_135[0][0]']             \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_136 (Conv2D)            (None, 7, 7, 128)    114816      ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_137 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_136[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_66 (Concatenate)   (None, 7, 7, 928)    0           ['conv2d_137[0][0]',             \n",
      "                                                                  'average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_138 (Conv2D)            (None, 7, 7, 128)    118912      ['concatenate_66[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_139 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_138[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " concatenate_67 (Concatenate)   (None, 7, 7, 960)    0           ['conv2d_139[0][0]',             \n",
      "                                                                  'concatenate_66[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_140 (Conv2D)            (None, 7, 7, 128)    123008      ['concatenate_67[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_141 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_140[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_68 (Concatenate)   (None, 7, 7, 992)    0           ['conv2d_141[0][0]',             \n",
      "                                                                  'concatenate_67[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_142 (Conv2D)            (None, 7, 7, 128)    127104      ['concatenate_68[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_143 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_142[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_69 (Concatenate)   (None, 7, 7, 1024)   0           ['conv2d_143[0][0]',             \n",
      "                                                                  'concatenate_68[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_144 (Conv2D)            (None, 7, 7, 128)    131200      ['concatenate_69[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_145 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_144[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_70 (Concatenate)   (None, 7, 7, 1056)   0           ['conv2d_145[0][0]',             \n",
      "                                                                  'concatenate_69[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_146 (Conv2D)            (None, 7, 7, 128)    135296      ['concatenate_70[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_147 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_146[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_71 (Concatenate)   (None, 7, 7, 1088)   0           ['conv2d_147[0][0]',             \n",
      "                                                                  'concatenate_70[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_148 (Conv2D)            (None, 7, 7, 128)    139392      ['concatenate_71[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_149 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_148[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_72 (Concatenate)   (None, 7, 7, 1120)   0           ['conv2d_149[0][0]',             \n",
      "                                                                  'concatenate_71[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_150 (Conv2D)            (None, 7, 7, 128)    143488      ['concatenate_72[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_151 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_150[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_73 (Concatenate)   (None, 7, 7, 1152)   0           ['conv2d_151[0][0]',             \n",
      "                                                                  'concatenate_72[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_152 (Conv2D)            (None, 7, 7, 128)    147584      ['concatenate_73[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_153 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_152[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_74 (Concatenate)   (None, 7, 7, 1184)   0           ['conv2d_153[0][0]',             \n",
      "                                                                  'concatenate_73[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_154 (Conv2D)            (None, 7, 7, 128)    151680      ['concatenate_74[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_155 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_154[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_75 (Concatenate)   (None, 7, 7, 1216)   0           ['conv2d_155[0][0]',             \n",
      "                                                                  'concatenate_74[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_156 (Conv2D)            (None, 7, 7, 128)    155776      ['concatenate_75[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_157 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_156[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_76 (Concatenate)   (None, 7, 7, 1248)   0           ['conv2d_157[0][0]',             \n",
      "                                                                  'concatenate_75[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_158 (Conv2D)            (None, 7, 7, 128)    159872      ['concatenate_76[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_159 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_158[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_77 (Concatenate)   (None, 7, 7, 1280)   0           ['conv2d_159[0][0]',             \n",
      "                                                                  'concatenate_76[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_160 (Conv2D)            (None, 7, 7, 128)    163968      ['concatenate_77[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_161 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_160[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_78 (Concatenate)   (None, 7, 7, 1312)   0           ['conv2d_161[0][0]',             \n",
      "                                                                  'concatenate_77[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_162 (Conv2D)            (None, 7, 7, 128)    168064      ['concatenate_78[0][0]']         \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_163 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_162[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_79 (Concatenate)   (None, 7, 7, 1344)   0           ['conv2d_163[0][0]',             \n",
      "                                                                  'concatenate_78[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_164 (Conv2D)            (None, 7, 7, 128)    172160      ['concatenate_79[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_165 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_164[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_80 (Concatenate)   (None, 7, 7, 1376)   0           ['conv2d_165[0][0]',             \n",
      "                                                                  'concatenate_79[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_166 (Conv2D)            (None, 7, 7, 128)    176256      ['concatenate_80[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_167 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_166[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_81 (Concatenate)   (None, 7, 7, 1408)   0           ['conv2d_167[0][0]',             \n",
      "                                                                  'concatenate_80[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_168 (Conv2D)            (None, 7, 7, 128)    180352      ['concatenate_81[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_169 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_168[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_82 (Concatenate)   (None, 7, 7, 1440)   0           ['conv2d_169[0][0]',             \n",
      "                                                                  'concatenate_81[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_170 (Conv2D)            (None, 7, 7, 128)    184448      ['concatenate_82[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_171 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_170[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_83 (Concatenate)   (None, 7, 7, 1472)   0           ['conv2d_171[0][0]',             \n",
      "                                                                  'concatenate_82[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_172 (Conv2D)            (None, 7, 7, 128)    188544      ['concatenate_83[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_173 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_172[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_84 (Concatenate)   (None, 7, 7, 1504)   0           ['conv2d_173[0][0]',             \n",
      "                                                                  'concatenate_83[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_174 (Conv2D)            (None, 7, 7, 128)    192640      ['concatenate_84[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_175 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_174[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_85 (Concatenate)   (None, 7, 7, 1536)   0           ['conv2d_175[0][0]',             \n",
      "                                                                  'concatenate_84[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_176 (Conv2D)            (None, 7, 7, 128)    196736      ['concatenate_85[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_177 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_176[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_86 (Concatenate)   (None, 7, 7, 1568)   0           ['conv2d_177[0][0]',             \n",
      "                                                                  'concatenate_85[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_178 (Conv2D)            (None, 7, 7, 128)    200832      ['concatenate_86[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_179 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_178[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_87 (Concatenate)   (None, 7, 7, 1600)   0           ['conv2d_179[0][0]',             \n",
      "                                                                  'concatenate_86[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_180 (Conv2D)            (None, 7, 7, 128)    204928      ['concatenate_87[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_181 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_180[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_88 (Concatenate)   (None, 7, 7, 1632)   0           ['conv2d_181[0][0]',             \n",
      "                                                                  'concatenate_87[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_182 (Conv2D)            (None, 7, 7, 128)    209024      ['concatenate_88[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_183 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_182[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_89 (Concatenate)   (None, 7, 7, 1664)   0           ['conv2d_183[0][0]',             \n",
      "                                                                  'concatenate_88[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_184 (Conv2D)            (None, 7, 7, 128)    213120      ['concatenate_89[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_185 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_184[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_90 (Concatenate)   (None, 7, 7, 1696)   0           ['conv2d_185[0][0]',             \n",
      "                                                                  'concatenate_89[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_186 (Conv2D)            (None, 7, 7, 128)    217216      ['concatenate_90[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_187 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_186[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_91 (Concatenate)   (None, 7, 7, 1728)   0           ['conv2d_187[0][0]',             \n",
      "                                                                  'concatenate_90[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_188 (Conv2D)            (None, 7, 7, 128)    221312      ['concatenate_91[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_189 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_188[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_92 (Concatenate)   (None, 7, 7, 1760)   0           ['conv2d_189[0][0]',             \n",
      "                                                                  'concatenate_91[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_190 (Conv2D)            (None, 7, 7, 128)    225408      ['concatenate_92[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_191 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_190[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_93 (Concatenate)   (None, 7, 7, 1792)   0           ['conv2d_191[0][0]',             \n",
      "                                                                  'concatenate_92[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_192 (Conv2D)            (None, 7, 7, 128)    229504      ['concatenate_93[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_193 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_192[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_94 (Concatenate)   (None, 7, 7, 1824)   0           ['conv2d_193[0][0]',             \n",
      "                                                                  'concatenate_93[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_194 (Conv2D)            (None, 7, 7, 128)    233600      ['concatenate_94[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_195 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_194[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_95 (Concatenate)   (None, 7, 7, 1856)   0           ['conv2d_195[0][0]',             \n",
      "                                                                  'concatenate_94[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_196 (Conv2D)            (None, 7, 7, 128)    237696      ['concatenate_95[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_197 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_196[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_96 (Concatenate)   (None, 7, 7, 1888)   0           ['conv2d_197[0][0]',             \n",
      "                                                                  'concatenate_95[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_198 (Conv2D)            (None, 7, 7, 128)    241792      ['concatenate_96[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_199 (Conv2D)            (None, 7, 7, 32)     36896       ['conv2d_198[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_97 (Concatenate)   (None, 7, 7, 1920)   0           ['conv2d_199[0][0]',             \n",
      "                                                                  'concatenate_96[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1920)        0           ['concatenate_97[0][0]']         \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            9605        ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 17,890,501\n",
      "Trainable params: 17,890,501\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57c4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#Checkpoint to save the best model per epoch\n",
    "model_path=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
    "checkpoint=ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8154db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#model.fit(x=training_set,validation_data=validation_set,epochs=15,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caef6743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 261954.2500 - accuracy: 0.3532\n",
      "Epoch 1: val_loss improved from inf to 232.16656, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense01-0.2791.hdf5\n",
      "276/276 [==============================] - 253s 464ms/step - loss: 261954.2500 - accuracy: 0.3532 - val_loss: 232.1666 - val_accuracy: 0.2791\n",
      "Epoch 2/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 117.4352 - accuracy: 0.4484\n",
      "Epoch 2: val_loss improved from 232.16656 to 68.71246, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense02-0.5000.hdf5\n",
      "276/276 [==============================] - 49s 174ms/step - loss: 117.4352 - accuracy: 0.4484 - val_loss: 68.7125 - val_accuracy: 0.5000\n",
      "Epoch 3/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 90.3976 - accuracy: 0.4462\n",
      "Epoch 3: val_loss improved from 68.71246 to 48.56048, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense03-0.5814.hdf5\n",
      "276/276 [==============================] - 50s 176ms/step - loss: 90.3976 - accuracy: 0.4462 - val_loss: 48.5605 - val_accuracy: 0.5814\n",
      "Epoch 4/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 70.3629 - accuracy: 0.5065\n",
      "Epoch 4: val_loss improved from 48.56048 to 42.91690, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense04-0.5814.hdf5\n",
      "276/276 [==============================] - 49s 173ms/step - loss: 70.3629 - accuracy: 0.5065 - val_loss: 42.9169 - val_accuracy: 0.5814\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 55.7634 - accuracy: 0.5189\n",
      "Epoch 5: val_loss improved from 42.91690 to 27.20257, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense05-0.5581.hdf5\n",
      "276/276 [==============================] - 50s 174ms/step - loss: 55.7634 - accuracy: 0.5189 - val_loss: 27.2026 - val_accuracy: 0.5581\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 29.5763 - accuracy: 0.5480\n",
      "Epoch 6: val_loss did not improve from 27.20257\n",
      "276/276 [==============================] - 48s 168ms/step - loss: 29.5763 - accuracy: 0.5480 - val_loss: 49.7596 - val_accuracy: 0.5233\n",
      "Epoch 7/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 30.7245 - accuracy: 0.5560\n",
      "Epoch 7: val_loss did not improve from 27.20257\n",
      "276/276 [==============================] - 48s 168ms/step - loss: 30.7245 - accuracy: 0.5560 - val_loss: 27.6777 - val_accuracy: 0.5872\n",
      "Epoch 8/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 38.7262 - accuracy: 0.5407\n",
      "Epoch 8: val_loss did not improve from 27.20257\n",
      "276/276 [==============================] - 47s 167ms/step - loss: 38.7262 - accuracy: 0.5407 - val_loss: 114.1020 - val_accuracy: 0.3023\n",
      "Epoch 9/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 42.8959 - accuracy: 0.5414\n",
      "Epoch 9: val_loss did not improve from 27.20257\n",
      "276/276 [==============================] - 48s 168ms/step - loss: 42.8959 - accuracy: 0.5414 - val_loss: 55.3846 - val_accuracy: 0.5116\n",
      "Epoch 10/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 29.1829 - accuracy: 0.5828\n",
      "Epoch 10: val_loss improved from 27.20257 to 26.19956, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense10-0.6453.hdf5\n",
      "276/276 [==============================] - 49s 173ms/step - loss: 29.1829 - accuracy: 0.5828 - val_loss: 26.1996 - val_accuracy: 0.6453\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 24.5966 - accuracy: 0.5625\n",
      "Epoch 11: val_loss did not improve from 26.19956\n",
      "276/276 [==============================] - 48s 168ms/step - loss: 24.5966 - accuracy: 0.5625 - val_loss: 50.4916 - val_accuracy: 0.3663\n",
      "Epoch 12/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 17.9819 - accuracy: 0.6090\n",
      "Epoch 12: val_loss improved from 26.19956 to 12.61147, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense12-0.6570.hdf5\n",
      "276/276 [==============================] - 49s 174ms/step - loss: 17.9819 - accuracy: 0.6090 - val_loss: 12.6115 - val_accuracy: 0.6570\n",
      "Epoch 13/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 12.4972 - accuracy: 0.6439\n",
      "Epoch 13: val_loss improved from 12.61147 to 12.28483, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense13-0.6047.hdf5\n",
      "276/276 [==============================] - 49s 173ms/step - loss: 12.4972 - accuracy: 0.6439 - val_loss: 12.2848 - val_accuracy: 0.6047\n",
      "Epoch 14/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 20.4861 - accuracy: 0.5799\n",
      "Epoch 14: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: 20.4861 - accuracy: 0.5799 - val_loss: 25.1019 - val_accuracy: 0.5640\n",
      "Epoch 15/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 67.5118 - accuracy: 0.5044\n",
      "Epoch 15: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: 67.5118 - accuracy: 0.5044 - val_loss: 134.3580 - val_accuracy: 0.3779\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2747\n",
      "Epoch 16: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2747 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 17: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 18: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 19/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 19: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 20/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 20: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 21: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 22/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 22: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 23: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 24/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 24: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 25/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 25: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 26/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 26: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 27: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 28/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 28: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 29/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 29: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 30/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 30: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 31/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 31: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 32/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 32: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 33/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 33: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 34/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 34: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 35/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 35: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 175ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 36/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 36: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 37/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 37: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 170ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 38/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 38: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 39/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 39: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 40/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 40: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 41/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 41: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 42/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 42: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 43/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 43: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 44/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 44: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 45/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 45: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 46/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 46: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 47/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 47: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 48: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 49/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 49: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 50/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 50: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 51/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 51: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 52/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 52: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 53/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 53: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 54/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 54: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 55/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 55: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 56/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 56: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 57: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 58/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 58: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 59/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 59: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 60/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 60: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 61/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 61: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 62/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 62: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 63/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 63: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 64/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 64: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 65/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 65: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 66/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 66: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 67/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 67: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 68/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 68: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 69/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 69: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 70/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 70: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 71/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 71: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 72/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 72: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 73/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 73: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 74/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 74: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 75/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 75: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 76/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 76: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 77/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 77: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 78/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 78: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 79/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 79: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 80/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 80: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 81/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 81: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 82/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 82: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 83/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 83: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 84/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 84: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 85/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 85: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 86/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 86: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 87: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 88/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 88: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 89/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 89: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 90/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 90: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 170ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 91/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 91: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 170ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 92/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 92: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 49s 172ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 93/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 93: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 163ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 94/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 94: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 163ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 95/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 95: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 163ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 96/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 96: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 97/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 97: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 98/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 98: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 99/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 99: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 100/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 100: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 101/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 101: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 102/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 102: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 103/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 103: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 104/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 104: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 105/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 105: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 106/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 106: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 107/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 107: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 108/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 108: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 109/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 109: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 110/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 110: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 111/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 111: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 112/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 112: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 113/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 113: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 114/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 114: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 115/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 115: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 116/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 116: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 117: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 118/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 118: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 49s 174ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 119/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 119: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 120/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 120: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 163ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 121/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 121: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 163ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 122/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 122: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 123/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 123: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 124/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 124: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 125/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 125: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 177ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 126/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 126: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 177ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 127/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 127: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 178ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 128/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 128: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 51s 179ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 129/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 129: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 177ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 130/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 130: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 177ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 131/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 131: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 178ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 132/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 132: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 177ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 133/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 133: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 51s 179ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 134/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 134: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 49s 175ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 135/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 135: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 175ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 136/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 136: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 49s 174ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 137/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 137: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 49s 172ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 138/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 138: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 49s 171ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 139/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 139: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 140/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 140: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 141/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 141: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 142/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 142: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 143/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 143: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 50s 175ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 144/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 144: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 145/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 145: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 46s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 146/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 146: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 147: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 148/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 148: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 149/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 149: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 150/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 150: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 151/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 151: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 164ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 152/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 152: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 153/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 153: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 154/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 154: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 155/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 155: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 156/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 156: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 170ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 157/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 157: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 158/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 158: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 159/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 159: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 160/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 160: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 161/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 161: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 162/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 162: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 163/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 163: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 164/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 164: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 165/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 165: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 166/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 166: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 167/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 167: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 168/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 168: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 169/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 169: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 170/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 170: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 171/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 171: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 172/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 172: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 173/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 173: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 174/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 174: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 175/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 175: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 176/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 176: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 177: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 178/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 178: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 179/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 179: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 49s 172ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 180/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 180: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 48s 169ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 181/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 181: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 182/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 182: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 168ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 183/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 183: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 184/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 184: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 185/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 185: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 186/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 186: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 165ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 187/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 187: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 188/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 188: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 189/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 189: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 190/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 190: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 191/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 191: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 192/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 192: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 193/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 193: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 194/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 194: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 195/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 195: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 196/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 196: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 197/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 197: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 166ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 198/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 198: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 199/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 199: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n",
      "Epoch 200/200\n",
      "276/276 [==============================] - ETA: 0s - loss: nan - accuracy: 0.2616\n",
      "Epoch 200: val_loss did not improve from 12.28483\n",
      "276/276 [==============================] - 47s 167ms/step - loss: nan - accuracy: 0.2616 - val_loss: nan - val_accuracy: 0.2616\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "history=model.fit(\n",
    "    training_set,\n",
    "    epochs=200,\n",
    "    validation_data=test_set,\n",
    "    batch_size=5,\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "280ac620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model,load_model\n",
    "model=load_model(r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\Densenet_Checkpoints\\Dense13-0.6047.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "053a4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]], shape=(172, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ar=np.empty(0)\n",
    "for im,y in test_set:\n",
    "    ar=np.append(ar,y)\n",
    "yt=np.zeros((172,5))\n",
    "count=0\n",
    "for i in range(0,172):\n",
    "    for j in range(5):\n",
    "        yt[i][j]=ar[count]\n",
    "        count+=1\n",
    "yt=tf.convert_to_tensor(yt,dtype=tf.float32)\n",
    "print(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e371bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 6s 109ms/step\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "yp=model.predict(test_set)\n",
    "arr=np.zeros(yp.shape)\n",
    "for i in range(yp.shape[0]):\n",
    "    for j in range(yp.shape[1]):\n",
    "        c=yp[i].argmax()\n",
    "        arr[i][c]=1\n",
    "yp=arr\n",
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad35be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.6046511627906976\n",
      "Precision= 0.8571428571428571\n",
      "Recall= 1.0\n",
      "F1 Score= 0.888888888888889\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(yt,yp)\n",
    "print('Accuracy=',accuracy)\n",
    "precision=precision_score(yt,yp,average=None)\n",
    "print('Precision=',precision[precision.argmax()])\n",
    "recall=recall_score(yt,yp,average=None)\n",
    "print('Recall=',recall[recall.argmax()])\n",
    "f1=f1_score(yt,yp,average=None)\n",
    "print('F1 Score=',f1[f1.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79077122",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd637fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Train acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Test acc')\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Train loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Test loss')\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba832aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[9.9918884e-01 8.1117853e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "['Groundnut__Alternaria__Leafspot', 'Groundnut__Early__Late__Leafspot', 'Groundnut__Healthy', 'Groundnut__Rosette', 'Groundnut__Rust']\n",
      "The disease of the given groundnut leaf is Groundnut__Alternaria__Leafspot predicted with 99.91888403892517 % confidence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image=tf.keras.utils.load_img(r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Images for checking\\img.jpg',target_size=(224,224))\n",
    "test_image=tf.keras.utils.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=model.predict(test_image)\n",
    "result=result.flatten()\n",
    "print(result)\n",
    "print(class_names)\n",
    "index=result.argmax()\n",
    "confidence=result[index]*100;\n",
    "pred_class=class_names[index]\n",
    "if pred_class!='Groundnut__Healthy':\n",
    "    print(f'The disease of the given groundnut leaf is {pred_class} predicted with {confidence} % confidence')\n",
    "else:\n",
    "    print(f'The groundnut leaf is healthy predicted with {confidence} % confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d27dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
