{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ykAZa7jcxm7c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing import image\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout, Input, Conv2D, DepthwiseConv2D, Dense, GlobalAveragePooling2D\n",
    "from keras.layers import BatchNormalization, ReLU, Multiply, Reshape, Add\n",
    "from keras.models import Model\n",
    "from keras.initializers import he_normal\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a8fMkUNOdnr5"
   },
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "train_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Train'\n",
    "test_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Test'\n",
    "valid_dir=r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Dataset\\Valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P9bSHqKPdnr7"
   },
   "outputs": [],
   "source": [
    "#Rescaling and augmentation of data\n",
    "data_augmentation=tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\",input_shape=(224,224,3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "],name=\"data_augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnuvkqsRdnr-",
    "outputId": "5e6f6709-c15b-436c-81c3-b813bb7e5982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1376 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n",
      "Found 172 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE=(224,224)\n",
    "BATCH_SIZE=5\n",
    "training_set=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=train_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = test_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode = 'categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory = valid_dir,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode = 'categorical',\n",
    ")\n",
    "class_names=validation_set.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CDkmBpoxX1nC"
   },
   "outputs": [],
   "source": [
    "# Add Rescaling outside the data_augmentation\n",
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "\n",
    "training_set = training_set.map(lambda x, y: (rescale(x), y))\n",
    "test_set = test_set.map(lambda x, y: (rescale(x), y))\n",
    "validation_set = validation_set.map(lambda x, y: (rescale(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EftB31iv9Tcg"
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * tf.keras.activations.sigmoid(x)\n",
    "\n",
    "def SEBlock(input_x, filters, se_ratio=0.25):\n",
    "    se = GlobalAveragePooling2D()(input_x)\n",
    "    se = Reshape((1, 1, filters))(se)\n",
    "    se = Dense(filters * se_ratio, activation=swish)(se)\n",
    "    se = Dense(filters, activation='sigmoid')(se)\n",
    "    return Multiply()([input_x, se])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zC1bKTCy9Wq3"
   },
   "outputs": [],
   "source": [
    "def MBConv(input_x, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio=0.25, drop_connect_rate=0.2):\n",
    "    # Expansion phase\n",
    "    x = Conv2D(in_channels * expand_ratio, (1, 1), padding='same', use_bias=False)(input_x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = swish(x)\n",
    "\n",
    "    # Depthwise Convolution\n",
    "    x = DepthwiseConv2D(kernel_size, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = swish(x)\n",
    "\n",
    "    # Squeeze and Excitation\n",
    "    x = SEBlock(x, in_channels * expand_ratio, se_ratio)\n",
    "\n",
    "    # Projection phase\n",
    "    x = Conv2D(out_channels, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Adding DropConnect\n",
    "    if drop_connect_rate:\n",
    "        x = tf.keras.layers.Dropout(drop_connect_rate)(x)\n",
    "\n",
    "    # Adding skip connection\n",
    "    if stride == 1 and in_channels == out_channels:\n",
    "        x = Add()([x, input_x])\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B_baWjA69ZJv"
   },
   "outputs": [],
   "source": [
    "def EfficientNetB5(input_shape=(224, 224, 3), num_classes=5):\n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    # Stem\n",
    "    x = Conv2D(48, (3, 3), strides=(2, 2), padding='same', use_bias=False)(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = swish(x)\n",
    "\n",
    "    # MBConv Blocks\n",
    "    blocks_args = [\n",
    "        (3, 24, 3, 1, 1),\n",
    "        (5, 48, 3, 2, 6),\n",
    "        (5, 64, 5, 2, 6),\n",
    "        (10, 128, 3, 2, 6),\n",
    "        (10, 176, 5, 1, 6),\n",
    "        (7, 304, 5, 2, 6),\n",
    "        (7, 512, 3, 1, 6)\n",
    "    ]\n",
    "    for (repeats, out_channels, kernel_size, stride, expand_ratio) in blocks_args:\n",
    "        x = MBConv(x, x.shape[-1], out_channels, kernel_size, stride, expand_ratio)\n",
    "\n",
    "    # Top layers\n",
    "    x = Conv2D(2048, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = swish(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y1h9OqVWkxvf"
   },
   "outputs": [],
   "source": [
    "model=EfficientNetB5(input_shape=(224,224,3),num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_n3ie4TdnsK",
    "outputId": "f1fb8f7c-e0f3-418d-a91f-0700cc182c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 112, 112, 48  1296        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 112, 112, 48  192        ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.sigmoid (TFOpLambda)   (None, 112, 112, 48  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 112, 112, 48  0           ['batch_normalization[0][0]',    \n",
      "                                )                                 'tf.math.sigmoid[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 112, 112, 48  2304        ['tf.math.multiply[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 112, 112, 48  192        ['conv2d_1[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_1 (TFOpLambda)  (None, 112, 112, 48  0          ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 112, 112, 48  0          ['batch_normalization_1[0][0]',  \n",
      " )                              )                                 'tf.math.sigmoid_1[0][0]']      \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 112, 112, 48  432        ['tf.math.multiply_1[0][0]']     \n",
      " v2D)                           )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 112, 112, 48  192        ['depthwise_conv2d[0][0]']       \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_2 (TFOpLambda)  (None, 112, 112, 48  0          ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 112, 112, 48  0          ['batch_normalization_2[0][0]',  \n",
      " )                              )                                 'tf.math.sigmoid_2[0][0]']      \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 48)          0           ['tf.math.multiply_2[0][0]']     \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1, 48)     0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1, 1, 12)     588         ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1, 1, 48)     624         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 112, 112, 48  0           ['tf.math.multiply_2[0][0]',     \n",
      "                                )                                 'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 112, 112, 24  1152        ['multiply[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 112, 112, 24  96         ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 112, 112, 24  0           ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 112, 112, 14  3456        ['dropout[0][0]']                \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 112, 112, 14  576        ['conv2d_3[0][0]']               \n",
      " rmalization)                   4)                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_3 (TFOpLambda)  (None, 112, 112, 14  0          ['batch_normalization_4[0][0]']  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 112, 112, 14  0          ['batch_normalization_4[0][0]',  \n",
      " )                              4)                                'tf.math.sigmoid_3[0][0]']      \n",
      "                                                                                                  \n",
      " depthwise_conv2d_1 (DepthwiseC  (None, 56, 56, 144)  1296       ['tf.math.multiply_3[0][0]']     \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 56, 56, 144)  576        ['depthwise_conv2d_1[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_4 (TFOpLambda)  (None, 56, 56, 144)  0          ['batch_normalization_5[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 56, 56, 144)  0          ['batch_normalization_5[0][0]',  \n",
      " )                                                                'tf.math.sigmoid_4[0][0]']      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 144)         0           ['tf.math.multiply_4[0][0]']     \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 1, 144)    0           ['global_average_pooling2d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1, 1, 36)     5220        ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1, 1, 144)    5328        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 56, 56, 144)  0           ['tf.math.multiply_4[0][0]',     \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 56, 56, 48)   6912        ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 56, 56, 48)  192         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 56, 56, 48)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 56, 56, 288)  13824       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 56, 56, 288)  1152       ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_5 (TFOpLambda)  (None, 56, 56, 288)  0          ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 56, 56, 288)  0          ['batch_normalization_7[0][0]',  \n",
      " )                                                                'tf.math.sigmoid_5[0][0]']      \n",
      "                                                                                                  \n",
      " depthwise_conv2d_2 (DepthwiseC  (None, 28, 28, 288)  7200       ['tf.math.multiply_5[0][0]']     \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 28, 28, 288)  1152       ['depthwise_conv2d_2[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_6 (TFOpLambda)  (None, 28, 28, 288)  0          ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 28, 28, 288)  0          ['batch_normalization_8[0][0]',  \n",
      " )                                                                'tf.math.sigmoid_6[0][0]']      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 288)         0           ['tf.math.multiply_6[0][0]']     \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 1, 288)    0           ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1, 1, 72)     20808       ['reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1, 1, 288)    21024       ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)          (None, 28, 28, 288)  0           ['tf.math.multiply_6[0][0]',     \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 28, 28, 64)   18432       ['multiply_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 28, 28, 64)  256         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 28, 28, 64)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 28, 28, 384)  24576       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 28, 28, 384)  1536       ['conv2d_7[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_7 (TFOpLambda)  (None, 28, 28, 384)  0          ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 28, 28, 384)  0          ['batch_normalization_10[0][0]', \n",
      " )                                                                'tf.math.sigmoid_7[0][0]']      \n",
      "                                                                                                  \n",
      " depthwise_conv2d_3 (DepthwiseC  (None, 14, 14, 384)  3456       ['tf.math.multiply_7[0][0]']     \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 14, 14, 384)  1536       ['depthwise_conv2d_3[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_8 (TFOpLambda)  (None, 14, 14, 384)  0          ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.math.multiply_8 (TFOpLambda  (None, 14, 14, 384)  0          ['batch_normalization_11[0][0]', \n",
      " )                                                                'tf.math.sigmoid_8[0][0]']      \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 384)         0           ['tf.math.multiply_8[0][0]']     \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 1, 384)    0           ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1, 1, 96)     36960       ['reshape_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1, 1, 384)    37248       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)          (None, 14, 14, 384)  0           ['tf.math.multiply_8[0][0]',     \n",
      "                                                                  'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 14, 14, 128)  49152       ['multiply_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 14, 14, 128)  512        ['conv2d_8[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 14, 14, 128)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 14, 14, 768)  98304       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 14, 14, 768)  3072       ['conv2d_9[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_9 (TFOpLambda)  (None, 14, 14, 768)  0          ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  (None, 14, 14, 768)  0          ['batch_normalization_13[0][0]', \n",
      " )                                                                'tf.math.sigmoid_9[0][0]']      \n",
      "                                                                                                  \n",
      " depthwise_conv2d_4 (DepthwiseC  (None, 14, 14, 768)  19200      ['tf.math.multiply_9[0][0]']     \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 14, 14, 768)  3072       ['depthwise_conv2d_4[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_10 (TFOpLambda  (None, 14, 14, 768)  0          ['batch_normalization_14[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpLambd  (None, 14, 14, 768)  0          ['batch_normalization_14[0][0]', \n",
      " a)                                                               'tf.math.sigmoid_10[0][0]']     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4 (Gl  (None, 768)         0           ['tf.math.multiply_10[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 1, 768)    0           ['global_average_pooling2d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1, 1, 192)    147648      ['reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1, 1, 768)    148224      ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)          (None, 14, 14, 768)  0           ['tf.math.multiply_10[0][0]',    \n",
      "                                                                  'dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 14, 176)  135168      ['multiply_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 14, 14, 176)  704        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 14, 14, 176)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 14, 14, 1056  185856      ['dropout_4[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 14, 14, 1056  4224       ['conv2d_11[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_11 (TFOpLambda  (None, 14, 14, 1056  0          ['batch_normalization_16[0][0]'] \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpLambd  (None, 14, 14, 1056  0          ['batch_normalization_16[0][0]', \n",
      " a)                             )                                 'tf.math.sigmoid_11[0][0]']     \n",
      "                                                                                                  \n",
      " depthwise_conv2d_5 (DepthwiseC  (None, 7, 7, 1056)  26400       ['tf.math.multiply_11[0][0]']    \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 7, 7, 1056)  4224        ['depthwise_conv2d_5[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.math.sigmoid_12 (TFOpLambda  (None, 7, 7, 1056)  0           ['batch_normalization_17[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_12 (TFOpLambd  (None, 7, 7, 1056)  0           ['batch_normalization_17[0][0]', \n",
      " a)                                                               'tf.math.sigmoid_12[0][0]']     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_5 (Gl  (None, 1056)        0           ['tf.math.multiply_12[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 1, 1056)   0           ['global_average_pooling2d_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1, 1, 264)    279048      ['reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1, 1, 1056)   279840      ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_5 (Multiply)          (None, 7, 7, 1056)   0           ['tf.math.multiply_12[0][0]',    \n",
      "                                                                  'dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 7, 7, 304)    321024      ['multiply_5[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 7, 7, 304)   1216        ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 7, 7, 304)    0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 7, 7, 1824)   554496      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 7, 7, 1824)  7296        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_13 (TFOpLambda  (None, 7, 7, 1824)  0           ['batch_normalization_19[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_13 (TFOpLambd  (None, 7, 7, 1824)  0           ['batch_normalization_19[0][0]', \n",
      " a)                                                               'tf.math.sigmoid_13[0][0]']     \n",
      "                                                                                                  \n",
      " depthwise_conv2d_6 (DepthwiseC  (None, 7, 7, 1824)  16416       ['tf.math.multiply_13[0][0]']    \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 7, 7, 1824)  7296        ['depthwise_conv2d_6[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_14 (TFOpLambda  (None, 7, 7, 1824)  0           ['batch_normalization_20[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_14 (TFOpLambd  (None, 7, 7, 1824)  0           ['batch_normalization_20[0][0]', \n",
      " a)                                                               'tf.math.sigmoid_14[0][0]']     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_6 (Gl  (None, 1824)        0           ['tf.math.multiply_14[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)            (None, 1, 1, 1824)   0           ['global_average_pooling2d_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 1, 1, 456)    832200      ['reshape_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1, 1, 1824)   833568      ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " multiply_6 (Multiply)          (None, 7, 7, 1824)   0           ['tf.math.multiply_14[0][0]',    \n",
      "                                                                  'dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 7, 7, 512)    933888      ['multiply_6[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 7, 7, 512)    0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 7, 7, 2048)   1048576     ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_15 (TFOpLambda  (None, 7, 7, 2048)  0           ['batch_normalization_22[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_15 (TFOpLambd  (None, 7, 7, 2048)  0           ['batch_normalization_22[0][0]', \n",
      " a)                                                               'tf.math.sigmoid_15[0][0]']     \n",
      "                                                                                                  \n",
      " global_average_pooling2d_7 (Gl  (None, 2048)        0           ['tf.math.multiply_15[0][0]']    \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_14 (Dense)               (None, 5)            10245       ['global_average_pooling2d_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,180,893\n",
      "Trainable params: 6,156,141\n",
      "Non-trainable params: 24,752\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sMuGHoE5_gqy"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(learning_rate=5e-5)\n",
    "\n",
    "# Recompile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reset the callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=r\"C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_{epoch:02d}_{val_accuracy:.4f}.hdf5\",\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 1e-4\n",
    "    decay_rate = 0.97\n",
    "    decay_step = 1\n",
    "    lrate = initial_lr * (decay_rate ** (epoch // decay_step))\n",
    "    return lrate\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "callbacks_list = [lr_scheduler, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrL5zwpc_l6J",
    "outputId": "4a62e3f7-e818-4635-9876-740007106d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 1.1953 - accuracy: 0.5015\n",
      "Epoch 1: val_loss improved from inf to 1.49550, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_01_0.2616.hdf5\n",
      "276/276 [==============================] - 108s 287ms/step - loss: 1.1953 - accuracy: 0.5015 - val_loss: 1.4955 - val_accuracy: 0.2616 - lr: 1.0000e-04\n",
      "Epoch 2/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.9329 - accuracy: 0.6167\n",
      "Epoch 2: val_loss did not improve from 1.49550\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.9328 - accuracy: 0.6170 - val_loss: 1.5857 - val_accuracy: 0.3488 - lr: 9.7000e-05\n",
      "Epoch 3/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.8041 - accuracy: 0.6742\n",
      "Epoch 3: val_loss did not improve from 1.49550\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.8067 - accuracy: 0.6737 - val_loss: 1.5297 - val_accuracy: 0.4593 - lr: 9.4090e-05\n",
      "Epoch 4/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.7301 - accuracy: 0.7229\n",
      "Epoch 4: val_loss did not improve from 1.49550\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.7309 - accuracy: 0.7224 - val_loss: 1.5596 - val_accuracy: 0.4651 - lr: 9.1267e-05\n",
      "Epoch 5/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.7573\n",
      "Epoch 5: val_loss improved from 1.49550 to 1.19984, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_05_0.6047.hdf5\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.6511 - accuracy: 0.7573 - val_loss: 1.1998 - val_accuracy: 0.6047 - lr: 8.8529e-05\n",
      "Epoch 6/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.6132 - accuracy: 0.7544\n",
      "Epoch 6: val_loss improved from 1.19984 to 1.01895, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_06_0.6337.hdf5\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.6132 - accuracy: 0.7544 - val_loss: 1.0189 - val_accuracy: 0.6337 - lr: 8.5873e-05\n",
      "Epoch 7/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.5449 - accuracy: 0.7811\n",
      "Epoch 7: val_loss did not improve from 1.01895\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.5455 - accuracy: 0.7805 - val_loss: 1.1967 - val_accuracy: 0.5930 - lr: 8.3297e-05\n",
      "Epoch 8/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.4855 - accuracy: 0.8116\n",
      "Epoch 8: val_loss did not improve from 1.01895\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.4856 - accuracy: 0.8118 - val_loss: 1.0947 - val_accuracy: 0.6628 - lr: 8.0798e-05\n",
      "Epoch 9/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.4550 - accuracy: 0.8269\n",
      "Epoch 9: val_loss did not improve from 1.01895\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.4561 - accuracy: 0.8263 - val_loss: 1.4943 - val_accuracy: 0.5465 - lr: 7.8374e-05\n",
      "Epoch 10/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.4378 - accuracy: 0.8349\n",
      "Epoch 10: val_loss did not improve from 1.01895\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.4383 - accuracy: 0.8343 - val_loss: 1.0862 - val_accuracy: 0.7151 - lr: 7.6023e-05\n",
      "Epoch 11/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3737 - accuracy: 0.8576\n",
      "Epoch 11: val_loss did not improve from 1.01895\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.3737 - accuracy: 0.8576 - val_loss: 1.1268 - val_accuracy: 0.6860 - lr: 7.3742e-05\n",
      "Epoch 12/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.8662\n",
      "Epoch 12: val_loss did not improve from 1.01895\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.3540 - accuracy: 0.8656 - val_loss: 1.2325 - val_accuracy: 0.6686 - lr: 7.1530e-05\n",
      "Epoch 13/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.3025 - accuracy: 0.8793\n",
      "Epoch 13: val_loss did not improve from 1.01895\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.3041 - accuracy: 0.8786 - val_loss: 1.2084 - val_accuracy: 0.6628 - lr: 6.9384e-05\n",
      "Epoch 14/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.8895\n",
      "Epoch 14: val_loss improved from 1.01895 to 1.01438, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_14_0.7151.hdf5\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.3129 - accuracy: 0.8888 - val_loss: 1.0144 - val_accuracy: 0.7151 - lr: 6.7303e-05\n",
      "Epoch 15/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.8989\n",
      "Epoch 15: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.2837 - accuracy: 0.8983 - val_loss: 1.4708 - val_accuracy: 0.6453 - lr: 6.5284e-05\n",
      "Epoch 16/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9019\n",
      "Epoch 16: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.2640 - accuracy: 0.9019 - val_loss: 1.0590 - val_accuracy: 0.7093 - lr: 6.3325e-05\n",
      "Epoch 17/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2322 - accuracy: 0.9150\n",
      "Epoch 17: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.2322 - accuracy: 0.9150 - val_loss: 1.4482 - val_accuracy: 0.6163 - lr: 6.1425e-05\n",
      "Epoch 18/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9273\n",
      "Epoch 18: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.2014 - accuracy: 0.9273 - val_loss: 1.9673 - val_accuracy: 0.5174 - lr: 5.9583e-05\n",
      "Epoch 19/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9273\n",
      "Epoch 19: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.2050 - accuracy: 0.9266 - val_loss: 1.9521 - val_accuracy: 0.5174 - lr: 5.7795e-05\n",
      "Epoch 20/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9367\n",
      "Epoch 20: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.1922 - accuracy: 0.9360 - val_loss: 1.1303 - val_accuracy: 0.7267 - lr: 5.6061e-05\n",
      "Epoch 21/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9288\n",
      "Epoch 21: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.1918 - accuracy: 0.9288 - val_loss: 1.6515 - val_accuracy: 0.5756 - lr: 5.4379e-05\n",
      "Epoch 22/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9498\n",
      "Epoch 22: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.1387 - accuracy: 0.9491 - val_loss: 1.8834 - val_accuracy: 0.5698 - lr: 5.2748e-05\n",
      "Epoch 23/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9331\n",
      "Epoch 23: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.1904 - accuracy: 0.9331 - val_loss: 2.3601 - val_accuracy: 0.4419 - lr: 5.1166e-05\n",
      "Epoch 24/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9564\n",
      "Epoch 24: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.1402 - accuracy: 0.9557 - val_loss: 1.2841 - val_accuracy: 0.6512 - lr: 4.9631e-05\n",
      "Epoch 25/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9484\n",
      "Epoch 25: val_loss did not improve from 1.01438\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.1583 - accuracy: 0.9477 - val_loss: 1.4575 - val_accuracy: 0.6163 - lr: 4.8142e-05\n",
      "Epoch 26/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9695\n",
      "Epoch 26: val_loss improved from 1.01438 to 0.93112, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_26_0.7500.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0994 - accuracy: 0.9688 - val_loss: 0.9311 - val_accuracy: 0.7500 - lr: 4.6697e-05\n",
      "Epoch 27/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1310 - accuracy: 0.9549\n",
      "Epoch 27: val_loss did not improve from 0.93112\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.1313 - accuracy: 0.9549 - val_loss: 1.5957 - val_accuracy: 0.6512 - lr: 4.5297e-05\n",
      "Epoch 28/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1050 - accuracy: 0.9651\n",
      "Epoch 28: val_loss did not improve from 0.93112\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.1071 - accuracy: 0.9644 - val_loss: 1.1132 - val_accuracy: 0.7093 - lr: 4.3938e-05\n",
      "Epoch 29/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9636\n",
      "Epoch 29: val_loss improved from 0.93112 to 0.92991, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_29_0.7849.hdf5\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.1149 - accuracy: 0.9637 - val_loss: 0.9299 - val_accuracy: 0.7849 - lr: 4.2620e-05\n",
      "Epoch 30/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1379 - accuracy: 0.9636\n",
      "Epoch 30: val_loss did not improve from 0.92991\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.1382 - accuracy: 0.9637 - val_loss: 1.1663 - val_accuracy: 0.6860 - lr: 4.1341e-05\n",
      "Epoch 31/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9600\n",
      "Epoch 31: val_loss improved from 0.92991 to 0.84261, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_31_0.7965.hdf5\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.1131 - accuracy: 0.9593 - val_loss: 0.8426 - val_accuracy: 0.7965 - lr: 4.0101e-05\n",
      "Epoch 32/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9593\n",
      "Epoch 32: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.1440 - accuracy: 0.9593 - val_loss: 1.1675 - val_accuracy: 0.6860 - lr: 3.8898e-05\n",
      "Epoch 33/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9687\n",
      "Epoch 33: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.1139 - accuracy: 0.9688 - val_loss: 1.5352 - val_accuracy: 0.6105 - lr: 3.7731e-05\n",
      "Epoch 34/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9716\n",
      "Epoch 34: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0823 - accuracy: 0.9717 - val_loss: 1.2245 - val_accuracy: 0.6919 - lr: 3.6599e-05\n",
      "Epoch 35/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9782\n",
      "Epoch 35: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0674 - accuracy: 0.9782 - val_loss: 1.4745 - val_accuracy: 0.6221 - lr: 3.5501e-05\n",
      "Epoch 36/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9767\n",
      "Epoch 36: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 42s 146ms/step - loss: 0.0828 - accuracy: 0.9760 - val_loss: 1.1374 - val_accuracy: 0.6977 - lr: 3.4436e-05\n",
      "Epoch 37/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0907 - accuracy: 0.9775\n",
      "Epoch 37: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0913 - accuracy: 0.9767 - val_loss: 1.2908 - val_accuracy: 0.6628 - lr: 3.3403e-05\n",
      "Epoch 38/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9746\n",
      "Epoch 38: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 42s 147ms/step - loss: 0.0916 - accuracy: 0.9746 - val_loss: 1.5385 - val_accuracy: 0.6453 - lr: 3.2401e-05\n",
      "Epoch 39/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9782\n",
      "Epoch 39: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0682 - accuracy: 0.9775 - val_loss: 1.4251 - val_accuracy: 0.6453 - lr: 3.1429e-05\n",
      "Epoch 40/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9825\n",
      "Epoch 40: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0772 - accuracy: 0.9826 - val_loss: 1.0359 - val_accuracy: 0.7035 - lr: 3.0486e-05\n",
      "Epoch 41/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0528 - accuracy: 0.9840\n",
      "Epoch 41: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 1.8690 - val_accuracy: 0.5988 - lr: 2.9571e-05\n",
      "Epoch 42/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9767\n",
      "Epoch 42: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0905 - accuracy: 0.9760 - val_loss: 2.1677 - val_accuracy: 0.5349 - lr: 2.8684e-05\n",
      "Epoch 43/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9811\n",
      "Epoch 43: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0840 - accuracy: 0.9811 - val_loss: 1.7476 - val_accuracy: 0.5988 - lr: 2.7824e-05\n",
      "Epoch 44/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9760\n",
      "Epoch 44: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0779 - accuracy: 0.9760 - val_loss: 1.1047 - val_accuracy: 0.7674 - lr: 2.6989e-05\n",
      "Epoch 45/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9782\n",
      "Epoch 45: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0690 - accuracy: 0.9775 - val_loss: 1.2082 - val_accuracy: 0.6977 - lr: 2.6179e-05\n",
      "Epoch 46/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9840\n",
      "Epoch 46: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0613 - accuracy: 0.9840 - val_loss: 1.1305 - val_accuracy: 0.7093 - lr: 2.5394e-05\n",
      "Epoch 47/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9811\n",
      "Epoch 47: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0687 - accuracy: 0.9811 - val_loss: 1.1064 - val_accuracy: 0.7209 - lr: 2.4632e-05\n",
      "Epoch 48/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9753\n",
      "Epoch 48: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 141ms/step - loss: 0.0665 - accuracy: 0.9753 - val_loss: 1.3031 - val_accuracy: 0.6744 - lr: 2.3893e-05\n",
      "Epoch 49/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9898\n",
      "Epoch 49: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0385 - accuracy: 0.9898 - val_loss: 1.2481 - val_accuracy: 0.7209 - lr: 2.3176e-05\n",
      "Epoch 50/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9738\n",
      "Epoch 50: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0786 - accuracy: 0.9731 - val_loss: 1.4755 - val_accuracy: 0.6570 - lr: 2.2481e-05\n",
      "Epoch 51/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9891\n",
      "Epoch 51: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0489 - accuracy: 0.9884 - val_loss: 1.5497 - val_accuracy: 0.6570 - lr: 2.1807e-05\n",
      "Epoch 52/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9840\n",
      "Epoch 52: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0687 - accuracy: 0.9840 - val_loss: 1.1778 - val_accuracy: 0.6744 - lr: 2.1152e-05\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9855\n",
      "Epoch 53: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0652 - accuracy: 0.9847 - val_loss: 1.5439 - val_accuracy: 0.6802 - lr: 2.0518e-05\n",
      "Epoch 54/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0502 - accuracy: 0.9847\n",
      "Epoch 54: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0505 - accuracy: 0.9847 - val_loss: 1.0480 - val_accuracy: 0.7326 - lr: 1.9902e-05\n",
      "Epoch 55/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9927\n",
      "Epoch 55: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0357 - accuracy: 0.9927 - val_loss: 1.2405 - val_accuracy: 0.7093 - lr: 1.9305e-05\n",
      "Epoch 56/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9789\n",
      "Epoch 56: val_loss did not improve from 0.84261\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0648 - accuracy: 0.9782 - val_loss: 0.9559 - val_accuracy: 0.7500 - lr: 1.8726e-05\n",
      "Epoch 57/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9905\n",
      "Epoch 57: val_loss improved from 0.84261 to 0.82800, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_57_0.7907.hdf5\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0365 - accuracy: 0.9906 - val_loss: 0.8280 - val_accuracy: 0.7907 - lr: 1.8164e-05\n",
      "Epoch 58/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9898\n",
      "Epoch 58: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 1.0423 - val_accuracy: 0.7442 - lr: 1.7619e-05\n",
      "Epoch 59/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9891\n",
      "Epoch 59: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0384 - accuracy: 0.9884 - val_loss: 1.1744 - val_accuracy: 0.7209 - lr: 1.7091e-05\n",
      "Epoch 60/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9884\n",
      "Epoch 60: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0453 - accuracy: 0.9884 - val_loss: 1.1333 - val_accuracy: 0.7326 - lr: 1.6578e-05\n",
      "Epoch 61/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9949\n",
      "Epoch 61: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.0228 - accuracy: 0.9949 - val_loss: 1.0190 - val_accuracy: 0.7558 - lr: 1.6081e-05\n",
      "Epoch 62/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9855\n",
      "Epoch 62: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 141ms/step - loss: 0.0485 - accuracy: 0.9855 - val_loss: 1.1896 - val_accuracy: 0.7442 - lr: 1.5598e-05\n",
      "Epoch 63/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0245 - accuracy: 0.9956\n",
      "Epoch 63: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0252 - accuracy: 0.9949 - val_loss: 0.8992 - val_accuracy: 0.7965 - lr: 1.5130e-05\n",
      "Epoch 64/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9898\n",
      "Epoch 64: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0308 - accuracy: 0.9898 - val_loss: 0.9115 - val_accuracy: 0.7849 - lr: 1.4676e-05\n",
      "Epoch 65/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 0.9891\n",
      "Epoch 65: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0445 - accuracy: 0.9891 - val_loss: 1.4455 - val_accuracy: 0.6802 - lr: 1.4236e-05\n",
      "Epoch 66/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9862\n",
      "Epoch 66: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0418 - accuracy: 0.9862 - val_loss: 1.1725 - val_accuracy: 0.7267 - lr: 1.3809e-05\n",
      "Epoch 67/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9884\n",
      "Epoch 67: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0420 - accuracy: 0.9884 - val_loss: 1.3426 - val_accuracy: 0.7035 - lr: 1.3395e-05\n",
      "Epoch 68/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9920\n",
      "Epoch 68: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 1.0587 - val_accuracy: 0.7849 - lr: 1.2993e-05\n",
      "Epoch 69/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9949\n",
      "Epoch 69: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0211 - accuracy: 0.9949 - val_loss: 1.4226 - val_accuracy: 0.7035 - lr: 1.2603e-05\n",
      "Epoch 70/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9913\n",
      "Epoch 70: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0356 - accuracy: 0.9913 - val_loss: 1.1854 - val_accuracy: 0.7326 - lr: 1.2225e-05\n",
      "Epoch 71/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9913\n",
      "Epoch 71: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 1.0030 - val_accuracy: 0.7733 - lr: 1.1858e-05\n",
      "Epoch 72/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9898\n",
      "Epoch 72: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0378 - accuracy: 0.9898 - val_loss: 1.0609 - val_accuracy: 0.7558 - lr: 1.1503e-05\n",
      "Epoch 73/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0341 - accuracy: 0.9884\n",
      "Epoch 73: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0360 - accuracy: 0.9876 - val_loss: 1.0675 - val_accuracy: 0.7384 - lr: 1.1157e-05\n",
      "Epoch 74/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0211 - accuracy: 0.9927\n",
      "Epoch 74: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0220 - accuracy: 0.9920 - val_loss: 1.1236 - val_accuracy: 0.7093 - lr: 1.0823e-05\n",
      "Epoch 75/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9956\n",
      "Epoch 75: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0210 - accuracy: 0.9956 - val_loss: 0.9637 - val_accuracy: 0.7907 - lr: 1.0498e-05\n",
      "Epoch 76/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9913\n",
      "Epoch 76: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0431 - accuracy: 0.9913 - val_loss: 1.0563 - val_accuracy: 0.7616 - lr: 1.0183e-05\n",
      "Epoch 77/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9956\n",
      "Epoch 77: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 1.2370 - val_accuracy: 0.7035 - lr: 9.8776e-06\n",
      "Epoch 78/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9906\n",
      "Epoch 78: val_loss did not improve from 0.82800\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0360 - accuracy: 0.9906 - val_loss: 1.0536 - val_accuracy: 0.7384 - lr: 9.5813e-06\n",
      "Epoch 79/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9913\n",
      "Epoch 79: val_loss improved from 0.82800 to 0.76097, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_79_0.7965.hdf5\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0338 - accuracy: 0.9913 - val_loss: 0.7610 - val_accuracy: 0.7965 - lr: 9.2938e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9927\n",
      "Epoch 80: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0342 - accuracy: 0.9927 - val_loss: 0.8818 - val_accuracy: 0.7616 - lr: 9.0150e-06\n",
      "Epoch 81/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0344 - accuracy: 0.9898\n",
      "Epoch 81: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0352 - accuracy: 0.9891 - val_loss: 0.9665 - val_accuracy: 0.7384 - lr: 8.7446e-06\n",
      "Epoch 82/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9898\n",
      "Epoch 82: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0260 - accuracy: 0.9898 - val_loss: 0.9043 - val_accuracy: 0.7733 - lr: 8.4822e-06\n",
      "Epoch 83/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0644 - accuracy: 0.9840\n",
      "Epoch 83: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0694 - accuracy: 0.9833 - val_loss: 1.1369 - val_accuracy: 0.7209 - lr: 8.2278e-06\n",
      "Epoch 84/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9935\n",
      "Epoch 84: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0300 - accuracy: 0.9927 - val_loss: 1.1021 - val_accuracy: 0.7267 - lr: 7.9809e-06\n",
      "Epoch 85/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0418 - accuracy: 0.9913\n",
      "Epoch 85: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0428 - accuracy: 0.9906 - val_loss: 1.3536 - val_accuracy: 0.6744 - lr: 7.7415e-06\n",
      "Epoch 86/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9898\n",
      "Epoch 86: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0336 - accuracy: 0.9891 - val_loss: 1.0754 - val_accuracy: 0.7209 - lr: 7.5093e-06\n",
      "Epoch 87/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9935\n",
      "Epoch 87: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0254 - accuracy: 0.9935 - val_loss: 0.9884 - val_accuracy: 0.7500 - lr: 7.2840e-06\n",
      "Epoch 88/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9949\n",
      "Epoch 88: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0186 - accuracy: 0.9949 - val_loss: 0.8994 - val_accuracy: 0.7791 - lr: 7.0655e-06\n",
      "Epoch 89/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9935\n",
      "Epoch 89: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0235 - accuracy: 0.9927 - val_loss: 1.1872 - val_accuracy: 0.7384 - lr: 6.8535e-06\n",
      "Epoch 90/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9964\n",
      "Epoch 90: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0165 - accuracy: 0.9956 - val_loss: 1.1795 - val_accuracy: 0.7209 - lr: 6.6479e-06\n",
      "Epoch 91/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0303 - accuracy: 0.9935\n",
      "Epoch 91: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0350 - accuracy: 0.9927 - val_loss: 1.0121 - val_accuracy: 0.7558 - lr: 6.4485e-06\n",
      "Epoch 92/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9956\n",
      "Epoch 92: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0203 - accuracy: 0.9956 - val_loss: 1.0173 - val_accuracy: 0.7558 - lr: 6.2550e-06\n",
      "Epoch 93/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9978\n",
      "Epoch 93: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0118 - accuracy: 0.9978 - val_loss: 0.9610 - val_accuracy: 0.7384 - lr: 6.0674e-06\n",
      "Epoch 94/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9927\n",
      "Epoch 94: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0295 - accuracy: 0.9927 - val_loss: 1.0584 - val_accuracy: 0.7558 - lr: 5.8853e-06\n",
      "Epoch 95/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9956\n",
      "Epoch 95: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0188 - accuracy: 0.9956 - val_loss: 0.8657 - val_accuracy: 0.7965 - lr: 5.7088e-06\n",
      "Epoch 96/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9964\n",
      "Epoch 96: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0159 - accuracy: 0.9964 - val_loss: 1.0630 - val_accuracy: 0.7674 - lr: 5.5375e-06\n",
      "Epoch 97/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9927\n",
      "Epoch 97: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0258 - accuracy: 0.9927 - val_loss: 1.1428 - val_accuracy: 0.7384 - lr: 5.3714e-06\n",
      "Epoch 98/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9876\n",
      "Epoch 98: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0389 - accuracy: 0.9876 - val_loss: 1.0978 - val_accuracy: 0.7384 - lr: 5.2102e-06\n",
      "Epoch 99/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9949\n",
      "Epoch 99: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0155 - accuracy: 0.9949 - val_loss: 0.9417 - val_accuracy: 0.7849 - lr: 5.0539e-06\n",
      "Epoch 100/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9891\n",
      "Epoch 100: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0484 - accuracy: 0.9891 - val_loss: 1.0321 - val_accuracy: 0.7500 - lr: 4.9023e-06\n",
      "Epoch 101/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9956\n",
      "Epoch 101: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0220 - accuracy: 0.9949 - val_loss: 0.9712 - val_accuracy: 0.7733 - lr: 4.7553e-06\n",
      "Epoch 102/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9891\n",
      "Epoch 102: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.0329 - accuracy: 0.9891 - val_loss: 0.9652 - val_accuracy: 0.7791 - lr: 4.6126e-06\n",
      "Epoch 103/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9891\n",
      "Epoch 103: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0387 - accuracy: 0.9891 - val_loss: 0.9357 - val_accuracy: 0.7849 - lr: 4.4742e-06\n",
      "Epoch 104/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9956\n",
      "Epoch 104: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0254 - accuracy: 0.9949 - val_loss: 0.9365 - val_accuracy: 0.7965 - lr: 4.3400e-06\n",
      "Epoch 105/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9935\n",
      "Epoch 105: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 1.0518 - val_accuracy: 0.7674 - lr: 4.2098e-06\n",
      "Epoch 106/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9978\n",
      "Epoch 106: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0151 - accuracy: 0.9971 - val_loss: 1.0505 - val_accuracy: 0.7674 - lr: 4.0835e-06\n",
      "Epoch 107/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9978\n",
      "Epoch 107: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0180 - accuracy: 0.9971 - val_loss: 0.9728 - val_accuracy: 0.7733 - lr: 3.9610e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0619 - accuracy: 0.9884\n",
      "Epoch 108: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0619 - accuracy: 0.9884 - val_loss: 0.9540 - val_accuracy: 0.7907 - lr: 3.8422e-06\n",
      "Epoch 109/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9920\n",
      "Epoch 109: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0301 - accuracy: 0.9913 - val_loss: 0.9173 - val_accuracy: 0.7733 - lr: 3.7269e-06\n",
      "Epoch 110/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9920\n",
      "Epoch 110: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.9587 - val_accuracy: 0.7791 - lr: 3.6151e-06\n",
      "Epoch 111/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9964\n",
      "Epoch 111: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0181 - accuracy: 0.9964 - val_loss: 0.8884 - val_accuracy: 0.7849 - lr: 3.5066e-06\n",
      "Epoch 112/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0354 - accuracy: 0.9920\n",
      "Epoch 112: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.0401 - accuracy: 0.9913 - val_loss: 0.8956 - val_accuracy: 0.7791 - lr: 3.4014e-06\n",
      "Epoch 113/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9956\n",
      "Epoch 113: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 141ms/step - loss: 0.0231 - accuracy: 0.9949 - val_loss: 0.9256 - val_accuracy: 0.7849 - lr: 3.2994e-06\n",
      "Epoch 114/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9971\n",
      "Epoch 114: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0129 - accuracy: 0.9971 - val_loss: 0.9590 - val_accuracy: 0.7791 - lr: 3.2004e-06\n",
      "Epoch 115/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9913\n",
      "Epoch 115: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 42s 146ms/step - loss: 0.0322 - accuracy: 0.9906 - val_loss: 0.8884 - val_accuracy: 0.7849 - lr: 3.1044e-06\n",
      "Epoch 116/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9964\n",
      "Epoch 116: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0205 - accuracy: 0.9956 - val_loss: 0.8619 - val_accuracy: 0.7907 - lr: 3.0113e-06\n",
      "Epoch 117/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9942\n",
      "Epoch 117: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0215 - accuracy: 0.9942 - val_loss: 0.8468 - val_accuracy: 0.7907 - lr: 2.9209e-06\n",
      "Epoch 118/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9971\n",
      "Epoch 118: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 145ms/step - loss: 0.0134 - accuracy: 0.9964 - val_loss: 0.8649 - val_accuracy: 0.7907 - lr: 2.8333e-06\n",
      "Epoch 119/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9964\n",
      "Epoch 119: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0179 - accuracy: 0.9956 - val_loss: 0.9537 - val_accuracy: 0.7733 - lr: 2.7483e-06\n",
      "Epoch 120/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9971\n",
      "Epoch 120: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0162 - accuracy: 0.9971 - val_loss: 0.8862 - val_accuracy: 0.7965 - lr: 2.6659e-06\n",
      "Epoch 121/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9949\n",
      "Epoch 121: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0225 - accuracy: 0.9942 - val_loss: 0.9721 - val_accuracy: 0.7616 - lr: 2.5859e-06\n",
      "Epoch 122/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9942\n",
      "Epoch 122: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 84s 299ms/step - loss: 0.0220 - accuracy: 0.9942 - val_loss: 0.9127 - val_accuracy: 0.7674 - lr: 2.5083e-06\n",
      "Epoch 123/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0227 - accuracy: 0.9949\n",
      "Epoch 123: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0229 - accuracy: 0.9949 - val_loss: 0.8473 - val_accuracy: 0.7907 - lr: 2.4331e-06\n",
      "Epoch 124/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9964\n",
      "Epoch 124: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0124 - accuracy: 0.9956 - val_loss: 0.8959 - val_accuracy: 0.7849 - lr: 2.3601e-06\n",
      "Epoch 125/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9920\n",
      "Epoch 125: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0302 - accuracy: 0.9920 - val_loss: 0.8551 - val_accuracy: 0.7965 - lr: 2.2893e-06\n",
      "Epoch 126/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9971\n",
      "Epoch 126: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0114 - accuracy: 0.9971 - val_loss: 0.8093 - val_accuracy: 0.8140 - lr: 2.2206e-06\n",
      "Epoch 127/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9935\n",
      "Epoch 127: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0290 - accuracy: 0.9935 - val_loss: 0.8029 - val_accuracy: 0.8140 - lr: 2.1540e-06\n",
      "Epoch 128/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9935\n",
      "Epoch 128: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0179 - accuracy: 0.9935 - val_loss: 0.7641 - val_accuracy: 0.8256 - lr: 2.0893e-06\n",
      "Epoch 129/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9942\n",
      "Epoch 129: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0250 - accuracy: 0.9942 - val_loss: 0.8474 - val_accuracy: 0.8081 - lr: 2.0267e-06\n",
      "Epoch 130/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9985\n",
      "Epoch 130: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0136 - accuracy: 0.9985 - val_loss: 0.8030 - val_accuracy: 0.8081 - lr: 1.9659e-06\n",
      "Epoch 131/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 131: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.8247 - val_accuracy: 0.8081 - lr: 1.9069e-06\n",
      "Epoch 132/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9935\n",
      "Epoch 132: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0269 - accuracy: 0.9927 - val_loss: 0.8019 - val_accuracy: 0.8081 - lr: 1.8497e-06\n",
      "Epoch 133/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 133: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.8791 - val_accuracy: 0.7965 - lr: 1.7942e-06\n",
      "Epoch 134/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9905\n",
      "Epoch 134: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0337 - accuracy: 0.9906 - val_loss: 0.8581 - val_accuracy: 0.8023 - lr: 1.7404e-06\n",
      "Epoch 135/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9978\n",
      "Epoch 135: val_loss did not improve from 0.76097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0104 - accuracy: 0.9978 - val_loss: 0.8401 - val_accuracy: 0.8081 - lr: 1.6882e-06\n",
      "Epoch 136/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9876\n",
      "Epoch 136: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0435 - accuracy: 0.9876 - val_loss: 0.8518 - val_accuracy: 0.8023 - lr: 1.6375e-06\n",
      "Epoch 137/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9956\n",
      "Epoch 137: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0175 - accuracy: 0.9949 - val_loss: 0.8145 - val_accuracy: 0.8023 - lr: 1.5884e-06\n",
      "Epoch 138/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9942\n",
      "Epoch 138: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0181 - accuracy: 0.9935 - val_loss: 0.7998 - val_accuracy: 0.8081 - lr: 1.5407e-06\n",
      "Epoch 139/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9927\n",
      "Epoch 139: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0433 - accuracy: 0.9920 - val_loss: 0.8390 - val_accuracy: 0.8023 - lr: 1.4945e-06\n",
      "Epoch 140/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9978\n",
      "Epoch 140: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0146 - accuracy: 0.9971 - val_loss: 0.8523 - val_accuracy: 0.7965 - lr: 1.4497e-06\n",
      "Epoch 141/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9927\n",
      "Epoch 141: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0291 - accuracy: 0.9927 - val_loss: 0.8520 - val_accuracy: 0.7907 - lr: 1.4062e-06\n",
      "Epoch 142/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9956\n",
      "Epoch 142: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0190 - accuracy: 0.9956 - val_loss: 0.8429 - val_accuracy: 0.8023 - lr: 1.3640e-06\n",
      "Epoch 143/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9978\n",
      "Epoch 143: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0131 - accuracy: 0.9971 - val_loss: 0.8076 - val_accuracy: 0.8081 - lr: 1.3231e-06\n",
      "Epoch 144/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9927\n",
      "Epoch 144: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0218 - accuracy: 0.9920 - val_loss: 0.7843 - val_accuracy: 0.8081 - lr: 1.2834e-06\n",
      "Epoch 145/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.9927\n",
      "Epoch 145: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 44s 153ms/step - loss: 0.0259 - accuracy: 0.9920 - val_loss: 0.7987 - val_accuracy: 0.7965 - lr: 1.2449e-06\n",
      "Epoch 146/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9913\n",
      "Epoch 146: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0339 - accuracy: 0.9906 - val_loss: 0.8656 - val_accuracy: 0.7849 - lr: 1.2075e-06\n",
      "Epoch 147/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9942\n",
      "Epoch 147: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0208 - accuracy: 0.9942 - val_loss: 0.8400 - val_accuracy: 0.7907 - lr: 1.1713e-06\n",
      "Epoch 148/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9964\n",
      "Epoch 148: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0201 - accuracy: 0.9956 - val_loss: 0.8512 - val_accuracy: 0.8023 - lr: 1.1362e-06\n",
      "Epoch 149/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9964\n",
      "Epoch 149: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.8484 - val_accuracy: 0.7907 - lr: 1.1021e-06\n",
      "Epoch 150/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9913\n",
      "Epoch 150: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0400 - accuracy: 0.9906 - val_loss: 0.8269 - val_accuracy: 0.7965 - lr: 1.0690e-06\n",
      "Epoch 151/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9956\n",
      "Epoch 151: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0210 - accuracy: 0.9949 - val_loss: 0.8100 - val_accuracy: 0.8023 - lr: 1.0370e-06\n",
      "Epoch 152/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9985\n",
      "Epoch 152: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.7755 - val_accuracy: 0.8081 - lr: 1.0058e-06\n",
      "Epoch 153/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0298 - accuracy: 0.9935\n",
      "Epoch 153: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0314 - accuracy: 0.9927 - val_loss: 0.7916 - val_accuracy: 0.8023 - lr: 9.7567e-07\n",
      "Epoch 154/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9978\n",
      "Epoch 154: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0118 - accuracy: 0.9978 - val_loss: 0.7841 - val_accuracy: 0.8023 - lr: 9.4640e-07\n",
      "Epoch 155/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9978\n",
      "Epoch 155: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.7961 - val_accuracy: 0.8023 - lr: 9.1801e-07\n",
      "Epoch 156/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9949\n",
      "Epoch 156: val_loss did not improve from 0.76097\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0178 - accuracy: 0.9949 - val_loss: 0.7612 - val_accuracy: 0.8081 - lr: 8.9047e-07\n",
      "Epoch 157/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9949\n",
      "Epoch 157: val_loss improved from 0.76097 to 0.75576, saving model to C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_157_0.8023.hdf5\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 0.7558 - val_accuracy: 0.8023 - lr: 8.6375e-07\n",
      "Epoch 158/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9935\n",
      "Epoch 158: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0264 - accuracy: 0.9927 - val_loss: 0.7648 - val_accuracy: 0.8023 - lr: 8.3784e-07\n",
      "Epoch 159/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0258 - accuracy: 0.9942\n",
      "Epoch 159: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0274 - accuracy: 0.9935 - val_loss: 0.8238 - val_accuracy: 0.7965 - lr: 8.1271e-07\n",
      "Epoch 160/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0141 - accuracy: 0.9949\n",
      "Epoch 160: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.8202 - val_accuracy: 0.8023 - lr: 7.8833e-07\n",
      "Epoch 161/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9978\n",
      "Epoch 161: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0158 - accuracy: 0.9971 - val_loss: 0.8062 - val_accuracy: 0.8023 - lr: 7.6468e-07\n",
      "Epoch 162/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 162: val_loss did not improve from 0.75576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.8439 - val_accuracy: 0.7965 - lr: 7.4174e-07\n",
      "Epoch 163/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9971\n",
      "Epoch 163: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0149 - accuracy: 0.9964 - val_loss: 0.8555 - val_accuracy: 0.7965 - lr: 7.1948e-07\n",
      "Epoch 164/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9978\n",
      "Epoch 164: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0106 - accuracy: 0.9971 - val_loss: 0.8703 - val_accuracy: 0.7965 - lr: 6.9790e-07\n",
      "Epoch 165/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9898\n",
      "Epoch 165: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0388 - accuracy: 0.9891 - val_loss: 0.8707 - val_accuracy: 0.7907 - lr: 6.7696e-07\n",
      "Epoch 166/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9964\n",
      "Epoch 166: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0174 - accuracy: 0.9956 - val_loss: 0.8284 - val_accuracy: 0.7965 - lr: 6.5665e-07\n",
      "Epoch 167/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9971\n",
      "Epoch 167: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.8310 - val_accuracy: 0.7907 - lr: 6.3695e-07\n",
      "Epoch 168/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9956\n",
      "Epoch 168: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0300 - accuracy: 0.9949 - val_loss: 0.7695 - val_accuracy: 0.8081 - lr: 6.1785e-07\n",
      "Epoch 169/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9978\n",
      "Epoch 169: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0089 - accuracy: 0.9978 - val_loss: 0.8158 - val_accuracy: 0.8023 - lr: 5.9931e-07\n",
      "Epoch 170/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9956\n",
      "Epoch 170: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0174 - accuracy: 0.9956 - val_loss: 0.8068 - val_accuracy: 0.8023 - lr: 5.8133e-07\n",
      "Epoch 171/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0258 - accuracy: 0.9898\n",
      "Epoch 171: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0275 - accuracy: 0.9891 - val_loss: 0.7952 - val_accuracy: 0.8081 - lr: 5.6389e-07\n",
      "Epoch 172/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9985\n",
      "Epoch 172: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0110 - accuracy: 0.9978 - val_loss: 0.8270 - val_accuracy: 0.8081 - lr: 5.4697e-07\n",
      "Epoch 173/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9964\n",
      "Epoch 173: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0154 - accuracy: 0.9964 - val_loss: 0.8211 - val_accuracy: 0.8023 - lr: 5.3056e-07\n",
      "Epoch 174/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9949\n",
      "Epoch 174: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0207 - accuracy: 0.9949 - val_loss: 0.8331 - val_accuracy: 0.8023 - lr: 5.1465e-07\n",
      "Epoch 175/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9920\n",
      "Epoch 175: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0251 - accuracy: 0.9913 - val_loss: 0.8100 - val_accuracy: 0.8140 - lr: 4.9921e-07\n",
      "Epoch 176/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9971\n",
      "Epoch 176: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0170 - accuracy: 0.9971 - val_loss: 0.7748 - val_accuracy: 0.8140 - lr: 4.8423e-07\n",
      "Epoch 177/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9964\n",
      "Epoch 177: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0155 - accuracy: 0.9964 - val_loss: 0.7770 - val_accuracy: 0.8140 - lr: 4.6971e-07\n",
      "Epoch 178/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9993\n",
      "Epoch 178: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0075 - accuracy: 0.9993 - val_loss: 0.8024 - val_accuracy: 0.8081 - lr: 4.5561e-07\n",
      "Epoch 179/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9913\n",
      "Epoch 179: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.8027 - val_accuracy: 0.8081 - lr: 4.4195e-07\n",
      "Epoch 180/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9971\n",
      "Epoch 180: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0145 - accuracy: 0.9964 - val_loss: 0.7944 - val_accuracy: 0.8081 - lr: 4.2869e-07\n",
      "Epoch 181/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9942\n",
      "Epoch 181: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0243 - accuracy: 0.9942 - val_loss: 0.8150 - val_accuracy: 0.8023 - lr: 4.1583e-07\n",
      "Epoch 182/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9906\n",
      "Epoch 182: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 142ms/step - loss: 0.0330 - accuracy: 0.9906 - val_loss: 0.7730 - val_accuracy: 0.8140 - lr: 4.0335e-07\n",
      "Epoch 183/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9964\n",
      "Epoch 183: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0135 - accuracy: 0.9964 - val_loss: 0.7921 - val_accuracy: 0.8081 - lr: 3.9125e-07\n",
      "Epoch 184/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9971\n",
      "Epoch 184: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0131 - accuracy: 0.9964 - val_loss: 0.8340 - val_accuracy: 0.8023 - lr: 3.7951e-07\n",
      "Epoch 185/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9978\n",
      "Epoch 185: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0143 - accuracy: 0.9971 - val_loss: 0.7943 - val_accuracy: 0.8081 - lr: 3.6813e-07\n",
      "Epoch 186/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9935\n",
      "Epoch 186: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0268 - accuracy: 0.9935 - val_loss: 0.8208 - val_accuracy: 0.8081 - lr: 3.5708e-07\n",
      "Epoch 187/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9913\n",
      "Epoch 187: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0387 - accuracy: 0.9913 - val_loss: 0.8236 - val_accuracy: 0.8023 - lr: 3.4637e-07\n",
      "Epoch 188/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9956\n",
      "Epoch 188: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 142ms/step - loss: 0.0254 - accuracy: 0.9956 - val_loss: 0.7781 - val_accuracy: 0.8140 - lr: 3.3598e-07\n",
      "Epoch 189/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9985\n",
      "Epoch 189: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.7819 - val_accuracy: 0.8081 - lr: 3.2590e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9913\n",
      "Epoch 190: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 139ms/step - loss: 0.0382 - accuracy: 0.9913 - val_loss: 0.8073 - val_accuracy: 0.8081 - lr: 3.1612e-07\n",
      "Epoch 191/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9971\n",
      "Epoch 191: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.8047 - val_accuracy: 0.8081 - lr: 3.0664e-07\n",
      "Epoch 192/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9927\n",
      "Epoch 192: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 141ms/step - loss: 0.0291 - accuracy: 0.9927 - val_loss: 0.7811 - val_accuracy: 0.8140 - lr: 2.9744e-07\n",
      "Epoch 193/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0303 - accuracy: 0.9913\n",
      "Epoch 193: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0320 - accuracy: 0.9906 - val_loss: 0.8180 - val_accuracy: 0.8081 - lr: 2.8852e-07\n",
      "Epoch 194/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9985\n",
      "Epoch 194: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.8246 - val_accuracy: 0.7965 - lr: 2.7986e-07\n",
      "Epoch 195/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9993\n",
      "Epoch 195: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 40s 140ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.8529 - val_accuracy: 0.7965 - lr: 2.7147e-07\n",
      "Epoch 196/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9942\n",
      "Epoch 196: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0156 - accuracy: 0.9942 - val_loss: 0.8235 - val_accuracy: 0.8023 - lr: 2.6332e-07\n",
      "Epoch 197/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9949\n",
      "Epoch 197: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0202 - accuracy: 0.9949 - val_loss: 0.8275 - val_accuracy: 0.7965 - lr: 2.5542e-07\n",
      "Epoch 198/200\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9935\n",
      "Epoch 198: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0187 - accuracy: 0.9935 - val_loss: 0.8394 - val_accuracy: 0.7965 - lr: 2.4776e-07\n",
      "Epoch 199/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9949\n",
      "Epoch 199: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 144ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 0.8024 - val_accuracy: 0.8081 - lr: 2.4033e-07\n",
      "Epoch 200/200\n",
      "275/276 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9956\n",
      "Epoch 200: val_loss did not improve from 0.75576\n",
      "276/276 [==============================] - 41s 143ms/step - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.8289 - val_accuracy: 0.7965 - lr: 2.3312e-07\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_set,\n",
    "    epochs=200,\n",
    "    validation_data=validation_set,\n",
    "    callbacks=callbacks_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PbxUwVhSdnsU"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model,load_model\n",
    "model=load_model(r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Models\\EfficientNetB5_Checkpoints\\EfficientB5_157_0.8023.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6mgHbuBDdnsV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]], shape=(172, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "ar=np.empty(0)\n",
    "for im,y in test_set:\n",
    "    ar=np.append(ar,y)\n",
    "yt=np.zeros((172,5))\n",
    "count=0\n",
    "for i in range(0,172):\n",
    "    for j in range(5):\n",
    "        yt[i][j]=ar[count]\n",
    "        count+=1\n",
    "yt=tf.convert_to_tensor(yt,dtype=tf.float32)\n",
    "print(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wb0nyO5TdnsW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 21s 488ms/step\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "yp=model.predict(test_set)\n",
    "arr=np.zeros(yp.shape)\n",
    "for i in range(yp.shape[0]):\n",
    "    for j in range(yp.shape[1]):\n",
    "        c=yp[i].argmax()\n",
    "        arr[i][c]=1\n",
    "yp=arr\n",
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zXkEEJ_ZdnsX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8313953488372093\n",
      "Precision= 1.0\n",
      "Recall= 1.0\n",
      "F1 Score= 0.9156626506024096\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(yt,yp)\n",
    "print('Accuracy=',accuracy)\n",
    "precision=precision_score(yt,yp,average=None)\n",
    "print('Precision=',precision[precision.argmax()])\n",
    "recall=recall_score(yt,yp,average=None)\n",
    "print('Recall=',recall[recall.argmax()])\n",
    "f1=f1_score(yt,yp,average=None)\n",
    "print('F1 Score=',f1[f1.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0z1b5xKRdnsY"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2o4PO_BRdnsY"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABKnUlEQVR4nO2deXgUVdbG35MdkpBAiGxhF1dkDavigiKbiqKOiDAgOgzqiOKI67iMo+MyoygwijiD+6comygq7isoJgjIqggoYQ1BkpAFQnK+P05fq7pT3ekmvST0+T1PP1VdVV11qrr7vveccxdiZiiKoijRS0ykDVAURVEiiwqBoihKlKNCoCiKEuWoECiKokQ5KgSKoihRjgqBoihKlKNCoCh+QETtiIiJKM6PY8cT0Ve1PY+ihAsVAuWYg4i2EdFhImrqsf17VyHcLkKmKUqdRIVAOVbZCuBK84aITgPQMHLmKErdRYVAOVZ5GcAfbe/HAXjJfgARpRHRS0SUT0S/ENHfiCjGtS+WiP5NRPuIaAuA4Q6f/R8R7SKiHUT0IBHFBmokEbUkosVEtJ+INhPRn2z7ehNRDhEVEdEeInrCtT2JiF4hogIiOkBE3xFRs0CvrSgGFQLlWOUbAI2I6GRXAT0KwCsex8wAkAagA4CzIMJxtWvfnwBcAKA7gGwAl3l89gUARwAc7zrmfADXHoWdrwPIA9DSdY1/EtFA176nADzFzI0AdATwhmv7OJfdrQFkAJgEoOworq0oAFQIlGMb4xUMArABwA6zwyYOdzJzMTNvA/A4gLGuQ/4A4Elm3s7M+wE8bPtsMwDDANzMzCXMvBfANNf5/IaIWgM4HcDtzFzOzKsA/BeWJ1MB4HgiasrMB5n5G9v2DADHM3MlM+cyc1Eg11YUOyoEyrHMywBGAxgPj7AQgKYA4gH8Ytv2C4BWrvWWALZ77DO0dX12lys0cwDAswCOC9C+lgD2M3OxFxuuAXACgI2u8M8FtvtaCuB1ItpJRI8RUXyA11aU31EhUI5ZmPkXSNJ4GIAFHrv3QWrWbW3b2sDyGnZBQi/2fYbtAA4BaMrM6a5XI2Y+NUATdwJoQkSpTjYw80/MfCVEYB4FMI+Ikpm5gpn/zsynAOgPCWH9EYpylKgQKMc61wAYyMwl9o3MXAmJuT9ERKlE1BbALbDyCG8AmExEWUTUGMAdts/uAvABgMeJqBERxRBRRyI6KxDDmHk7gGUAHnYlgLu47H0FAIhoDBFlMnMVgAOuj1UR0TlEdJorvFUEEbSqQK6tKHZUCJRjGmb+mZlzvOy+EUAJgC0AvgLwfwDmuPY9Bwm/rAawEtU9ij8CSACwHsBvAOYBaHEUJl4JoB3EO1gI4D5m/si1bwiAdUR0EJI4HsXMZQCau65XBMl9fA4JFynKUUE6MY2iKEp0ox6BoihKlKNCoCiKEuWoECiKokQ5KgSKoihRTr0bCrdp06bcrl27SJuhKIpSr8jNzd3HzJlO++qdELRr1w45Od5aAyqKoihOENEv3vZpaEhRFCXKUSFQFEWJclQIFEVRopx6lyNQFOXYpaKiAnl5eSgvL4+0KfWWpKQkZGVlIT7e/wFpVQgURakz5OXlITU1Fe3atQMRRdqcegczo6CgAHl5eWjfvr3fn9PQkKIodYby8nJkZGSoCBwlRISMjIyAPSoVAkVR6hQqArXjaJ6fCkG4WbMG+PrrSFuhKIryOyoE4ea++4C//CXSViiK4kFBQQG6deuGbt26oXnz5mjVqtXv7w8fPuzzszk5OZg8eXJA12vXrh327dtXG5ODhiaLw01xMXDoUKStUBTFg4yMDKxatQoAcP/99yMlJQW33nrr7/uPHDmCuDjnIjM7OxvZ2dnhMDMkqEcQbkpLgSNHIm2Foih+MH78eEyaNAl9+vTBbbfdhhUrVqBfv37o3r07+vfvj02bNgEAPvvsM1xwwQUAREQmTJiAs88+Gx06dMD06dNrvM4TTzyBzp07o3PnznjyyScBACUlJRg+fDi6du2Kzp07Y+7cuQCAO+64A6eccgq6dOniJlS1QT2CcFNaClRURNoKRanz3Hwz4KqgB41u3QBXOes3eXl5WLZsGWJjY1FUVIQvv/wScXFx+Oijj3DXXXdh/vz51T6zceNGfPrppyguLsaJJ56I6667zmu7/tzcXDz//PP49ttvwczo06cPzjrrLGzZsgUtW7bEkiVLAACFhYUoKCjAwoULsXHjRhARDhw4ENjNeEE9gnBTVqYegaLUIy6//HLExsYCkML48ssvR+fOnTFlyhSsW7fO8TPDhw9HYmIimjZtiuOOOw579uzxev6vvvoKl1xyCZKTk5GSkoKRI0fiyy+/xGmnnYYPP/wQt99+O7788kukpaUhLS0NSUlJuOaaa7BgwQI0bNgwKPeoHkG40dCQovhFoDX3UJGcnPz7+j333INzzjkHCxcuxLZt23D22Wc7fiYxMfH39djYWBw5iv/8CSecgJUrV+Ldd9/F3/72N5x77rm49957sWLFCnz88ceYN28eZs6ciU8++STgc3uiHkG40dCQotRbCgsL0apVKwDACy+8EJRzDhgwAIsWLUJpaSlKSkqwcOFCDBgwADt37kTDhg0xZswYTJ06FStXrsTBgwdRWFiIYcOGYdq0aVi9enVQbFCPINyUlgK22oKiKPWH2267DePGjcODDz6I4cOHB+WcPXr0wPjx49G7d28AwLXXXovu3btj6dKlmDp1KmJiYhAfH49nnnkGxcXFGDFiBMrLy8HMeOKJJ4JiAzFzUE4ULrKzs7neTkxTVQXExgLJycDBg5G2RlHqHBs2bMDJJ58caTPqPU7PkYhymdmxjauGhsKJGf9DQ0OKotQhVAjCSWmpLDVZrChKHUKFIJwYIaiqkpeiKEodQIUgnBghANQrUBSlzqBCEE7Kyqx1FQJFUeoIKgThRD0CRVHqINqPIJzYhSCcLYf27QMKC4GOHcN3TUWpZxQUFODcc88FAOzevRuxsbHIzMwEAKxYsQIJCQk+P//ZZ58hISEB/fv3r7bvhRdeQE5ODmbOnBl8w4OACkE4iZRH8Le/AZ9/DmzYEL5rKko9o6ZhqGvis88+Q0pKiqMQ1HU0NBROIpUj2LULyM8P3/UU5RghNzcXZ511Fnr27InBgwdj165dAIDp06f/PhT0qFGjsG3bNsyaNQvTpk1Dt27d8OWXX3o957Zt2zBw4EB06dIF5557Ln799VcAwJtvvonOnTuja9euOPPMMwEA69atQ+/evdGtWzd06dIFP/30U0juUz2CcBKp0NCBA0BJSfiupyjBIMLjUDMzbrzxRrz11lvIzMzE3Llzcffdd2POnDl45JFHsHXrViQmJuLAgQNIT0/HpEmT/PIibrzxRowbNw7jxo3DnDlzMHnyZCxatAgPPPAAli5dilatWv0+vPSsWbNw00034aqrrsLhw4dRWVlZu/v3ggpBOIlUaOjAAenVXFkpQ1woilIjhw4dwtq1azFo0CAAQGVlJVq0aAEA6NKlC6666ipcfPHFuPjiiwM67/Lly7FgwQIAwNixY3HbbbcBAE4//XSMHz8ef/jDHzBy5EgAQL9+/fDQQw8hLy8PI0eORKdOnYJ0d+6oEISTSAlBYaEsS0qARo3Cd11FqQ0RHoeamXHqqadi+fLl1fYtWbIEX3zxBd5++2089NBD+OGHH2p9vVmzZuHbb7/FkiVL0LNnT+Tm5mL06NHo06cPlixZgmHDhuHZZ5/FwIEDa30tTzRHEE7sOYJwh4YADQ8pSgAkJiYiPz//dyGoqKjAunXrUFVVhe3bt+Occ87Bo48+isLCQhw8eBCpqakoLi6u8bz9+/fH66+/DgB49dVXMWDAAADAzz//jD59+uCBBx5AZmYmtm/fji1btqBDhw6YPHkyRowYgTVr1oTkXlUIwkkkPIKqKqCoSNZVCBTFb2JiYjBv3jzcfvvt6Nq1K7p164Zly5ahsrISY8aMwWmnnYbu3btj8uTJSE9Px4UXXoiFCxfWmCyeMWMGnn/+eXTp0gUvv/wynnrqKQDA1KlTcdppp6Fz587o378/unbtijfeeAOdO3dGt27dsHbtWvzxj38Myb3qMNTh5MYbAdOOeMUKoFev0F+zsBBIT5f1VauArl1Df01FOUp0GOrgoMNQ12Ui0WrIPrm1egSKojigQhBOIhEaMoliQIVAURRHVAjCSSQ6lKlHoNQz6lu4uq5xNM8vZEJARK2J6FMiWk9E64joJodjiIimE9FmIlpDRD1CZU+doLQUIJL1cIWG1CNQ6hFJSUkoKChQMThKmBkFBQVISkoK6HOh7EdwBMBfmXklEaUCyCWiD5l5ve2YoQA6uV59ADzjWh6blJYCKSlAcXFkPAKdJ1mp42RlZSEvLw/5OiTKUZOUlISsrKyAPhMyIWDmXQB2udaLiWgDgFYA7EIwAsBLLPL/DRGlE1EL12ePPUpLpUNXpIRAPQKljhMfH4/27dtH2oyoIyw5AiJqB6A7gG89drUCsN32Ps+17dikrMzq2auhIUVR6gghFwIiSgEwH8DNzFx0lOeYSEQ5RJRTr13G0lIgLU3Ww+kRNGwIJCSoECiK4khIhYCI4iEi8CozL3A4ZAeA1rb3Wa5tbjDzbGbOZuZsM1FEnaWoCJg1y72pqMGEhoDwCkFaGpCcrEKgKIojoWw1RAD+B2ADMz/h5bDFAP7oaj3UF0Bhvc8PvP8+cN11wNlnyzwAduxCEM7QUHq6CoGiKF4JpUdwOoCxAAYS0SrXaxgRTSKiSa5j3gWwBcBmAM8BuD6E9oQHU9jm5gKu4WUBAMzuOYJwegQqBIqi+CCUrYa+AkA1HMMAbgiVDRHBdBpr08Z9VrBDh0QMwi0EhYVARoZcX4VAURQHtGdxsDFCkJYGHD5sbTc5g3CHhtQjUBSlBlQIgo0RgkaNnIUgEq2GNFmsKIoPVAiCTVmZTAeZnOxe6zcCkZoqy3AIAbN7slh7FiuK4oAKQbApKwMaNADi4yMfGiovFxs0NKQoig9UCIKNEYKEBHchMFPYmUliwuERmF7FGhpSFMUHKgTBxi4E9lr/vn2yPO44ICYmvELQqJEKgaIoXlEhCDbeQkNGCJo2BeLiwhMaMtdPSpJRT8vLgcrK0F9XUZR6hQpBsPEWGjJCkJEhIhEOj8BcPz5ePALAeegLRVGiGhWCYOMrNNSwobzi4sIrBAkJlhBoeEhRFA9UCIKNr9BQ06ayHq7QkLmGCoGiKD5QIQg2vjwCIwSRDA2pECiK4oEKQbDxlSOwewThEAL1CBRF8QMVgmBjDw1VVgJVVbI9EqEhJ49AexcriuJBKCevj07sHgEgBX5iooxEGqnQkLEFUI9AUZRqqBAEG08hOHwYIJKZyyIZGoqPl3UVAkVRPFAhCDb20BAgQmDCMZEMDakQKIriBRWCYFJZKQV8UpJ7aMjeqxgIX2jI7hEkJcl6eXnor6soSr1Ck8XBxAw17RkaMkKQmSnLcHcoi4+XPAUgM5UpiqLYUCEIJnYhsIeGnDyCcHcoUyFQFMULKgTBxMkjcAoNRWKIibg4SVpraEhRFA9UCIJJTaGhjAxZRiI0RCR5AvUIFEXxQIUgmPgKDaWlWdvCHRqKc7UJSExUIVAUpRraaiiY2IUgNlbWKyrcO5MB4fUIjDcAqBAoiuKICkEwsQuBwXgEJiwEhLdDmb1XcWKi5ggURamGCkEwsQuBKehNh7LUVOu4cIWGDh92FwLNESiK4oDmCIKJt1ZDprexIdyhIYOGhhRFcUA9gmBiwi4NGgDMsn74cOSEwCk0pEKgKIoH6hEEE2+thjyFwCk0VFIC5OUF1x4nj0BzBIqieBB9QvD998DGjaE5d21CQ488Apx+enDtUY9AURQ/iL7Q0BVXAI0aATk5wT+3XQjMKJ/+hob27gV27w6uPU7J4sLC4F5DUZR6T3QJwW+/AT/9JOvbtokgAECTJsE5vxGCpCT30FB5ec2hoUOH5NjKSqsPQm2pqNBksaIoNRJdoaGVK631116TUMzYscE7f1mZFLYxMVZNvKxMav81eQSmgDZiEgw8PYJAhWDVKuDXX4Nnj6IodZLoEgITDurYEbjvPskV7NoVvPPbQ0CmADahGH+FoLTU+/mZgYIC7/vXrJHC21CbZDEzMHgwcO+98v7WW4FHH/Xvs4qi1CtCJgRENIeI9hLRWi/7zyaiQiJa5XrdGypbfic3F2jfHrj6ais0U1wcvPPbhcAUwEVFsvQnNGTO4Y3Fi4GsLGD/fuf9N90EXHut9d4zWRxIh7L16yVvYYTn3XeBTz7x77OKotQrQpkjeAHATAAv+TjmS2a+IIQ2uJOTA2RnA+PHS+uh8vLgJo2dhMCbR1BVJa8Ylxb74xFs3So279rlnNfYs0dyH+a8hw9beRAgsNDQ55/L0kyzWVzs2zZFUeotIfMImPkLAF6qrhFg/34pSLOzgVatgHnzgJNOCp1HEBsrL29CAEhi2OCPEBhbvbX8KSgQG0xcvzbJYiME5prFxcHNXyiKUmeIdI6gHxGtJqL3iOjUkF4pN1eWPXta21JTpeC1F8iGykpg0iTgxx/9v4ZTxzFvoSHAPU/gT2jIlxAwWyGjDRtk6ZQsLi+3ej17g9ndI2CWpXoEinJMEkkhWAmgLTN3BTADwCJvBxLRRCLKIaKc/Pz8o7taSgowciTQo4e1zQwEZ8IfdvLygGefBZYu9f8ankKQkGAV2mbyeMDyCOx5An88AmOnOae9QC8qsoTFLgR2j8DYUNOAdz/+KGGmpCQRn/JyEUb1CBTlmCRiQsDMRcx80LX+LoB4Imrq5djZzJzNzNmZZgL4QOnXD5g/H2jc2NpmhMApPGQKXdMxzBt/+xvw/vuy7ksInEJDTh6Bv6Gh5cuBhg2tTmj2BLIRAqeexfZreeObb2R51lnyHMx1VQgU5ZgkYkJARM2JZMYUIurtssVH28gQ4EsIjAA4eQt2ZswAFiyQ9UBDQ598AsyZI+uBhoZ++EFq6r/8ItvszUp9hYbs1/KGaVJ76qly/+YeNDSkKMckIWs1RESvATgbQFMiygNwH4B4AGDmWQAuA3AdER0BUAZgFHNNwesgU1uPgFn2m2PLytxDQAkJMjsZ4OwRPP44sH07MGGC1b7f39BQVZWsm0LaCEHnziIEzM7JYqDmvgT5+WJvs2ZyHXMPwfAItmwB5s4F7rjDmjlNUZSIEjIhYOYra9g/E9K8NHL4IwS+PIKKComdG7EoKZFchCEhwSrYnYRg2zZrgvlAQ0Pmc55CcMYZwKxZUng7jTVkv5Y39u0DMjOtezEewpEj1cUlUObNA+66S/o7HG2YT1GUoBLpVkORxZ/QkC+PwBTa3oTAXmA6hYZ277Zq54GGhkxOwGyzCwEgvaadehbbr+UNM8eyeT47d1r7ausVmM//9pv79l27gIkTdZhsRYkAKgTA0XsERgjsTSw9PQKDk0cASMHIHHirIVPwe3oEJ51kvT/aZLE3j8DYWxu8CcHHHwPPPSe5D0VRwooKAXD0OQK7R3DokISJAhUCZikcTV8Gf0NDTkKQni4vs72y0lkI/MkRePMIapsw9iYE5nnv21e78yuKEjDRNQy1J7UNDdmPMQWZt9CQPYnsGWO3dxDzVuNmdhcC0xfACMH+/TLshLm+KWidbIikR2BEyFMIzL2pEChK2IluIWjQQMbkCUZoyEkITG08Kcm9hUycx2M/cKD6OT0xnboAEQJTINs9gowM6/omhxBoaKi8XO7F7hHYhSBUHoFnrkNRlLAR3UJAJIVdMEJDvoTAHhYCjk4IzPnN+EXmvV0ImjaVaxFZQhBostjUyMOdI1CPQFEiRnTnCAAp7GrrEZSWWgWyU2jIUwg8Q0N2IfBW0BobW7SQQtRzGG3jEcTEAMnJvj0CXzkCUxDbPQL7sB6hEoJw5AiuvVY6ACqK4oYKgTePIJDmo4CM3Q8cnUdgzxF48wiMja1auW/3DA0ZG5yEwJ8cgSn07R6BndqGhowI7d8vA9t17Cj3Fo7Q0OLF0jpJURQ3VAhqCg3ZW/R4YheJPXtkWdvQkLcat7HHLgQxMSIEFRWyNHMUpKQ4J4sDCQ01bSr2m3uwT71ZG+wewddfS0/j7dtDHxoyo7N6eiKKoqgQ1CgEgPdasH27GfwtkNDQccfJ0p8cgbExK8va1rq1CIAp3GryCPwRArtHYM5lfx9MIdixQ9YPHAi9EJjmtCoEilINFYKaQkOe63bshfbReAQdO8rSCEFaWmBC0K6dFHAmnGIXAl8egVOOYMMG4M47JcRFZI3SavIERrSC2WooL89aD3VoyAijXXQVRQGgQuCfR+AtYVxbITj+eFmawqlx48BCQ+3b1ywE/noEb7wBPPII8NZbEmKKjXW/n2bNZBnMfgR2j8CeLA7F2IPmGalHoCjVUCHwJQQm5u7NI7BvN6Ghhg2tbd5CQ6ZwNcNBGCFITw/cIzh82BqK2tTaU1Ks0Un9FQIjZGvXug8GZzwCsy2YHoERArtHUFER3OlDDcYjOHiw5ol5FCXK8EsIiCiZiGJc6ycQ0UVEVIshKOsQvkJDphbsr0eQnGxNRg949wjat5eZz8aOlfem1VDjxv4LQWqqJVTr1smybVtZeuvdTCQ2OQmBETJAEsUGIwRpafLZYOUIDh60xMfkCIxHE4rwkP2cGh5SFDf89Qi+AJBERK0AfABgLIAXQmVUWElNlVq1GdYZsAaQM0Lgb47As7mlNyEAgPPPtwpZz9CQU2jk4EEp1E3NPCMDaNRI1n/4Qa5txhnyNt4RYM1b7Mnu3VY4yO4RmHOlpoq3E4zQUHKyrJv7LCiQZ9m+vbwPRcLYPoObhocUxQ1/hYCYuRTASABPM/PlAEI72Xy4cBpvqLxcQiv+eASmMPYceRTwHhoymO320FBVlbsoGYqLxdaUFPE67EKwdq14A2YYC1PQ2m0wJCZ6Dw0NGyZ9DZo3t7ab55OaKvaWlgLr11teSCBUVsq9efaFMEnjdu1k6a8QlJT4n09Qj0BRvOK3EBBRPwBXAVji2hYbGpPCjJMQGA/AFIjm/RdfyJj5pqAuLXWvPdsLYMB9rCEnEhKk8LZ7BOa8nhQXiwgQiQA0aWIJwbZtQJs21rG+PIKkJO+hoU6dpMPVXXdVP1dKighBWRlw/fXyChTjibRsaW2LjZV+BIAlBP6Ehg4ckO9n0SL/rq0egaJ4xV8huBnAnQAWMvM6IuoA4NOQWRVOnITAeACeHsFLL8mY+VOnyvuSEucwiqEmj4BI9nkKgVP45eBB98Rt8+aWEAD+C4GTR3DwoIhP8+ZA//7uCWm7R9CwoRyXl+c+7IS/mPtq0cLadsIJlhAEEhratk3sXrPGv2vbxUWFQFHc8EsImPlzZr6ImR91JY33MfPkENsWHvwRAuMRrF0rYZnp04ElS6zQkGmNE0iOwNCggVVTNjF+bx6BsfWNN4CHHrLeA1ai2NMOp9BQWRlw663AqlWyzSSKzf3asecIjEewe/fRhVc8PYKkJKBDB2tQu6wseb7+CIGZI8E+V4Iv9u+3rqtCoChu+Ntq6P+IqBERJQNYC2A9EU0NrWnB5ZtvgDFj3BvHALBq1UVFUlCNH2/Fv01zzJISid2vWwf86U9SKC5dKgV2w4ZWSOhohMAeNvInNAQA3bpJr+Kj9Qg2bQIef1zmNgash2LPDRiM2KSkyL3m58vz8LcwNc8OsDwCUyC3aiX3bOL8aWmS+/AnNGQEwD4yqi8KCqr321AUBYD/oaFTmLkIwMUA3gPQHtJyqN6wZw/w6qsOFUhT2O/dC6xeDbz4ooSAAClok5LEQ/j1V1n26CEF2e7dNQtBTaEhz31pabJ0Cg3ZPQKDXQj89QiSkqQXMQB89ZUsTTNOJyHw9Ai2bJH35eU1z3RWXi4C9eKL8t4zNJSVZXlB5hoZGf6FnQL1CAoK5HtLSqouYnfeCfTqBTz/fGg6sylKHcdfIYh39Ru4GMBiZq4AUK/+MaayXa0iawqlXbusQuWbb2SZkiKvkhIJCwFA585SYBohSE6unUdg9sXHW583HsH+/VbnJ3uOwJCcbLUUCsQjMIXdunVyDV+hIbPtuOPEVvsDtI+aCgD//S8wbpx1/rw8Of9PP8l7IxxpaSKgxiOw292smTWSqy+MJ+CvR2BmcEtPd54vOTcXmDAB+OQT/86nKMcQ/grBswC2AUgG8AURtQVQFCqjQoHpe2VvPAJACtNGjUQEjBCYg0whf/CgJQSnnmoJQUmJFGj2ljV2AhGCxERrvbRUCtNTTpF8BOAeGjKYFkSxse4tcWoSAvNZAFi+XO4lJsa9I5lh2DBgxQoJq9h7TQPVC9Q33xRvavlyeW8fQgKwPIKkJGDKFOCqq6p7BC1a+FfLN8fs2eN9dFiDGWwuI0OEx9PuAweAAQNk3YiWokQR/iaLpzNzK2YexsIvAM4JsW1BxasQAFKI7txpFVwGu0fwww8Sl09Lc/cIghUaSky0CtqyMin49+wBtm4VUSgqskJHdlJTpWZtH9q6pmQxAJx3nuz76iu5znHHWR3K7MTESNjE6T48Y+0//yzLadNk6U0IGjQAHnxQRMbuEaSmWt9FTSEaIwSVlTWHkgoL5XxNmsj1PO0+cAA48US5f9OnwReHD/vviShKPcDfZHEaET1BRDmu1+MQ76DeYMobr0JgDw0ZUlLcPYLOnWV78+ZSUB85UvtksZMQlJZaLWeKiqQAPXLEPSdgaNTIPSzkaYdTPwIA6N0b6NlThGD3buewkCeeHoG9QD1yRMY8Sk4GFiwQATOFqpMQGIxHQCSfbdFCjiuqweHcudP6Un0Vyv/4B/DKK7Lu5BEwi30ZGfI78EcIHntMmr3q/MrKMYK/oaE5AIoB/MH1KgLwfKiMCgUNGkgZ6CgEJhyxc6d7IZqcLO8LCoCNG92FwH6MNyEwbohTyMVuGFA9NGSEoLDQKhSdPIKbbwYme7TkNXYQVa/lG4/gtNOAM88Evv1W4uNOiWJvthrsBeqvv4oY/PWv0kpo4cLqHoHJETgJgeksZ0JcnqL8ySciMIB4AXv2SOLe6Vg7jzwiNgGWR2C3u6xM8jCNG0vy2h8h+PhjqRyYJLii1HP8FYKOzHwfM29xvf4OoEMoDQsFTZp4afVoDw0NGCCFZ2KihFuSk6WgPHwYGDpUjrcXmr5yBP37A999J809veGPR2CEwMkj+NOfgMsvd99mhMnTGzDXAUQIbrlFasI7d/onBMY+M0SE3SPYvFmWAwfKOTdu9J0jMJhavXl2Rgg8a/k33GB15MvPFzHIznY+1lBaKq8jR+R9Rkb1ZLFZT0/3TwgqKkQ8AWl+q62MlGMAf4WgjIjOMG+I6HQAtRx9LPw0aeIjNHTokCQKO3SQeLEpTM2ye3fg7LNl3VMIvHkERFZh5Q1TKCYmWuMI/fabFXYoKrJa5zgJgRPx8XI+JyFISZF9nTpJOOi11+SanuElJ4xomeGz7QWqyQ907AicfLI0UfWVIzAYj8C0iDKtuOy1/PXrRVjy8sTbMPtq8gjMMzTPwXgEhYVW3wb7OE+tWsk1fBXuq1fLfVxwgfxePj02Otgr0U1czYcAACYBeImITGziNwDjQmNS6PApBIDU9lq2lOSoqfWZwv2vf7Va2vgrBP5g9whiY+XcO3daYaWaQkPeSElxLtBuvhkYPtxKIp99tng8ZngHf2xt107stXsEP/8s21q2FCFYsKD6oHpOQuA5E5qTRzB/viwPHxZvwBT8bdtKLd+bR2C8qn/8Q0SpY0erA1tRkRT+diHIypKGAd4S84DMswxIQvyjj6SH+cCBzscqSj3BLyFg5tUAuhJRI9f7IiK6GYCfA73UDZo0sSqubtjHvmnZEvjzn61a+GmnycsefsnMlFp0VZWVRwBqLwTGll27LLGpKTTkjZQU58Hl2rZ173wG+A5d2TGhoRYtqre++flnKWhjYkQITG08Lk4E4NAh5xxBaqoIrH1Mo+Rk91r+/PkiXBUVkoswBX/Lllai3wkjBP37A2e4HFp7hxInIQDEK/AmBMuWifd0/PHyHH/91fk4RalHBDRDGTMXuXoYA8AtIbAnpDg1IQfg3ga/ZUupZXZwpUAmT5ZwgD3MEhtrDTYXTI8AsBLXtQkNGVs8m47WFmNr8+bVY+2bN1tzMJ98srX9xBNlWVgoghAb697UNSZGzmXvLGcK961bgRtvlOd/xRWyb/t2SySaN/fd78AIgT1Zb0R/2zZZehMCO+Xl4kUwi0fQv79sb9NGhUA5JqjNVJUUNCvChNfQkKdH4Ak53KqpsTdsKLH22Fhrhq1A8BQCUwiaQuzQIaudfKChIaccQW0wHkHz5u4eAbMMPWGEwOQQAOmAB8ixZWXOTWlbtLCG+jDvd+4UL+y554BRo4D775d927dLmCczU4TOJPqdcBKCnj1l+d13ll2A1WoIcBeC/fvFtkWLRMx27LByE/4KwcGDMrT3vfdane0UpQ7hb47AiXrXXKJJE2lEUl7uMUVAw4ZWmMBz0hRvNG8uNdWGDaXA6tzZvTDzFyePYO9e93CHKZg8h5jwRUpKzW3xA8WIZKdO8ryMQJnOdWZQtzZtrCGr7UJQXu4sBG+95e7ttGwJvPuu2P+vf8lIqczypW3fLp37jNeRmem9PX9+voi4vdNa06aS4zBCYLyatDRLaO1CkJsrfUbWrLGuaX4jbdrIvR86ZH1/Trz3HvDww7I+f/7RTeqjKCHEp0dARMVEVOTwKgbgUHV2++wcItpLRGu97Ccimk5Em4loDRH1qMV9+IXJv3oNDyUluQ954Au7RxAfD3TpcnRGOXkEgLSUMWzfLrYFUsNv1cp9roRg0KuXhFS6dHGPs5nWQaZGHRNjhYQ8PQKnSXqOP95dRFu2tETs4otlSSQ9u7duFQE2Nfv0dO8D4O3bJ1+6Z1+KXr3cPYKGDeXZJiSId2cXAvtQ3Z5jMpmWVjt2SGVg9uzqNgDS4gkA7r5bvlftiKbUMXwKATOnMnMjh1cqM9fkTbwAYIiP/UMBdHK9JgJ4JhDDjwafw0y0aCEFkFMYyAkjBJ6zkgWKk0cASAFlCvLt2wMLCwEyRpFpbRNMTKLZnmg1g8TZeyeffLLkAk44Qd77Cg15Yp5B586WlwGIEHz6qZzHhGeMcHsOgAeIEDh15uvVS3pB5+eLXXbxz8oCvvwSuOYaCfvYhcBzlFYjBKtWAfPmAf/5j/P9bNwoz+388+X9smVeb11RIkFtQkM+YeYviKidj0NGAHiJmRnAN0SUTkQtmDlkg7h4HYEUkEHQApk0PTtbCo1AC2hP7P0IAPccRYcOUljl5QWWKAZqb1dNGCFgtkJE9lr9jTdKgWvUNxAhMM/gkkvct7dubY0O6ikEBw5UHybDlxAAQE5OdSE44QTpW7Fpk5zPySPwFIKFC2W5Zo0IjGerrA0bRBh79RLv8euvgQsv9Hr7ihJuapMsri2tAGy3vc9zbQsZPj2C4cNlCGV/uewyqanXNiHrzSMArOTrnj2BC0GoadxYevcePGh5BPZQVN++0mfBXlB7yxF40qOHFMKjR7tvb91alg0bWqEn+/k98SYEPXuK5/fdd9WF4OmnpQZ/3nnA669bYR3jEcTFWTUKEwp75x3r8/Z1QJoYb9okCfQGDaQCYeaBUJQ6QiSFwG+IaKIZ8C7/aObKdeFTCCKFpxAcd5wVnjJNWJlDX8MPFHsBvHeveDZOzWcbNpTC01eOwJOTTpKC1976CLCEoFs3K+5fkxA45UlSU+XcRgjsyeT0dBGZSy+VfERVlXwPxiNo1kxyIIDcS7Nmco6OHSWM5SkE27dL0tzcyxlnyHVrmtRHUcJIJIVgB4DWtvdZrm3VYObZzJzNzNmZtUiA1gshiIuzQhxGCIC66REAEmfbu9ddwOwQWWEkf0ND3jBC0MPWrsBJCL76SsJV3jwCwEoYm45lnlx8sXU/Q4dKwW3CRU429ewp4Z5PPpEWQqbCYjwK0+LojDOkh3Sw8ze33y4D7CnKURBJIVgM4I+u1kN9ARSGMj8ASEUwJqaOCwFghYfsQlCXPYL8fN8tlIwQ+Bsa8oZJHPfp42wHIENEDBwoHQErKnwLwZ49EtN3EoLmzYHTT5cfTd++sm316uqD85k8QXa25EV69JA+A0OHSujMCIHxCM4/X4YAnzABeP992TZvHvDAA9Vt+PBDGVTQ29hHmzeLCFdVAc88A/z73zVP0qMoDoRMCIjoNQDLAZxIRHlEdA0RTSKiSa5D3gWwBcBmAM8BuD5UthhiYnz0Lo4UTkJgn9zd5CDqmkdgCv7duy2PwBvB8gg6dZJa/JVXup8bsIQgN1cEwCRwfQkBIAWntybD06fLjGvm+ygp8S4EPXvKeE3Ll8vk2Lm5Ujhv2CCuqHleSUkiACefDIwdKz/Gm24SISguts576BAwcaJM/7lpk7N9gwbJcCibNslnCwq0RZJyVISy1dCVNexnADeE6vre8Nq7OFL48ggyMsQTyM+ve0JgEtmmVmrmanDC9EL2N0fgC8/RXBs0kJY4RgjMfNNmnCVvQtC1q4ThjhzxLgTdu8vL3qfDMzTUpYt4DaZfAyBC9cIL0hEuNlauZQ+bNW4sItG/v4xianpGf/GFNFoAJGlthsH45JPq+ZJdu2R/QYF78nnxYmvaTUXxk3qRLA4mzZr5NyVu2MjMlFqm/Y9+2mliaFqaJQB1LTRkZhP76afweQRO2HMQgIwaaw9TeROCpCSrE2BNnQjtXoCnRzBunPQ3sH8/RFKTHz1aks733FP9nP36SZho2TLxdBITraaxBQUyleegQdIU1Wy3k5Mjy+JiYOZMEaPzzpNe2jpHghIgUScExx9fx+YnT06W2p3pbATIJCybN0ssyxQwdc0jAORhfv+91L5ryhHs2iVNTYMtBOb8diE47zzLc/A1O5wJD9lbDTnRuLEVovMUAjNonidt2gBz5khoyUxo5Mn994toTJ0q3oEp8O+8UzrIPf645Ds+/dSaP8Hw3Xdy7ZgY6b/Qq5f0u/jpJ++hJEXxQtQJQadOVplUZ4mNtZpiGgGoi0LQqZOM+wP45xE0aFC9k1gwMOffsUNeffoAI0ZIyMiXXUYIavIIiCwB8GduZ3/p108G67v2WuDcc6Xz2ksviTdx883iGQ4cKLHMNR4jvufkAKecYgle795WWOm99yTk9fzz1hwQiuKDqBQCwJpZsc5TV0NDgHgEpqbqq8C9/HJJaq5dK4VfsElPlxq0mUyoTx+pZefk+B6ob8QI4OqrLUHwhRECf6b0DIR27URozOQ248ZJ0vm+++T9OefI0t4/gVk8guxsCR8BIgRt20ofiKVLgTfflJZJ06cHbpO2PIo6olYI6lR4yBd1OTRkHiZQc8171iyrzX2wSUsTjyA3VxLA3bpJzL2mgQCbNpXwjT/PNlRCYOjdW0JCzz8vXpYRsFatJLT08MNW7eXXX6WPRK9e0vLo/PMtwRg8GPj8c3neADBjhrSi8pdXXpEw3/ffB+/ewsny5TJEyBdfRNqSekXUCYFpil5vhKAuh4bsA8IFe6TTQDChoU2bpO9FbVsmOdGihZw3VN9DbCzwz38C48db8z4YZs+WHMXYsRLTXLxYtvfqZXkAJrw1ZIj01/jiC8k77Ngh3oG/vPOONGm94ALpFV1b/vtfmQ61tLT25/LFkSPALbfIPY8eLcK4dWtor3kMEXVCkJJiNXapFxiPoC6GhkwTUqDuCIEZgyjYTJkCvPyy/6PTBpOsLBGDFSukOevNNwNnnSXrnpx1ltUU+cUXpTXatGnVj9u922pqa2CWkVf79hXBOfdc8QwmTRJPwc7Bg9KooXt36S09e7b0mPZkzhzxUKZMEVG47DLgttskNxIs1q6VJrPTpkmnvmXL5Ht6+ungXSMcPP20VAQ8GwaEA2auV6+ePXtybRkwgPn002t9mvDwr3/JJIm//RZpS5xp3pw5NTWyNjz4oDyjhATmW2+NrC2h5L33mNPSmM8+m/ngQe/HjRzJPGSIrM+YIc8mN9f9mGuvZY6PZ96zx9q2ZYsc+/TTzF9/Ld+rmaQzPZ25sFCO27WL+YQTmImYzz+fuWtXOWbAAOaqKut8v/3GHBMjvxFznqws+Z7atmXevbv2z2TrVuYGDZgzMpj/7/+s7X/4g9h88KC7Tb4oLGQ+cODo7Dh0iHn9+qP7LDPz6tXMcXHyjGbPlm1btzJfcw3zN99Y16gFAHLYS7ka8YI90FcwhGDCBOZmzWp9mvCQn888b16krfDOGWcwd+wYWRtmzrQKGvMnOlYpLmY+csT3MUeOMFdUyPpvvzEnJTFPmuR+zCmnyPN6+GHmbduY585lfvFF2bZmjRzz7bfM48czv/qqbP/nP2X7lCnMsbHMH38s76uqmJ98Uo5ZuNC6xoIFsu2jj5ivv17OX1XFnJPD3LAhc+/elrj4y6ZNzK+9Jktm5uuuE0HbutX9uK++kmunpDC3aMFcVOR8vqeflsrDjBnMTZowd+jAXFAQmE3MzHffLaK3YYO8X7VK7Pz555o/u3kzc3Y2c2Ymc79+ImATJzInJ8s9ZGczl5WJ4D7+eOC2uVAh8ODhh+XOA/0NKg7Mm8f8zDORteGVVywh+PzzyNpSFxk3Tmr3pjD87TfrebVpI4UfILX09HTmysrq5xg6VGrd69ZJATVmjPv+igrxEjp3tj4/aZIUxIcPVz/fokVSA+7ZUyo7/nL66Zbto0aJdzFxYvXjqqqYb7tNvCNARGn9evnM0qWyf80aKbzN+Xr0EFEZMsS32JaXM99xB/MPP8j7khIREUA8rYcess7pGXrwfLZ33inHxcTIf2nDBvFw0tKYL7+c+e9/l/1nninLd9/1/1l5oELgwbx5cufffVfrUyl1gXfesf54u3ZF2pq6x7Jl8mwyM5kfe4z5gw/k/Z/+JMvEROa+fWV9+HDnc+TmSi3ehC9Wr65+zGuvyb7//Efed+jAfOGF3u165x3xVkaMqL6vsrJ6Ybx7t4SjJk2SApRIPBNfte7Dh0XcJkwQ8TK/k3POkUK6cWPxJpYtk+s98wxX8yw//1zEdOxY5tdfFzEBmAcNkv2zZ/PvNfeEBHlGI0eKEAHMa9fKce+9J/tbtmS+/XbxxOLimC+7jPnXX63rFRdbgnHokAg0wPznP3u/Tz9QIfBg61a585kza30qpS5gwgCNGvkfD4423n+feeBAeU5XXimFaH6+rC9YwLx/v8T4X33V+zlWr5bC/bLLnPdXVkptOj6e+dJL5VrPPuvbLpPf+eILKdzvukti/VlZzBdf7H7sf/8rx37/vbz/9FPmN9+s+d5HjWJu2lRE55pr5I+flibnmj7d/diqKuY+feT6ZWVSWKeliZg0a2YJSffusszJYT75ZAnb/PijPNeWLSW8tHevFPyTJ4uIHXcc84knitgCzCedJPu3b/dt/6JF8pni4prv1QcqBB5UVTG3aiW/D+UYYO1aq0ameCc/X8IOgOQIjganmrqdggLm9u3lGnfdVXM+o7hYCsjERP49RGKS056hvgsvlNpxoGJvDx2uXCnbdu5kfv55K5di56OP5NhJk0QUUlPF66islBzIc89JIZ+YKGEyIubFi+Wzb7xh5ViYpZBJSZFnkpQk4aQjR5gHD5ZrXHddYPdSC1QIHLjiChF95RggL09+ylddFWlL6j6TJ8uzmjAhdNf49VdpdeQvs2dLiOS55+SzCxZIHqNFC2mMUFUlNeqkJOYbbwzcnn37RGD8LTuqqpjPPVeeU3y8hLycuOYaOea557yfa8UKEYHBgy2xMDbdfntwWk75iQqBA6ZV3S+/BOV0SiQpKZE/7MMPR9qSus8vv0gNd+7cSFviTklJ9W1PP82/NzlNTpZ8wLffHt35Z85k/vJL/48vLmbeuNHZLkNJiYSG6gm+hIBkf/0hOzubc8wQvLVg1SrpC/Pqq9XnSFfqITk5MtlLcnKkLan7VFTIgHx1naoq+YO+9ZYMuXH77dXnZVD8hohymTnbcV+0CkFlpYwuPGZM/euAqCiKEii+hCDqhpgwxMbKQJj2yZ0URVGikagVAkCGSFm7to7NYawoihJmol4ImGXkWkVRlGglqoWgd28Zvv7rryNtiaIoSuSIaiFITgZ69NA8gaIo0U1UCwEg4aEVK2T+dUVRlGhEheAMmdDJc44ORVGUaCHqheC882SCqyefjLQliqIokSHqhSA1FZg8GVi0SJqSKoqiRBtRLwSACEFyMvDII5G2RFEUJfyoEADIyJChJt56S4ZhURRFiSZUCFwMHgwcPKhJY0VRog8VAhfnnAPExAAffhhpSxRFUcKLCoGL9HSgVy/go48ibYmiKEp4USGwMWiQdC4rLIy0JYqiKOFDhcDGoEEyT8Enn0TaEkVRlPARUiEgoiFEtImINhPRHQ77xxNRPhGtcr2uDaU9NdGvH5CWBrzzTiStUBRFCS9xoToxEcUC+A+AQQDyAHxHRIuZeb3HoXOZ+S+hsiMQ4uOBYcOAt98WzyA2NtIWKYqihJ5QegS9AWxm5i3MfBjA6wBGhPB6QeGii4D8fMkVKIqiRAOhFIJWALbb3ue5tnlyKRGtIaJ5RNQ6hPb4xZAhMkfB4sWRtkRRFCU8RDpZ/DaAdszcBcCHAF50OoiIJhJRDhHl5Ofnh9Sg9HTgzDOBN94ADh8O6aUURVHqBKEUgh0A7DX8LNe232HmAmY2MwH8F0BPpxMx82xmzmbm7MzMzJAYa2fKFGDLFmDatJBfSlEUJeKEUgi+A9CJiNoTUQKAUQDcAi5E1ML29iIAG0Joj99ccAFwySXA3/8O/PxzpK1RFEUJLSETAmY+AuAvAJZCCvg3mHkdET1ARBe5DptMROuIaDWAyQDGh8qeQHnqKSAxETj3XGDr1khboyiKEjqImSNtQ0BkZ2dzTk5OWK61cqV0MmvaFNi4ESAKy2UVRVGCDhHlMnO2075IJ4vrND16AI89Bvz4I/DDD5G2RlEUJTSoENTA0KGyfP/9yNqhKIoSKlQIaqBlS6BLF+C99yJtiaIoSmhQIfCDoUOBr74CiosjbYmiKErwUSHwgyFDgCNHgKVLI22JoihK8FEh8IP+/YH27YHrr5fEsaIoyrGECoEfJCRYyeKhQ3WCe0VRji1UCPzkhBOAZ5+VoSc++CDS1iiKogQPFYIAGD4cyMgAXn450pYoiqIEDxWCAEhIAEaNAt56C/j8c+DVV4F61jFbURSlGioEATJ2LFBeDpx9NjBmDDBvXqQtUhRFqR0qBAHSuzcweTLw4IMyBMXkyUBhYaStUhRFOXpCNmfxsQqRjEwKSP+C3r2BiROB114DYlRWFUWph2jRVQt69gQeeURmM7vxRs0XKIpSP1GPoJZMnQrs3Qv8+9/A4MHARRfV/BlFUZS6hHoEQeCf/wROOQW45Rbg0KGaj1cURalLqBAEgfh44MknZVrLJ5+MtDWKoiiBoUIQJAYNAkaMkNZEu3ZF2hpFURT/USEIIo8/Dhw+DNx0E/D668CqVZG2SFEUpWZUCIJIx46SJ3jzTeDKK4HsbODOO6XT2S+/yDElJcCBAxE1U1EUxQ1tNRRk7r9fBKBdO2D6dGleCsgsZ6tXA1ddJbmENWukT4KiKEqkUY8gyCQmApdeKn0MXnwR2LZNxGHNGuCzz4B33gHWrgW++y7ChiqKorhQIQgxbdsCf/6z9DoePx6orARiY4FXXhFx+PTTSFuoKEq0o0IQBpo3B845R/IEp50GjBwpQ1n37w9ccAFQVlbzObZtA0pLQ26qoihRiApBmLjiClmOHi0jmB44IGGk0lLg449l1rM9e6zjy8pkdNMFC4BffwVOPVWap+owFoqiBBsVgjAxejRw220yQN2wYcALL0jyODVV5je46ippdbRhgxT2118v8x2MHQuMGyeC8dFHwMKFkb4TRVGONYjrWRUzOzubc3JyIm1G0LjiCkkgl5ZKK6JTTpFE80svATfcIP0RCgqAe+4BFi0CioqA9euBhg0jbbmiKPUJIspl5mynfeoRRJgRI0QEWreW2v769dIP4eabpfnpG2+It3DnncCMGZJneOyxSFutKMqxhApBhBk+HOjcGZg5U0Rh1SogLw+YNk1aGg0cKC2MGjQAzjpLpsp89FFJHgPA/v3AAw8AJ50EZGUBkyZVzyPMmQNceCHw/PPamU1RlOpoaKiekZcHnHgicMIJ0j9h0iRg924Z6ygpCXj7beC++4C4OBGPCROA9u0l+Xz4sMy7fNFF0gO6Xz/na+TmAn//u4Sn0tPDeXeKooQKX6EhMHO9evXs2ZOjnXffZU5NZQaY27Vjzs2V7VVVzJddJtvNq3dvWa5ezfztt8w338zcuLFsO/105ltvZc7KYm7Zkvnii5lLSpiHDJH911/P/Oqrsq9PH+aZM+UazMy//cb8t78x794dscdQI0eOMHftynzPPZG2pDqbNzOvXBlpK5RoAkAOeylXI16wB/pSIRDWr2eeOpV571737UVFzP/4B3NODvPgwfIN/+EP7scUFzNPny4iEhPDPHw489ixcuyIEbJs1YqZiDkujrlbN+ZevWT7mDHMpaXMEydaxxtxcGLzZubDh4N99/6xaJHYmJzMXFAQ2mvl5TGXlfl3bFUVc/fuzI0aVf/+FCVUqBBEKfv3M0+ZwvzLL877KyqYDxyw3v/5z/KLaNiQeetW8RR69GAuLJTC68EHRRxOOUWOO+EEWc6axbx9O/PbbzOPGycexzPPMH/xhQjNiBHMlZVyvTvvlPP27cs8aBDz6NHMe/ZYNpSUyPWYpcb8xhviydjFZsYM5kmT5JyHDom3s3cvc36+3OuPP4r4DBrE3KSJ2DhlCvOf/sT8l78wb9ni/ZkdPixCVxOVlcxz5zJv2yZeUaNGzGedJdurqkSIX33VWRyWLbM8tj//2X3fypXyPH2Jazgw16+qkmc9bZq174cfmIcNE5E/WrZuZX7qKflNBJtdu5gXLw7+eQOhqko89++/t7ZVVDAvXRqae/aHiAkBgCEANgHYDOAOh/2JAOa69n8LoF1N51QhCB0HDjCfdJIVSjlwoHptfvFiCUu1aSP7s7PZLRSVmipCERvL3Lw5c0qKbB89WkQFYB46lPmcc0QMGjRgPvlkKVyWLpWCOzFRQlH2815yidTq582ztj32GHO/fu7HmVfbtrJ88EG5HsCckCCv2FjxgFatknvat09CXQUFUlNv1oz5q69k32+/SYhs0SKrcNy+XUQGYD7xRMubAsRTOvVU633XrswrVrgX7GPGyHO65hoRymuuYf7f/5g3bGDOyJDPTZggBcbu3cwDBjDfcIMlkE6Ul4sQHjjAvGOHJUAzZsj3WVkp390DD4hHuGOHXM9QVcU8fz7zpk1yfyeeKPf99ttiDxHz55+LUJuKwDnniPBOnsz89dfu59q8mfnZZ+X+W7dmXrLE2l9Wxtyli5zjqqusSsKECfJ9OYlgaSnznDnMDz3E/OKL3r2vI0es38R//mNtu+MO5gULrON+/lm84pIS98/v28f86afVtx86xPzPf0qlg1l+OxdfLP8Xz4rFnj3Ml14qNsTFMT/6qNzTAw/ItnvuEbEaP1687auvFhtDTUSEAEAsgJ8BdACQAGA1gFM8jrkewCzX+igAc2s6rwpBaKmsrPmYHTvkxSyhqMWLpcD59FP5wxYWyh8kNpZ5+XLmUaPkl3bqqcz/93/u5/rsMwndmILz1FOZr7tOCpsHH5Q/3KOPyh/KHNO3L/P558t6TIwIwvTp8vrvf5lnzxaBSkuTgnT1asmdrFkjIZwpU8TrAcQ7IRKByMqSZbt2zPHxUsD17Gldt08fEbSkJBGwW2+VewSkoDZhtQ4dxI4332Ru2tQSjIkTJUeTkCCeyf79zBdcwJyZaRW2jRsz33ijvO/WjblzZxHGmBhLxPr2Zf73v8X7uuIKCf0ZwTWv445jvvJK6/2558o5ABEbs3711SJUU6bI+wYNrJAhwJyeztypE/Pxx4uwGzEfN45/9x7NNQYPlu+qa1drW+fOljBmZMj6mWfy7yFLQArUG26wPnP//czr1jE/95zcw+23W96n/f7GjJHCfvlyedbPPcd87738u7caGytehzl3XJx4aZMny/cLMPfvLyJ/yy3yvIksWydMkPN9/70l9klJVsg1LU2eT8eOcv3585mffFK+84QE+f2anN0VV8hnU1LErrZt5VmbZ3HXXfJ/2LJFPnfWWfI9ZGeLHQ89JN/V668f/X87UkLQD8BS2/s7AdzpccxSAP1c63EA9sHVksnbS4WgfrBnjxQwzFLby8vzfuyWLVLbmzZNaqtOfPed/Bn+8Q859/bt8sf93/+cj6+q8h2z37dPkt8jR0pNbfJkKayXLJEC+uqrpeBITJSC4sknRQgyM6UWu3WrnGfmTBG9vXvFe3jlFffr7t8voZ7Bg6XQaNhQkvTbtrnb+v77IiSffy7b5s6VwiIhgfmDDyQ89te/Sl7o+OPln5uZKYV0q1bM114rQvj441I49u9vFfRTp8r6eecxf/SRXOeuu2S7ETJAQkDnny8F1scfWwXe/PkS6urbVwqmp56SCsN554lg5+TI99KmjSV6M2aI8Jrv4bHHxMM4/3x5pn/9q9znE0/IPZow2ejR7gV+8+byPbRrJx5jWZnYdvnlss/JGxwyRCooZ59tbbvuOkuQ4uLEC5s927p2YiLzwIFSCM+fLx7occdZggjI8zrvPPHm7r1Xvu9vvqkuwj17Mq9da323d98t21NSJLfXpo2IiPGirr1W9ptGHEQiuKNHy/My4c3mzZn/9S/vv+ma8CUEIWs+SkSXARjCzNe63o8F0IeZ/2I7Zq3rmDzX+59dx+zzdt5obz6qhI8dO4CDB6W5bjAwfzV/56E4dAjYtw9o1cp9e1WV2JaV5f1czDLc+amnyjHLlsk8GYmJ7sfl5QErVsiIuBddJNtKSoCUFKC4WD53/vnO16mslL4uZh+zdHhs3VrO540jR2S/+dwPPwBLlkiT5pgYGXtr/37g+OPF5ooKaQ4d49HryVzv++/lmo0aSdPnQYOApk1l/5dfyii/118P7NwpfXKuvFJGBQaAjRul+XXfvtL82pP9+4H//U9svuMO2VZRIc2w7cfk5cn3kp4OtGlT3dZXXhGbhgyR61VWWt9rebl0Et27V77T0aPlHIaqKul0mpLi/Zn6g6/mo/VCCIhoIoCJANCmTZuev5jpvhRFURS/iNQQEzsAtLa9z3JtczyGiOIApAEo8DwRM89m5mxmzs7MzAyRuYqiKNFJKIXgOwCdiKg9ESVAksGLPY5ZDGCca/0yAJ9wqFwURVEUxZGQzVnMzEeI6C+QhHAsgDnMvI6IHoAkLRYD+B+Al4loM4D9ELFQFEVRwkhIJ69n5ncBvOux7V7bejmAy0Npg6IoiuIbHX1UURQlylEhUBRFiXJUCBRFUaIcFQJFUZQop95NTENE+QCOpkdZU8gQFnUNtStw6qptaldg1FW7gLprW23sasvMjh2x6p0QHC1ElOOtV10kUbsCp67apnYFRl21C6i7toXKLg0NKYqiRDkqBIqiKFFONAnB7Egb4AW1K3Dqqm1qV2DUVbuAumtbSOyKmhyBoiiK4kw0eQSKoiiKAyoEiqIoUc4xLwRENISINhHRZiK6I8K2tCaiT4loPRGtI6KbXNvvJ6IdRLTK9RoWAdu2EdEPruvnuLY1IaIPiegn17JxmG060fZMVhFRERHdHKnnRURziGiva0Ils83xGZEw3fW7W0NEPcJs17+IaKPr2guJKN21vR0Rldme3aww2+X1uyOiO13PaxMRDQ6zXXNtNm0jolWu7eF8Xt7Kh9D/xrzNYXksvCDDX/8MoAOABACrAZwSQXtaAOjhWk8F8COAUwDcD+DWCD+rbQCaemx7DMAdrvU7ADwa4e9yN4C2kXpeAM4E0APA2pqeEYBhAN4DQAD6Avg2zHadDyDOtf6oza529uMi8LwcvzvX/2A1gEQA7V3/29hw2eWx/3EA90bgeXkrH0L+GzvWPYLeADYz8xZmPgzgdQAjImUMM+9i5pWu9WIAGwC08v2piDICwIuu9RcBXBw5U3AugJ+ZOWLzlDLzF5B5M+x4e0YjALzEwjcA0omoRbjsYuYPmPmI6+03kBkCw4qX5+WNEQBeZ+ZDzLwVwGbI/zesdhERAfgDgNdCcW1f+CgfQv4bO9aFoBWA7bb3eagjBS8RtQPQHcC3rk1/cbl3c8IdgnHBAD4golySOaIBoBkz73Kt7wbQLAJ2GUbB/c8Z6edl8PaM6tJvbwKk5mhoT0TfE9HnRDQgAvY4fXd15XkNALCHmX+ybQv78/IoH0L+GzvWhaBOQkQpAOYDuJmZiwA8A6AjgG4AdkFc03BzBjP3ADAUwA1EdKZ9J4svGpG2xiRTnV4E4E3XprrwvKoRyWfkDSK6G8ARAK+6Nu0C0IaZuwO4BcD/EVGjMJpUJ787G1fCvcIR9uflUD78Tqh+Y8e6EOwA0Nr2Psu1LWIQUTzkS36VmRcAADPvYeZKZq4C8BxC5BL7gpl3uJZ7ASx02bDHuJqu5d5w2+ViKICVzLzHZWPEn5cNb88o4r89IhoP4AIAV7kKELhCLwWu9VxILP6EcNnk47urC88rDsBIAHPNtnA/L6fyAWH4jR3rQvAdgE5E1N5VqxwFYHGkjHHFH/8HYAMzP2Hbbo/rXQJgrednQ2xXMhGlmnVIonEt5FmNcx02DsBb4bTLhlstLdLPywNvz2gxgD+6Wnb0BVBoc+9DDhENAXAbgIuYudS2PZOIYl3rHQB0ArAljHZ5++4WAxhFRIlE1N5l14pw2eXiPAAbmTnPbAjn8/JWPiAcv7FwZMMj+YJk1n+EKPndEbblDIhbtwbAKtdrGICXAfzg2r4YQIsw29UB0mJjNYB15jkByADwMYCfAHwEoEkEnlkygAIAabZtEXleEDHaBaACEo+9xtszgrTk+I/rd/cDgOww27UZEj82v7NZrmMvdX3HqwCsBHBhmO3y+t0BuNv1vDYBGBpOu1zbXwAwyePYcD4vb+VDyH9jOsSEoihKlHOsh4YURVGUGlAhUBRFiXJUCBRFUaIcFQJFUZQoR4VAURQlylEhUBQPiKiS3Ec9Ddqota7RLCPZ70FRqhEXaQMUpQ5SxszdIm2EooQL9QgUxU9c49Q/RjJvwwoiOt61vR0RfeIaSO1jImrj2t6MZC6A1a5Xf9epYonoOdeY8x8QUYOI3ZSiQIVAUZxo4BEausK2r5CZTwMwE8CTrm0zALzIzF0gg7tNd22fDuBzZu4KGf9+nWt7JwD/YeZTARyA9F5VlIihPYsVxQMiOsjMKQ7btwEYyMxbXIOD7WbmDCLaBxkqocK1fRczNyWifABZzHzIdo52AD5k5k6u97cDiGfmB8Nwa4riiHoEihIY7GU9EA7Z1iuhuTolwqgQKEpgXGFbLnetL4OMbAsAVwH40rX+MYDrAICIYokoLVxGKkogaE1EUarTgFyTl7t4n5lNE9LGRLQGUqu/0rXtRgDPE9FUAPkArnZtvwnAbCK6BlLzvw4y6qWi1Ck0R6AofuLKEWQz875I26IowURDQ4qiKFGOegSKoihRjnoEiqIoUY4KgaIoSpSjQqAoihLlqBAoiqJEOSoEiqIoUc7/AwaSNW/K9trXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Train acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Test acc')\n",
    "plt.title('Model accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Train loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Test loss')\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cl4yYfdQdnsZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[1. 0. 0. 0. 0.]\n",
      "['Groundnut__Alternaria__Leafspot', 'Groundnut__Early__Late__Leafspot', 'Groundnut__Healthy', 'Groundnut__Rosette', 'Groundnut__Rust']\n",
      "The disease of the given groundnut leaf is Groundnut__Alternaria__Leafspot predicted with 100.0 % confidence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image=tf.keras.utils.load_img(r'C:\\Users\\ritan\\Desktop\\Maths for ML\\Groundnut FD\\Images for checking\\img.jpg',target_size=(224,224))\n",
    "test_image=tf.keras.utils.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=model.predict(test_image)\n",
    "result=result.flatten()\n",
    "print(result)\n",
    "print(class_names)\n",
    "index=result.argmax()\n",
    "confidence=result[index]*100;\n",
    "pred_class=class_names[index]\n",
    "if pred_class!='Groundnut__Healthy':\n",
    "    print(f'The disease of the given groundnut leaf is {pred_class} predicted with {confidence} % confidence')\n",
    "else:\n",
    "    print(f'The groundnut leaf is healthy predicted with {confidence} % confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSMMvuX5mJt7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
